Sqoop 5分钟演示
=================================================================================
Sqoop使用 **唯一的名称或者持久性ID** 来识别connectors、links、jobs和配置。我们支持通过其 **唯
一名称** 或 **其数据库ID** 查询实体。

### 启动客户端
使用以下命令以 **交互模式** 启动客户端：
```shell
$ sqoop2-shell
```
配置客户端使用你的Sqoop服务器：
```shell
sqoop:000> set server --host node-slave01 --port 12000 --webapp sqoop

Server is set successfully
```
通过简单的版本检查验证连接是否正常工作：
```shell
sqoop:000> show version --all

client version:
  Sqoop 2.0.0-SNAPSHOT source revision 418c5f637c3f09b94ea7fc3b0a4610831373a25f
  Compiled by vbasavaraj on Mon Nov  3 08:18:21 PST 2014
server version:
  Sqoop 2.0.0-SNAPSHOT source revision 418c5f637c3f09b94ea7fc3b0a4610831373a25f
  Compiled by vbasavaraj on Mon Nov  3 08:18:21 PST 2014
API versions:
  [v1]
```
如上所示，描述了sqoop客户端版本，服务器版本和rest API支持的版本。

你可以使用 **help命令** 来检查 **sqoop shell** 中所有支持的命令：
```shell
sqoop:000> help

For information about Sqoop, visit: http://sqoop.apache.org/

Available commands:
  :exit    (:x  ) Exit the shell
  :history (:H  ) Display, manage and recall edit-line history
  help     (\h  ) Display this help message
  set      (\st ) Configure various client options and settings
  show     (\sh ) Display various objects and configuration options
  create   (\cr ) Create new object in Sqoop repository
  delete   (\d  ) Delete existing object in Sqoop repository
  update   (\up ) Update objects in Sqoop repository
  clone    (\cl ) Create new object based on existing one
  start    (\sta) Start job
  stop     (\stp) Stop job
  status   (\stu) Display status of a job
  enable   (\en ) Enable object in Sqoop repository
  disable  (\di ) Disable object in Sqoop repository
  grant    (\g  ) Grant access to roles and assign privileges
  revoke   (\r  ) Revoke access from roles and remove privileges

For help on a specific command type: help command
```

### 创建Link对象
检查你的Sqoop服务器上的注册连接器：
```shell
sqoop:000> show connector
+------------------------+---------+------------------------------------------------------------+----------------------+
|          Name          | Version |                           Class                            | Supported Directions |
+------------------------+---------+------------------------------------------------------------+----------------------+
| generic-jdbc-connector | 1.99.7  | org.apache.sqoop.connector.jdbc.GenericJdbcConnector       | FROM/TO              |
| kite-connector         | 1.99.7  | org.apache.sqoop.connector.kite.KiteConnector              | FROM/TO              |
| oracle-jdbc-connector  | 1.99.7  | org.apache.sqoop.connector.jdbc.oracle.OracleJdbcConnector | FROM/TO              |
| ftp-connector          | 1.99.7  | org.apache.sqoop.connector.ftp.FtpConnector                | TO                   |
| hdfs-connector         | 1.99.7  | org.apache.sqoop.connector.hdfs.HdfsConnector              | FROM/TO              |
| kafka-connector        | 1.99.7  | org.apache.sqoop.connector.kafka.KafkaConnector            | TO                   |
| sftp-connector         | 1.99.7  | org.apache.sqoop.connector.sftp.SftpConnector              | TO                   |
+------------------------+---------+------------------------------------------------------------+----------------------+
```
我们的例子包含多个连接器。**generic-jdbc-connector** 是依靠Java JDBC接口与数据源进行通信的基本
连接器。它应该与提供JDBC驱动程序的最常见的数据库一起工作。请注意，**你必须单独安装JDBC驱动程序。由于
不兼容的许可证，它们并未捆绑在Sqoop中**。

在我们的示例中，通用JDBC连接器的名称为 **generic-jdbc-connector**，我们将使用此值 **为此连接器创
建新的link对象**。请注意，**link名称应该是唯一的**。
```shell
sqoop:000> create link -connector generic-jdbc-connector

Creating link for connector with name generic-jdbc-connector
Please fill following values to create new link object
Name: mysql-brain-link

Database connection

Driver class: com.mysql.jdbc.Driver
Connection String: jdbc:mysql://172.16.177.168:3306/brain?characterEncoding=UTF-8&amp;useSSL=false
Username: myhive
Password: *********
Fetch Size:
Connection Properties:
There are currently 0 values in the map:
entry#

SQL Dialect

Identifier enclose:  #注意输入一个空格
New link was successfully created with validation status OK and name mysql-brain-lin
```
我们的新link对象是用分配的名称：First Link成功创建的。

在 **show connector -all** 显示中，我们看到有一个 **hdfs-connector** 注册。现在我们创建另一
个链接对象，但这次是为了 **hdfs连接器**。
```shell
sqoop:000> create link --connector hdfs-connector

Creating link for connector with name hdfs-connector
Please fill following values to create new link object
Name: hdfs-brain-link

HDFS cluster

URI: hdfs://node-master/
Conf directory: /test/sqoop
Additional configs::
There are currently 0 values in the map:
entry#
New link was successfully created with validation status OK and name hdfs-brain-link
```

### 创建Job对象
连接器现实有`From`用于读数据，`To`用于写数据。我们可以通过show connector -all命令看到，
generic-jdbc-connector连接器支持两个方向的操作（目前有三个只能支持`To`）。为了创建一个Job，我们需
要指定由link ID唯一标识的Job的From和To部分。我们已经在系统中创建了2个连接，可以用下面的命令验证：
```shell
sqoop:000> show link --all

2 link(s) to show:
link with name mysql-brain-link (Enabled: true, Created by hadoop at 2/4/18 10:55 AM, Updated by hadoop at 2/4/18 12:02 PM)
Using Connector generic-jdbc-connector with name {1}
  Database connection
    Driver class: com.mysql.jdbc.Driver
    Connection String: jdbc:mysql://172.16.177.167:3306/brain?characterEncoding=UTF-8&amp;useSSL=false
    Username: myhive
    Password:
    Fetch Size:
    Connection Properties:
      protocol = tcp
  SQL Dialect
    Identifier enclose:  
link with name hdfs-brain-link (Enabled: true, Created by hadoop at 2/4/18 11:28 AM, Updated by hadoop at 2/4/18 11:59 AM)
Using Connector hdfs-connector with name {1}
  HDFS cluster
    URI: hdfs://node-master/
    Conf directory: /etc/hadoop
    Additional configs::
```
接下来，我们可以使用这两个链接名称将From和To关联起来。
```shell
sqoop:000> create job -f "mysql-brain-link" -t "hdfs-brain-link"

Creating job for links with from name mysql-brain-link and to name hdfs-brain-link
Please fill following values to create new job object
Name: job_case_cf_baseinfo

Database source

Schema name: brain
Table name: CASE_CF_BASEINFO
SQL statement:
Column names:
There are currently 0 values in the list:
element#
Partition column:
Partition column nullable:
Boundary query:

Incremental read

Check column:
Last value:

Target configuration

Override null value:
Null value:
File format:
  0 : TEXT_FILE
  1 : SEQUENCE_FILE
  2 : PARQUET_FILE
Choose: 0
Compression codec:
  0 : NONE
  1 : DEFAULT
  2 : DEFLATE
  3 : GZIP
  4 : BZIP2
  5 : LZO
  6 : LZ4
  7 : SNAPPY
  8 : CUSTOM
Choose: 0
Custom codec:
Output directory: /test/sqoop/case_cf_baseinfo
Append mode:

Throttling resources

Extractors: 2
Loaders: 2

Classpath configuration

Extra mapper jars:
There are currently 0 values in the list:
element#
New job was successfully created with validation status OK  and name job_case_cf_baseinfo
```
我们新job对象是用指定名称：job-brain创建的。**请注意，如果分区列允许空值，则Sqoop需要至少两个提取器
来执行数据传输**。在此场景中指定1个提取器时，Sqoop应忽略此设置并继续2个提取器。

### 启动Job
你可以使用以下命令启动sqoop job：
```shell
sqoop:000> start job --name job_case_cf_baseinfo

Submission details
Job Name: job_case_cf_baseinfo
Server URL: http://node-slave01:12000/sqoop/
Created by: hadoop
Creation date: 2018-02-04 22:14:47 CST
Lastly updated by: hadoop
External ID: job_local51348447_0001
	http://localhost:8080/
2018-02-04 22:14:52 CST: SUCCEEDED
```










































ddd
