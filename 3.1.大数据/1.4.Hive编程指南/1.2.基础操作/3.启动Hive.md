启动Hive
=================================================================================
终于我们可以从Hive命令行界面（CLI）开始执行一些命令了。

在后面的会话中，我们将使用 **$HIVE_HOME/bin/hive** 命令，这是个bash shell脚本 ，用于启动CLI。
下面脚本中出现的$HIVE_HOME可以根据需要替换为用户的Hive安装目录路径。**如果用户已经将$HIVE_HOME/bin
加入到环境变量PATH中了，那么只需要输入hive就可以执行命令了**。

和以前一样，$是bash提示符。**在Hive CLI中，字符串hive>是Hive的提示符，而大于号 > 是第2个提示符**。

### 第一步：Hive配置Hadoop HDFS

#### 复制hive-site.xml
```shell
$ cd /opt/apache-hive-2.3.2/conf
$ cp hive-default.xml.template hive-site.xml
```

#### 新建hdfs目录
使用hadoop新建hdfs目录,因为在hive-site.xml中有默认如下配置：
```xml
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
    <description>location of default database for the warehouse</description>
</property>
```
进入hadoop安装目录，**执行hadoop命令新建/user/hive/warehouse目录，并授权**，用于存储文件：
```shell
$ cd /opt/hadoop-2.9.0

$ hadoop fs -mkdir -p /user/hive/warehouse  
$ hadoop fs -mkdir -p /tmp  
$ hadoop fs -chmod -R g+w /user/hive/warehouse  
$ hadoop fs -chmod -R g+w /tmp
```
用以下命令检查目录是否创建成功：
```shell
$ hadoop fs -ls /user/hive
```

#### 新建hive-env.sh
```shell
$ cd /opt/apache-hive-2.3.2/conf/
$ cp hive-env.sh.template hive-env.sh

$ vim hive-env.sh
```
**修改hive-env.sh**：
```
export HIVE_CONF_DIR=/opt/apache-hive-2.3.2/conf
export HIVE_AUX_JARS_PATH=/opt/apache-hive-2.3.2/lib
```
**配置hive-env.sh的可执行权限**：
```shell
$ sudo chmod 777 hive-env.sh
```

#### 修改hive-site.xml
**在前面加一段xml属性设置代码，用于表示${system:java.io.tmpdir}的值，目前系统变量里没有找到
java.io.tmpdir，所以替换一下**：
```xml
<property>
    <name>system:java.io.tmpdir</name>
    <value>/opt/apache-hive-2.3.2/tmp</value>
</property>
<property>
    <name>system:user.name</name>
    <value>fuhd</value>
</property>
```

### 第二步：初始化schema
当前默认使用的是derdy数据库，所以使用如下命令：
```shell
# 初始化,如果是mysql则derby可以直接替换成mysql
schematool -initSchema -dbType derby
```
```
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.2/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop-2.9.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:	 APP
Starting metastore schema initialization to 2.3.0
Initialization script hive-schema-2.3.0.derby.sql
Initialization script completed
schemaTool completed
```

### 第三步：创建表、执行语句
这里有个样例会话：
```shell
$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.2/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop-2.9.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/opt/apache-hive-2.3.2/lib/hive-common-2.3.2.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive>
```
注： 日志上说，**Hive 2中配置Hive-on-MR是过时的，要求使用spark、tez作为计算引擎**。

如果直接执行代码：
```shell
hive> CREATE TABLE x (a INT);
OK
Time taken: 0.48 seconds

hive> SELECT * FROM x;
OK
Time taken: 1.345 seconds

hive> SELECT *
    > FROM x;
OK
Time taken: 0.14 seconds

hive> DROP TABLE x;
OK
Time taken: 1.346 seconds

hive> exit;
$
```



































dddd
