启动Hive
=================================================================================
终于我们可以从Hive命令行界面（CLI）开始执行一些命令了。

在后面的会话中，我们将使用 **$HIVE_HOME/bin/hive** 命令，这是个bash shell脚本 ，用于启动CLI。
下面脚本中出现的$HIVE_HOME可以根据需要替换为用户的Hive安装目录路径。**如果用户已经将$HIVE_HOME/bin
加入到环境变量PATH中了，那么只需要输入hive就可以执行命令了**。

和以前一样，$是bash提示符。**在Hive CLI中，字符串hive>是Hive的提示符，而大于号 > 是第2个提示符**。

### 第一步：Hive配置Hadoop HDFS

#### 复制hive-site.xml
```shell
$ cd /opt/apache-hive-2.3.2/conf
$ cp hive-default.xml.template hive-site.xml
```

#### 新建hdfs目录
使用hadoop新建hdfs目录,因为在hive-site.xml中有默认如下配置：
```xml
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
    <description>location of default database for the warehouse</description>
</property>
```
进入hadoop安装目录，**执行hadoop命令新建/user/hive/warehouse目录，并授权**，用于存储文件：
```shell
$ cd /opt/hadoop-2.9.0

$ hadoop fs -mkdir -p /user/hive/warehouse  
$ hadoop fs -mkdir -p /user/hive/tmp  
$ hadoop fs -mkdir -p /user/hive/log  
$ hadoop fs -chmod -R 777 /user/hive/warehouse  
$ hadoop fs -chmod -R 777 /user/hive/tmp  
$ hadoop fs -chmod -R 777 /user/hive/log  
```
用以下命令检查目录是否创建成功：
```shell
$ hadoop fs -ls /user/hive
```

#### 修改hive-site.xml
搜索 **hive.exec.scratchdir**，将该name对应的value修改为 **/user/hive/tmp**：
```xml
<property>
    <name>hive.exec.scratchdir</name>  
    <value>/user/hive/tmp</value>  
</property>  
```
搜索 **hive.querylog.location**，将该name对应的value修改为 **/user/hive/log/hadoop**：
```xml
<property>
    <name>hive.querylog.location</name>
    <value>/user/hive/log/hadoop</value>
    <description>Location of Hive run time structured log file</description>
</property>
```

#### 创建tmp文件
```shell
$ mkdir /opt/apache-hive-2.3.2/tmp
```
并在hive-site.xml中修改：
+ **把{system:java.io.tmpdir}** 改成/opt/apache-hive-2.3.2/tmp；
+ **把{system:user.name}改成{user.name}**；

#### 新建hive-env.sh
```shell
$ cd /opt/apache-hive-2.3.2/conf/
$ cp hive-env.sh.template hive-env.sh

$ vim hive-env.sh
```
**修改hive-env.sh**：
```
HADOOP_HOME=/opt/hadoop-2.9.0
export HIVE_CONF_DIR=/opt/apache-hive-2.3.2/conf
export HIVE_AUX_JARS_PATH=/opt/apache-hive-2.3.2/lib
```

### 第二步：初始化schema
当前默认使用的是derdy数据库，所以使用如下命令：
```shell
# 初始化,如果是mysql则derby可以直接替换成mysql
schematool -initSchema -dbType derby
```
```
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.2/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop-2.9.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:	 APP
Starting metastore schema initialization to 2.3.0
Initialization script hive-schema-2.3.0.derby.sql
Initialization script completed
schemaTool completed
```
如果报错如下：
```
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.2/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop-2.9.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:	 APP
Starting metastore schema initialization to 2.3.0
Initialization script hive-schema-2.3.0.derby.sql
Error: FUNCTION 'NUCLEUS_ASCII' already exists. (state=X0Y68,code=30000)
org.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED! Metastore state would be inconsistent !!
Underlying cause: java.io.IOException : Schema script failed, errorcode 2
Use --verbose for detailed stacktrace.
*** schemaTool failed ***
```
**请删除$HIVE_HOME/conf/metastore_db/目录**。

### 第三步：创建表、执行语句
这里有个样例会话：
```shell
$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.2/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop-2.9.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/opt/apache-hive-2.3.2/lib/hive-common-2.3.2.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive>
```
注： 日志上说，**Hive 2中配置Hive-on-MR是过时的，要求使用spark、tez作为计算引擎**。

如果直接执行代码：
```shell
hive> CREATE TABLE x (a INT);
OK
Time taken: 0.48 seconds

hive> SELECT * FROM x;
OK
Time taken: 1.345 seconds

hive> SELECT *
    > FROM x;
OK
Time taken: 0.14 seconds

hive> DROP TABLE x;
OK
Time taken: 1.346 seconds

hive> exit;
$
```



































dddd
