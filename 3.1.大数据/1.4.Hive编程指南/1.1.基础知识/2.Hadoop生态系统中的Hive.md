Hadoop生态系统中的Hive
=================================================================================
当用户真正使用Hadoop的API来实现WordCount算法（见原书中的示例）时，甚至有更多底层细节需要用户自己
来控制。这是一个只适用于有经验的Java开发人员的工作，因此也就将Hadoop潜在地放在了一个非程序员用户无
法触及的位置，即使这些用户了解他们想使用的算法。

这也就是引用Hive的原因。**Hive不仅提供了一个熟悉SQL的用户所能熟悉的编程模型，还消除了大量的通用代码**，
甚至是那些有时是不得不使用Java编写的令人棘手的代码。

**这就是为什么Hive对于Hadoop是如此重要的原因**，无论用户是DBA还是Java开发工程师。Hive可以让你花
费相当少的精力就可以完成大量的工作。

下图显示了Hive的主要“模块”以及Hive是如何与Hadoop交互工作的。

![Hive组成模块](img/p1.jpeg)

有好几种方式可以与Hive进行交互。本书中，我们将主要关注于 **CLI，也就是命令行界面**。

**Hive发行版本中附带的模块有CLI，一个称为Hive网页界面（HWI）的简单网页界面，以及可通过JDBC、ODBC
和一个Thrift服务器进行编程访问的几个模块**。

**所有的命令和查询都会进入到Driver（驱动模块），通过该模块对输入进行解析编译，对需求的计算进行优化，
然后按照指定的步骤执行（通常是启动多个MapReduce任务（job）来执行）。当需要启动MapReduce任务（job）
时，Hive本身是不会生成Java MapReduce算法程序的。相反，Hive通过一个表示“job执行计划”的XML文件驱
动执行内置的、原生的Mapper和Reduce模块。换句话说，这些通用的模块函数类似于微型的语言翻译程序，而这个
驱动计算的“语言”是以XML形式编码的**。

**Hive通过和JobTracker通信来初始化MapReduce任务（job），而不必部署在JobTracker所在的管理节点上
执行**。在大型集群中，通常会有网关机专门用于部署像Hive这样的工具。在这些网关机上可远程和管理节点上的
JobTracker通信来执行任务（job）。通常，**要处理的数据文件是存储在HDFS中的，而HDFS是由NameNode进行
管理的**。

**Metastore（元数据存储）是一个独立的关系型数据库（通常是一个MySQL实例），Hive会在其中保存表模式和
其他系统元数据**。

尽管本书是关于Hive的，不过还是有必要提及其他的一些高级工具，这样用户可以根据需求进行选择。**Hive最适
合于数据仓库程序，对于数据仓库程序不需要实时响应查询，不需要记录级别的插入、更新和删除**。当然，Hive也
非常适合于有一定SQL知识的用户。

### Pig
**Hive的替代工具中最有名的就是Pig了**（`http://pig.apache.org`）。Pig是由Yahoo!开发完成的，
而同时期Facebook正在开发Hive。Pig现在同样也是一个和Hadoop紧密联系的顶级Apache项目。

**假设用户的输入数据具有一个或者多个源，而用户需要进行一组复杂的转换来生成一个或者多个输出数据集。
如果使用Hive，用户可能会使用嵌套查询来解决这个问题，但是在某些时刻会需要重新保存临时表（这个需要
用户自己进行管理）来控制复杂度**。

**Pig被描述成一种数据流语言**，而不是一种查询语言。**在Pig中，用户需要写一系列的声明语句来定义某
些关系和其他一些关系之间的联系，这里每个新的关系都会执行新的数据转换过程。Pig会查找这些声明，然后创
建一系列有次序的MapReduce任务（job），来对这些数据进行转换，直到产生符合用户预期的计算方法所得到
的最终结果**。

这种步进式的数据“流”可以比一组复杂的查询更加直观。也因此，**Pig常用于ETL（数据抽取，数据转换和数
据装载）过程的一部分**，也就是将外部数据装载到Hadoop集群中，然后转换成所期望的数据格式。

**Pig的一个缺点就是其所使用的定制语言不是基于SQL的**。这是可以理解的，因为Pig本身就不是被设计为一
种查询语言的，但是这也意味着不适合将SQL应用程序移植到Pig中，而经验丰富的SQL用户可能需要投入更高的
学习成本来学习Pig。

然而，**Hadoop团队通常会将Hive和Pig结合使用，对于特定的工作选择合适的工具**。

### HBase




































### Cascading、Crunch及其他
