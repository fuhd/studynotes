Hadoop生态系统中的Hive
=================================================================================
当用户真正使用Hadoop的API来实现WordCount算法（见原书中的示例）时，甚至有更多底层细节需要用户自己
来控制。这是一个只适用于有经验的Java开发人员的工作，因此也就将Hadoop潜在地放在了一个非程序员用户无
法触及的位置，即使这些用户了解他们想使用的算法。

这也就是引用Hive的原因。**Hive不仅提供了一个熟悉SQL的用户所能熟悉的编程模型，还消除了大量的通用代码**，
甚至是那些有时是不得不使用Java编写的令人棘手的代码。

**这就是为什么Hive对于Hadoop是如此重要的原因**，无论用户是DBA还是Java开发工程师。Hive可以让你花
费相当少的精力就可以完成大量的工作。

下图显示了Hive的主要“模块”以及Hive是如何与Hadoop交互工作的。

![Hive组成模块](img/p1.jpeg)

有好几种方式可以与Hive进行交互。本书中，我们将主要关注于 **CLI，也就是命令行界面**。

**Hive发行版本中附带的模块有CLI，一个称为Hive网页界面（HWI）的简单网页界面，以及可通过JDBC、ODBC
和一个Thrift服务器进行编程访问的几个模块**。

**所有的命令和查询都会进入到Driver（驱动模块），通过该模块对输入进行解析编译，对需求的计算进行优化，
然后按照指定的步骤执行（通常是启动多个MapReduce任务（job）来执行）。当需要启动MapReduce任务（job）
时，Hive本身是不会生成Java MapReduce算法程序的。相反，Hive通过一个表示“job执行计划”的XML文件驱
动执行内置的、原生的Mapper和Reduce模块。换句话说，这些通用的模块函数类似于微型的语言翻译程序，而这个
驱动计算的“语言”是以XML形式编码的**。

**Hive通过和JobTracker通信来初始化MapReduce任务（job），而不必部署在JobTracker所在的管理节点上
执行**。在大型集群中，通常会有网关机专门用于部署像Hive这样的工具。在这些网关机上可远程和管理节点上的
JobTracker通信来执行任务（job）。通常，**要处理的数据文件是存储在HDFS中的，而HDFS是由NameNode进行
管理的**。

**Metastore（元数据存储）是一个独立的关系型数据库（通常是一个MySQL实例），Hive会在其中保存表模式和
其他系统元数据**。

尽管本书是关于Hive的，不过还是有必要提及其他的一些高级工具，这样用户可以根据需求进行选择。**Hive最适
合于数据仓库程序，对于数据仓库程序不需要实时响应查询，不需要记录级别的插入、更新和删除**。当然，Hive也
非常适合于有一定SQL知识的用户。




























































dddd
