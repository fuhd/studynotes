分区表、管理表
=================================================================================
数据分区的一般概念存在已久。其可以有多种形式，但是 **通常使用分区来水平分散压力**。

**Hive中有分区表的概念**。我们可以看到 **分区表具有重要的性能优势**，而且分区表还可以将数据以一种
符合逻辑的方式进行组织，比如分层存储。

我们首先讨论一下 **分区管理表**。重新来看之前的那张emplyees表，并假设我们在一个非常大的跨国公司
工作。我们的HR人员 **经常会执行一些带WHERE语句的查询**，这样可以将结果限制在某个特定的国家或者某个
特定的第一级细分（为简单起见我们将只使用到state（州））。那么，**让我们先按照country（国家）再按
照state（州）来对数据进行分区吧**：
```sql
CREATE TABLE employees (
    name            STRING,
    salary          FLOAT,
    subordinates    ARRAY<STRING>,
    deductions      MAP<STRING, FLOAT>,
    address         STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>)
PARTITIONED BY (country STRING, state STRING);
```
**分区表改变了Hive对数据存储的组织方式**。如果我们是在mydb数据库中创建的这个表，那么对于这个表只
会有一个employees目录与之对应：
```
hdfs://master_server/user/hive/warehouse/mydb.db/employees
```
但是，**Hive现在将会创建好可以反映分区结构的子目录**。例如：
```
...
.../employees/country=CA/state=AB
.../employees/country=CA/state=BC
...
.../employees/country=US/state=AL
.../employees/country=US/state=AK
...
```
是的，**那些是实际的目录名称**。州目录下将会包含有零个文件或者多个文件，这些文件中存放着那些州的雇
员信息。

**分区字段**（这个例子中就是country和state）一旦创建好，表现得就得变通的字段一样。事实上，**除非
需要优化查询性能**，否则使用这些表的用户不需要关心这些“字段”是否是分区字段。

例如，下面这个查询语句将会查找出在美国伊利诺斯州的所有雇员：
```sql
SELECT * FROM employees WHERE country = 'US' AND state = '1L';
```
需要注意的是，**因为country和state的值已经包含在文件目录名称中了，所以也就没有必要将这些值存放到
它们目录下的文件中了。事实上，数据只能从这些文件中获得，因此用户需要在表的模式中说明这点，而且这个数
据浪费空间**。

**对数据进行分区，也许最重要的原因就是为了更快地查询**。在前面那个将结果范围限制在伊利诺斯州的雇员
的查询中，**仅仅需要扫描一个目录下的内容即可**。即使我们有成千上万个国家和州目录，除了一个目录其他
的都可以忽略不计。对于非常大的数据集，分区可以显著地搞高查询性能，除非对分区进行常见的范围筛选（例如，
按照地理位置范围或按照时间范围等）。

**当我们在WHERE子句中增加谓词来按照分区值进行过滤时，这此谓词被称为分区过滤器**。

如果表中的数据以及分区个数都非常大的话，执行这样一个包含有所有分区的查询可能会触发一个巨大的MapReduce
任务。**一个高度建议的安全措施就是将Hive设置为“strict（严格）”模式，这样如果对分区表进行查询而WHERE
子句没有加分区过滤的话，将会禁止提交这个任务**。用户也可以按照下面的语句将属性值设置为 **“nostrict
（非严格）”**：
```sql
hive> set hive.mapred.mode=strict;

hive> SELECT e.name, e.salary FROM employees e LIMIT 100;
FAILED: Error in semantic analysis: No partition predicate found for Alias "e" Table "employees"

hive> set hive.mapred.mode=nonstrict;

hive> SELECT e.name, e.salary FROM employees e LIMIT 100;
John Doe 100000.0
...
```
**可以通过SHOW PARTITIONS命令查看表中存在的所有分区**：
```sql
hive> SHOW PARTITIONS employees;
...
country=CA/state=AB
country=CA/state=BC
...
country=US/state=AL
country=US/state=AK
...
```
如果表中现在存在很多的分区，**而用户只想查看是否存储某个特定分区键的分区的话，用户还可以在这个命令
上增加一个指定了一个或者多个特定分区字段值的PARTITION子句，进行过滤查询**：
```sql
hive> SHOW PARTITIONS employees PARTITION(country='US');
country=US/state=AL
country=US/state=AK
...
hive> SHOW PARTITIONS employees PARTITION(country='US',state='AK');
country=US/state=AK
```
**DESCRIBE EXTENDED employees命令也会显示出分区键**：
```sql
hive> DESCRIBE EXTENDED employees;
name            string,
salary          float,
...
address         struct<...>,
country         string,
state           string,

Detailed Table Infomation...
partitionKeys: [FieldSchema(name:country, type:string, comment:null),
    FieldSchema(name:state, type:string, comment:null)],
...
```
输出信息中的模式信息部分会将country和state以及其他字段列在一起，因为就查询而言，它们就是字段。
**Detailed Table Infomation（详细表信息）将country和state作为分区键处理**。这两个键当前的
注释都是null，我们也可以像给普通的字段增加注释一样给分区字段增加注释。

**在管理表中用户可以通过载入数据的方式创建分区**。如下例中的语句在从一个本地目录（$HOME/california-employees）
载入数据到表中的时候，将会创建一个US/CA（表示加利福尼亚州）分区。**用户需要为每个分区字段指定一个值**。
**请注意我们在HiveQL中是如何引用HOME环境变量的**：
```sql
LOAD DATA LOCAL INPATH '${env:HOME}/califormia-employees'
INTO TABLE employees
PARTITION (country='US', state='CA')
```
**Hive将会创建这个分区对应的目录** .../employees/country=US/state=CA，而且$HOME/california-employees
这个目录下的文件将会 **被拷贝到上述分区目录下**。

### 1.外部分区表
**外部表同样可以使用分区**。事实上，用户可能会发现，这是管理大型生产数据集最常见的情况。这种结合
给用户 **提供了一个可以和其他工具共享数据的方式，同时也可以优化查询性能**。

我们举一个新例子，非常适合这种场景，**即日志文件分析**。对于日志信息，大多数的组织使用一个标准的
格式，其中记录有时间戳、严重程序（例如ERROR、WARTING、INFO），也许还包含有服务器名称和进程ID，
然后跟着一个可以为任何内容的文本信息。

我们可以按照如下方式来定义对应的Hive表：
```sql
CREATE EXTERNAL TABLE IF NOT EXISTS log_messages (
    hms           INT,
    severity      STRING,
    server        STRING,
    process_id    INT,
    message       STRING)
PARTITIONED BY (year INT, month INT, day INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
```
我们现在假定将日志数据按照天进行划分，划分数据尺寸合适，而且按天这个粒度进行查询速度也足够快。

回想下，之前我们创建过一个 **非分区外部表**，是一个股票交易表，那时 **要求使用一个LOCATION子句**。
对于 **外部分区表则没有这样的要求**。有一个 **ALTER TABLE语句可以单独进行增加分区。这个语句需要为
每一个分区键指定一个值**，本例中，也就是需要为year、month和day这3个分区键都指定值。下面是一个例子，
演示如何增加一个2012年1月2日的分区：
```sql
ALTER TABLE log_messages ADD PARTITION(year=2012,month=1,day=2)
    LOCATION 'hdfs://master_server/data/log_messages/2012/01/02';
```
**我们使用的目录组织习惯完全由我们自己定义**。这里，我们按照分层目录结构组织，因为这是一个合乎逻辑
的数据组织方式，但是并非要求一定如此。我们可以遵从Hive的目录命名习惯（例如，.../exchange=NASDAQ/symbol=AAPL），
**但是也并非要求一定如此**。

**Hive不关心一个分区对应的分区目录是否存在或者分区目录下是否有文件**。如果分区目录不存在或分区目录
下没有文件，则对于这个过滤分区的查询将没有返回结果。

这个功能所具有的另一个好处是：可以将新数据写入到一个专用的目录中，并与位于其他目录中的数据存在明显的
区别。同时，不管用户是将旧数据转移到一个“存档”位置还是直接删除掉，新数据被簒改的风险都被降低了，因为
新数据的数据子集位于不同的目录下。

**和非分区外部表一样，Hive并不控制这些数据。即使表被删除，数据也不会被删除**。

和分区管理表一样，**通过SHOW PARTITIONS命令可以查看一个外部表的分区**：
```sql
hive> SHOW PARTITIONS log_messages;
...
year=2011/month=12/day=31
year=2012/month=1/day=1
year=2012/month=1/day=2
...
```
同样地，**DESCRIBE EXTENDED log_messages语句会将分区键作为表的模式的一部分，和partitionKeys
列表的内容同时进行显示**：
```sql
hive> DESCRIBE EXTENDED log_messages;
...
message           string,
year              int,
month             int,
day               int

Detailed Table Information ...
partitionKeys: [FieldSchema(name:year, type:int, comment:null),
  FieldSchema(name:month,type:int, comment:null),
  FieldSchema(name:day, type:int, comment:null)],
...
```
**这个输出缺少了一个非常重要的信息，那就是分区数据实际存在的路径**。这里有一个路径字段，但是该字段
仅仅表示如果表是管理表其会使用到的Hive默认的目录。不过，我们 **可以通过如下方式查看到分区数据所在
的路径**：
```sql
hive> DESCRIBE EXTENDED log_messages PARTITION (year=2012,month=1,day=2);
...
location:s3n://ourbucket/logs/2011/01/02,
...
```
**我们通常会使用分区外部表，因为它具有非常多的优点，例如逻辑数据管理、高性能的查询等**。

**ALTER TABLE ... ADD PARTITION语句并非只有对外部表才能够使用**。对于管理表，当有分区数据不是由
我们之前讨论过的LOAD和INSERT语句产生时，用户同样可以使用这个命令指定分区路径。用户需要记住并非所
有的表数据都是放在通常的Hive ”warehouse”目录下的，同时当删除管理表时，这些数据不会连带被删除掉！
**因此，从“理智的”角度来看，是否敢于对管理表使用这个功能是一个问题**。

### 2.自定义表的存储格式
我们谈论过 **Hive的默认存储格式是文本文件格式**，这个也可以通过可选的子句 **STORED AS TEXTFILE**
显式指定，同时用户还可以在 **创建表时指定各种各样的分隔符**。这里我们重新展示下之前讨论过的那个
employees表：
```sql
CREATE TABLE employees (
    name                STRING,
    salary              FLOAT,
    subordinates        ARRAY<STRING>,
    deductions          MAP<STRING,FLOAT>,
    address             STRUCT<street:STRING, city:STRING,state:STRING, zip:INT>
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\001'
COLLECTION ITEMS TERMINATED BY '\002'
MAP KEYS TERMINATED BY '\003'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```
**TEXTFILE** 意味着所有字段都使用 **字母、数字、字符编码，包括那些国际字符集**，尽管我们可以发现
Hive默认是使用 **不可见字符** 来作为“终结者”（**分隔符**）的。使用TEXTFILE就意味着，每一行被认
为是一个单独的记录。

用户可以将TEXTFILE替换为其他Hive所 **支持的内置文件格式**，包括 **SEQUENCEFILE和RCFILE**，这两
种文件格式都是 **使用二进制编码和压缩（可选）** 来优化磁盘空间使用以及I/O带宽性能的（这些文件格式后面
再讲）。

对于 **记录是如何被编码成文件的，以及列是如何被编码为记录的**，Hive指出了它们之间的不同。**用户可以分
别自定义这些行为**。

**记录编码** 是通过一个 **inputformat对象** 来控制（例如TEXTFILE后面的Java代码实现）。Hive使用了
一个名为 **org.apache.hadoop.mapred.TextInputFormat** 的Java类（编译后的模块）。

**记录的解析** 是由 **序列化器/反序列化器（或者编写为SerDe）** 来控制的。对于TEXTFILE，Hive所使用
的 **SerDe** 是另外一个被称为 **org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe的Java**
类。

为了保持完整性，Hive还使用一个叫做 **outputformat** 的对象来将查询的输出 **写入到文件中或者输出到
控制台**。对于TEXTFILE，用于输出的Java类名为 **org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat**。
```
提示：

Hive使用一个inputformat对象将输入流分割成记录，然后使用一个outputformat对象来将记录格式化为输出流（例如查询的输出结果）
，再使用一个SerDe在读数据时将记录解析成列，在写数据时将列编码成记录。
```
**用户还可以指定第三方的输入和输出格式以及SerDe，这个功能允许用户自定义Hive本身不支持的其他广泛
的文件格式**。

这里有一个使用了自定义SerDe、输入格式和输出格式的完整的例子，其可以通过 **Avro协议** 访问这些文
件，示例：
```sql
CREATE TABLE kst
PARTITIONED BY (ds string)
ROW FORMAT SERDE 'com.linkedin.haivvreo.AvroSerDe'
WITH SERDEPROPERTIES ('schema.url'='http://schema_provider/kst.avsc')
STORED AS
INPUTFORMAT 'com.linkedin.haivvreo.AvroContainerInputFormat'
OUTPUTFORMAT 'com.linkedin.haivvreo.AvroContainerOutputFormat';
```




















































ddd
