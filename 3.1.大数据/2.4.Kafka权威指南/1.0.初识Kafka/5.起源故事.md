起源故事
================================================================================
**LinkedIn有一个数据收集系统和应用程序指标，它使用自定义的收集器和一些开源工具来保存和展示内部
数据。除了跟踪CPU使用率和应用性能这些一般性指标外，LinkedIn还有一个比较复杂的用户请求跟踪功能**。
它使用了监控系统，可以跟踪单个用户的请求是如何在内部应用间传播的。不过监控系统存在很多不足。它使
用的是轮询拉取度量指标的方式，指标之间的时间间隔较长，而且没有自助服务能力。 它使用起来不太方便，
很多简单的任务需要人工介入才能完成，而且一致性较差，同一个度量指标的名字在不同系统里的叫法不一样。

**与此同时，我们还创建了另一个用于收集用户活动信息的系统**。这是一个HTTP服务，前端的服务器会定
期连接进来，在上面发布一些消息（XML格式）。这些消息文件被转移到线下进行解析和校对。同样，这个系统
也存在很多不足。XML文件的格式无法保持一致，而且解析XML文件非常耗费计算资源。要想更改所创建的活动
类型，需要在前端应用和离线处理程序之间做大量的协调工作。即使是这样，在更改数据结构时，仍然经常出
现系统崩溃现象。而且批处理时间以小时计算，无法用它完成实时的任务。

**监控和用户活动跟踪无法使用同一个后端服务**。监控服务太过笨重，数据格式不适用于活动跟踪，而且无
法在活动跟踪中使用轮询拉取模型。另一方面，把跟踪服务用在度量指标上也过于脆弱，批处理模型不适用于
实时的监控和告警。不过，好在数据间存在很多共性，信息（比如特定类型的用户活动对应用程序性能的影响）
之间的关联度还是很高的。特定类型用户活动数量的下降说明相关应用程序存在问题，不过批处理的长时间延
迟意味着无法对这类问题作出及时反馈。

最开始，我们调研了一些现成的开源解决方案，希望能够找到一个系统，可以实时访问数据，并通过横向扩展
来处理大量的消息。我们使用ActiveMQ创建了一个原型系统，但它当时还无法满足横向扩展的需求。LinkedIn
不得不使用这种脆弱的解决方案，虽然ActiveMQ有很多缺陷会导致broker暂停服务。客户端的连接因此被阻
塞，处理用户请求的能力也受到影响。于是我们最后决定构建自己的基础设施。

## 1.Kafka的诞生
LinkedIn的开发团队由`Jay Kreps`领导。`Jay Kreps`是LinkedIn的首席工程师，之前负责分布式键
值存储系统Voldemort的开发。初建团队成员还包括Neha Narkhede，
