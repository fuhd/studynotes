起源故事
===================================================================================
**LinkedIn有一个数据收集系统和应用程序指标，它使用自定义的收集器和一些开源工具来保存和展示内部数据。
除了跟踪CPU使用率和应用性能这些一般性指标外，LinkedIn还有一个比较复杂的用户请求跟踪功能**。它使用
了监控系统，可以跟踪单个用户的请求是如何在内部应用间传播的。不过监控系统存在很多不足。它使用的是轮
询拉取度量指标的方式，指标之间的时间间隔较长，而且没有自助服务能力。 它使用起来不太方便，很多简单的
任务需要人工介入才能完成，而且一致性较差，同一个度量指标的名字在不同系统里的叫法不一样。

与此同时，我们还创建了另一个用于收集用户活动信息的系统。这是一个HTTP服务，前端的服务器会定期连接进
来，在上面发布一些消息（XML格式）。这些消息文件被转移到线下进行解析和校对。同样，这个系统也存在很
多不足。XML文件的格式无法保持一致，而且解析XML文件非常耗费计算资源。要想更改所创建的活动类型，需
要在前端应用和离线处理程序之间做大量的协调工作。即使是这样，在更改数据结构时，仍然经常出现系统崩溃
现象。而且批处理时间以小时计算，无法用它完成实时的任务。

监控和用户活动跟踪无法使用同一个后端服务。监控服务太过笨重，数据格式不适用于活动跟踪，而且无法在活
动跟踪中使用轮询拉取模型。另一方面，把跟踪服务用在度量指标上也过于脆弱，批处理模型不适用于实时的监
控和告警。不过，好在数据间存在很多共性，信息（比如特定类型的用户活动对应用程序性能的影响）之间的关
联度还是很高的。特定类型用户活动数量的下降说明相关应用程序存在问题，不过批处理的长时间延迟意味着无
法对这类问题作出及时反馈。

最开始，我们调研了一些现成的开源解决方案，希望能够找到一个系统，可以实时访问数据，并通过横向扩展来
处理大量的消息。我们使用ActiveMQ创建了一个原型系统，但它当时还无法满足横向扩展的需求。LinkedIn不得
不使用这种脆弱的解决方案，虽然ActiveMQ有很多缺陷会导致broker暂停服务。客户端的连接因此被阻塞，处
理用户请求的能力也受到影响。于是我们最后决定构建自己的基础设施。

## 1.Kafka的诞生
LinkedIn的开发团队由`Jay Kreps`领导。`Jay Kreps`是LinkedIn的首席工程师，之前负责分布式键值存储系统
Voldemort的开发。初建团队成员还包括Neha Narkhede，不久之后，Jun Rao也加入了进来。他们一起着手创
建一个消息系统，可以同时满足上述的两种需求，并且可以在未来进行横向扩展。他们的主要目标如下：
+ **使用推送和拉取模型解耦生产者和消费者**；
+ **为消息传递系统中的消息提供数据持久化，以便支持多个消费者**；
+ **通过系统优化实现高吞吐量**；
+ **系统可以随着数据流的增长进行横向扩展**；

最后我们看到的这个发布与订阅消息系统具有典型的消息系统接口，但从存储层来看，它更像是一个日志聚合系
统。**Kafka使用Avro作为消息序列化框架**，每天高效地处理数十亿级别的度量指标和用户活动跟踪信息。
LinkedIn已经拥有超过万亿级别的消息使用量（截止2015年8月），而且每天仍然需要处理超过千万亿字节的数
据。

## 2.走向开源
2010年底，Kafka作为开源项目在GitHub上发布。2011年7月，因为倍受开源社区的关注，它成为Apache软件基
金会的孵化器项目。2012年10月，Kafka从孵化器项目毕业。现在，**Kafka被很多组织用在一些大型的数据管道
上**。



