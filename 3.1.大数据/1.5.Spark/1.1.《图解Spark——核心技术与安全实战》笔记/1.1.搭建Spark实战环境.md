搭建Spark实战环境
================================================================================
## 1.基础环境搭建
在这里我们将 **搭建Spark集群运行模式**（当然在该集群中将 **安装Hadoop**，实战中需要它提供HDFS
文件存储和YARN资源调度管理）。该集群 **不少于3个节点**，如果没有物理节点，可以利用虚拟机软件，
如VMware、VirtualBox等工具进行搭建。

集群网络环境配置：

| 序号 | IP地址 | 机器名 | 运行进程 | 核数/内存 | 用户名 |
|:----|:-------|:------|:-------|:---------|:------|
| 1 | 192.168.1.10 | master | NN/SNN/DN/RM/Master/Worker | 1核/3G | spark |
| 2 | 192.168.1.11 | slave1 | DN/NM/Worker | 1核/2G | spark |
| 1 | 192.168.1.10 | slave2 | DN/NM/Worker | 1核/2G | spark |

目录：
+ `/app`
+ `/app/soft`
+ `/app/compile`
+ `/app/spark`
+ `/home/spark`

说明：
+ 所有节点均安装CentOS6.5版本64位系统，**防火墙/SElinux均禁用**，所有节点上均创建一个 **spark
用户**，该用户工作目录是`/home/spark`，**上传文件存放在`/home/spark/work`目录** 中。
+ 所有节点上均创建一个 **目录`/app`用于存放程序，并且拥有者是spark用户**。在`/app`目录下创建 
`soft`、`compile`和`spark`三个子目录：**`/app/soft`子目录用于存放支持Spark运行相关基础软件，
`/app/compile`子目录用于编译时存放源代码及编译结果，而`/app/spark`子目录则存放Spark和Hadoop
等**。

### 1.1.搭建集群样板机

#### 1.1.1.设置系统环境

+ **设置机器名**

为了方便Hadoop和Spark设置配置的方便，将设置每个服务器的机器名。以root用户登录，在命令行终端使
用：
```shell
vi /etc/sysconfig/network
```
打开配置文件，根据前面的规划设置服务器机器名：
```ini
NETWORKING=yes
HOSTNAME=master
```

+ **设置IP地址**

以root帐号登录，在如下配置文件中设置网卡信息：
```shell
vi /etc/sysconfig/network-scripts/ifcfg-eth0
```
主要参数信息如下：
```ini
#对应第一张网卡 
DEVICE=eth0
#以太网类型
TYPE=Ethernet
#是否启动时运行
ONBOOT=yes
#使用静态IP,而不是DHCP分配IP
BOOTPROTO=static 
#风格连接名称
NAME="System etho"
#eth0的MAC地址，根据实际情况而定，对应配置
HWADDR=00:50:56:94:04:3C
#指定本地IP地址
IPADDR=192.168.1.10
#指定子网掩码
NETMASK=255.255.255.0
#指定网关
GATEWAY=192.168.1.1
#DNS地址，配置后同步到/etc/resolv.conf文件
```
通过以上方法设置网络配置以后，使用命令：
```shell
service network restart
```
重启网络或重启机器，并 **使用ifconfig命令查看设置IP地址是否生效**。

+ **设置Host映射文件**

以root用户，在命令行使用命令：
```shell
vi /etc/hosts
```
编辑文件添加如下内容：
```shell
192.168.1.10 master
192.168.1.11 slave1
192.168.1.12 slave2
```
设置完毕后，可以使用：
```shell
ping master
```
直接检查master服务器是否连通以及检测服务器是响应速度。

+ **关闭防火墙和SELinux**

关闭防火墙和SELinux的原因在于Hadoop和Spark运行过程需要通过端口进行通信，而这些安全设施会进行
阻拦。另外，在使用SSH无密码访问时也存在同样的情况。

关闭iptables时，以root帐号使用如下命令：
```shell
service iptables status
```
查看 **iptables状态**，如果显式“iptables: Firewall is not running”表示iptables已经关闭；
如果显示iptables设置信息，则需要使用如下命令 **关闭iptables**：
```shell
chkconfig iptables off
```
同样，使用root用户编辑：
```shell
vi /etc/selinux/config
```
在该配置文件中设置 **SELINUX=disable，关闭防火墙和SELinux的设置需要重启机器才能够生效**。

#### 1.1.2.配置运行环境

+ **更新OpenSSL**

由于 **CentOS系统自带的OpenSSL存在Bug（6.5版本？）**，如果不更新OpenSSL，在部署过程会出现无
法通过SSH连接节点，使用如下命令 **更新SSH**：
```shell
yum update openssl
```

+ **修改OpenSSL配置**

在集群环境中需要 **进行SSH进行免密码登录，需要修改OpenSSL配置文件**，确认使用RSA算法（非对称加
密算法）进行公钥加密并确认生成私钥存放文件等。配置过程需以root用户身份，执行命令：
```shell
vi /etc/ssh/sshd_config
```
编辑：
```ini
#设置是否使用RSA算法进行加密 
RSAAuthentication yes
#设置是否使用口令验证
PubkeyAuthentication yes
#生成密钥存放的文件
AuthorizedKeysFile .ssh/authorized_keys
```
保存配置后，需要通过：
```shell
service sshd restart
```
重启SSH服务，以便生效配置。

+ **增加Spark组和用户**
在节点中创建spark组和spark用户，创建命令如下：
```shell
#-g指定组ID号
groupadd -g 1000 spark
#-u指定用户ID号
useradd -u 2000 -g spark spark
passwd spark
```
在后面执行中 **需要使用到sudo命令，应把spark用户加入到sudoers文件中**，先修改该配置文件的权限：
```shell
# u:用户,g:用户组，o:其他，a:所有
# + 表示增加权限，如u+x, u+r, u+w, g+w, g+r, o+r， a+r等
# - 表示取消权限，如u-x, u-r, u-w, g-w, g-r, o-r， a-r等
# = 表示赋予给定权限，并取消其他所有权限（如果有的话，如原来u是rwx，设置u=r，u就剩r）
chmod u+w /etc/sudoers
```
再使用`vi /etc/sudoers`命令打开该文件，找到这行：`root ALL=(ALL)ALL`，在它下面添加：
```ini
spark ALL=(ALL) ALL
```
根据前面集群安装配置规划，**创建运行环境所需要的目录结构，创建spark用户临时上传文件目录**，设置
这些目录所属组和用户均为：spark：
```shell
mkdir /app
chown -R spark:spark /app
mkdir /app/soft
mkdir /app/compile
mkdir /app/spark 
mkdir -p /home/spark/work
chown -R spark:spark /home/spark/work 
```

+ **安装和配置JDK**
如：jdk-7u55-linux-x64.tar.gz。下载完毕后，把该安装包保存在spark用户临时上传文件目录
`/home/spark/work`中，解压该文件并移动到`/app/soft`中。
```shell
cd /home/spark/work
tar -xzvf dk-7u55-linux-x64.tar.gz
mv jdk1.7.0_55 /app/soft
```
以root用户执行：`vi /etc/profile`命令，增加如下内容：
```shell
export JAVA_HOME=/app/soft/jdk1.7.0_55
export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin
```
配置完毕后，需要编译该配置文件或重新登录以生效该配置：
```shell
source /etc/profile
```

+ **安装和配置Scala**
Spark2.0版本使用的Scala2.11.X，需要在Scala官方站点下载Scala2.11.6及以上版本的安装包，如：
scala-2.11.8.tgz。把该安装包保存在spark用户临时上传目录`/home/spark/work`中，解压该文件并
移动到`/app/soft`中：
```shell
cd /home/spark/work
tar -xzvf scala-2.11.8.tgz
mv scala-2.11.8 /app/soft
```
配置`/etc/profile`文件，在该配置文件中加入相关Scala环境变量：
```shell
export SCALA_HOME=/app/soft/scala-2.11.8
export PATH=$PATH:$SCALA_HOME/bin
```
执行如下命令使配置生效：
```shell
source /etc/profile
```

### 1.2.配置集群环境 
**通过前面的步骤搭建完毕集群样板机，以该机为模板复制出两份**，按照前面规划集群配置，需要设置机器
名和IP地址，同时设置SSH无密码登录。

#### 1.2.1.复制样板机
可以通过复制样板机目录或使用VMware克隆功能进行样板机的复制，复制出两份副本，分别对应 **slave1和
slave2节点**。

#### 1.2.2.设置机器名和IP地址
略

#### 1.2.3.配置SSH无密码登录

+ **生成私角和公钥**

使用spark用户登录，在3个节点中使用如下命令生成私钥和公钥，为简单起见，在生成私钥和公钥过程中提示
问题均按回车键。
```shell
ssh-keygen -t rsa
```
命令执行完毕后，可以在`/home/spark/.ssh`目录中看见两个文件：**id_rsa和id_rsa.pub**，其中以
pub结尾的是公钥，**把公钥命名为authorized_keys_master.pub**，使用的命令如下：
```shell
cd /home/spark/.ssh
mv id_rsa.pub authorized_keys_master.pub
```
同样地把其他两个节点的公钥命名为：**authorized_keys_slave1.pub和authorized_keys_slave2.pub**。
```shell
mv id_rsa.pub authorized_keys_slave1.pub
mv id_rsa.pub authorized_keys_slave2.pub
```

+ **合并公钥信息**

把两个从节点（slave1和slave2）的公钥使用 **scp命令** 传送到master节点的`/home/spark/.ssh`
目录中。
```shell
scp authorized_keys_slave1.pub spark@master:/home/spark/.ssh
scp authorized_keys_slave2.pub spark@master:/home/spark/.ssh
```
使用 **cat命令** 把3个节点的公钥信息保存到a **uthorized_key文件** 中：
```shell
cat authorized_keys_master.pub >> authorized_keys
cat authorized_keys_slave1.pub >> authorized_keys
cat authorized_keys_slave2.pub >> authorized_keys
```
使用scp命令把密码文件分发到slave1和slave2节点，如下：
```shell
scp authorized_keys spark@slave1:/home/spark/.ssh
scp authorized_keys spark@slave2:/home/spark/.ssh
```
传输完毕后，**需要在3台节点中设置authorized_keys读写权限**：
```shell
cd /home/spark/.ssh
chmod 400 authorized_keys #设置400就可以？不应该是600??
```

+ **验证免密码登录**

在各个节点中使用ssh命令，验证它们之间是否可以免密码登录：
```shell
ssh master
ssh slave1
ssh slave2
```

## 2.编译Spark源代码
由于实际环境较为复杂，这就需要我们根据实际情况编译Spark源代码，生成所需要的部署包。Spark可以通
过Maven和SBT两种方式进行编译，再通过 **make-distribution.sh** 脚本生成部署包。

### 2.1.配置Spark编译环境 

#### 2.1.1.安装Git
SBT编译使用，略。

#### 2.1.2.下载Spark源码
用户可以从Apache等网站下载Spark源代码，下载最新的版本Spark2.0.0（注：**现在最新是Spark3.0**
）,将下载好的源代码包Spark2.0.0.tgz上传到`/home/spark/work`目录下。在主节点上解压缩，并把解
压缩移动到Spark编译目录`/app/compile`中，详细命令如下：
```shell
cd /home/spark/work/
tar -xzvf spark2-2.0.0.tgz
mv spark-2.0.0 /app/compile/spark-2.0.0-src
```

### 2.2.使用Maven编译Spark
需要注意的是，**Spark2.0.0编译时强制要求Maven3.3.9及以上版本**，否则在编译开始检查会报错。

#### 2.2.1.下载Maven安装包
略

#### 2.2.2.解压Maven并配置参数
略

#### 2.2.3.编译代码 
编译过程需要保证编译机器联网状态，以保证Maven从网上下载其依赖包。另外，**编译前需要设置JVM内存
大小，否则在编译过程中，会由于默认内存小而出现内存溢出的错误**。编译执行脚本如下，**其中参数`-P`
表示激活依赖的程序及版本**，`-DskipTests`表示编译时跳过测试环节。
```shell
cp -r /app/compile/spark-2.0.0-src /app/compile/spark-2.0.0-mvn
cd /app/compile/spark-2.0.0-mvn
export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512m -XX:ReservedCodeCacheSize=512m"
mvn -Pyarn -Phadoop-2.7 -Pspark-ganglia-lgpl -Pkinesis-asl -Phive -DskipTests clean package
```
由于编译过程中要下载较多的依赖包，因此整个编译时间取决于网速，如果出现异常或者假死的情况，可以使
用`Ctrl+C`组合键中断，然后再重新编译。

### 2.3.使用SBT编译Spark
可以使用SBT进行Scala或Java程序编译。**在Spark程序中自带了SBT编译的脚本和相关的程序**，我们只
需要直接调用SBT进行编译即可，编译脚本如下：
```shell
cp -r /app/compile/spark-2.0.0-src /app/compile/spark-2.0.0-sbt
cd /app/compile/spark-2.0.0-sbt
build/sbt assembly -Pyarn -Phadoop-2.7 -Pspark-ganglia-lgpl -Pkinesis-asl -Phive
```
在编译的过程中 **经常出现假死的情况**，出现这种情况只需重新执行编译脚本，整个过程需要几个小时才
能编译完成。

### 2.4.生成Spark部署包
通过Maven或SBT编译Spark源代码后，**可以使用Spark源代码dev目录下一个生成部署包的脚本
make-distribution.sh**。
```shell
cd /app/compile/spark-2.0.0-mvn/
export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
./dev/make-distribution.sh -name custom-spark -tgz -Pspark -Phadoop-2.7 -Phive -Phive-thriftserver -Pyarn
```
生成的部署包位于根目录下，文件名为：spark-2.0.0-bin-custom-spark.tgz。

## 3.搭建Spark运行集群

### 3.1.修改配置文件
把编译生成的`spark-2.0.0-bin-custom-spark.tgz`文件上传到master节点。并把安装包移动到
`/home/spark/work`目录下，然后解压：
```shell
cd /home/spark/work/
tar -xzvf spark-2.0.0-bin-2.7.2.tgz
mv spark-2.0.0-bin-2.7.2 /app/spark/spark-2.0.0
```

#### 3.1.1.配置/etc/profile
使用`sudo vi /etc/profile`命令打开配置文件编辑：
```shell
export SPARK_HOME=/app/spark/spark-2.0.0
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
```

#### 3.1.2.配置conf/slaves
**该配置文件用于设置集群中运行Worker节点信息，该文件在`$SPARK_HOME/conf`目录** 下，使用
`sudo vi slaves`打开，在该配置文件中添加如下内容：
```ini
master
slave1
slave2
```
需要注意的是，**在master节点不仅运行Master进程，也运行Worker进程**。

#### 3.1.3.配置conf/spark-env.sh
该配置文件用于 **设置Spark运行环境**，默认在`$SPARK_HOME/conf`目录中没有 **`spark-env.sh`**
文件，需要通过复制或修改`spark-env.sh.template`进行创建。命令如下：
```shell
cd /app/spark/spark-2.0.0/conf
cp spark-env.sh.template spark-env.sh
sudo vi spark-env.sh
```
在文件中加入如下内容，**设置master节点为运行Master进程节点，通信端口为：7077，在每个节点中运行
Worker数量为：1，使用的核数为：1个、内存为：1024MB**。
```shell
#设置运行Master进程节点地址
export SPARK_MASTER_IP=master
#设置Master通信端口
export SPARK_MASTER_PORT=7077
#设置每个节点中运行Executor数量
export SPARK_EXECUTOR_INSTANCES=1
#设置每个节点中运行Worker数量
export SPARK_WORKER_INSTANCES=1
#每个Worker使用的核数
export SPARK_WORKER_CORES=1
#每个Worker使用的内存大小
export SPARK_WORKER_MEMORY=1024M
#Master的监控界面WebUI端口
export SPARK_MASTER_WEBUI_PORT=8080
export SPARK_CONF_DIR=/app/spark/spark-2.0.0/conf
```

#### 3.1.4.分发Spark程序 
配置完毕后进入master节点中`/app/spark`目录，使用scp命令把spark目录复制到slave1和slave2节点 
中。
```shell
cd /app/spark
scp -r spark-2.0.0 spark@slave1:/app/spark/
scp -r spark-2.0.0 spark@slave2:/app/spark/
```

### 3.2.启动Spark
用户可以 **使用两种方式启动Spark程序：一种是通过同时启动Master和所有的Worker进程，另一种是
Master和Worker分别启动**。

+ **第一种方法**

使用 **`start-all.sh`脚本** ，通过 **读取slaves配置文件** 信息来启动所有Worker。
```shell
cd /app/spark/spark-2.0.0/sbin
sh start-all.sh
```

+ **第二种方法**

先使用 **`start-master.sh`** 启动Master进程，然后使用 **`start-slaves.sh`** 启动Worker，
命令如下：
```shell
cd /app/spark/spark-2.0.0/sbin
sh start-master.sh
sh start-slaves.sh spark://master:7077
```

### 3.3.验证启动
启动完毕以后可以通过 **jps命令** 查看启动情况，在master节点应该同时启动Master和Worker进程，
在slave1和slave2节点中只启动Worker进程。用户还可以通过 **`netstat -nlt`命令** 查看master
节点 的网络情况，或者使用带有条件的端口命令 **`netstat -an|grep 8080`** 查看8080端口的使用
情况。

另外，用户可以在Spark监控界面的WEBUI进行查看。在浏览器中输入`http://master:8080`，可以查看
到Spark集群的信息。

## 4.搭建Spark实战开发环境 


