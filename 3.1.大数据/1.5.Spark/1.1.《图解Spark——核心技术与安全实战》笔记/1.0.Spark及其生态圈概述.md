Spark及其生态圈概述
================================================================================
## 1.什么是Spark
Spark是加州大学伯克利分样AMP实验室开发的通用大数据处理框架。围绕着Spark推出了Spark SQL、Spark
Streaming、MLlib、GraphX和SparkR等组件，这些组件逐渐形成大数据处理一站式解决平台。

Scala是使用 **Scala语言** 实现。

Spark的中文意思是：电光火石。官方提供 的数据表明：如果数据由磁盘读取，速度是Hadoop MapReduce
的10倍以上；如果数据从内存中读取，速度可以高达100多倍。**Spark相对于Hadoop有如此之快的计算速度
有数据本地性、调度优化和传输优化等原因，其中最主要是基于内存计算和引入DAG执行引擎**。
1. **Spark默认情况下迭代过程的数据保存到内存中，后续的远行作业利用这些结果进行计算，而Hadoop每
次计算结果都直接存储到磁盘中，在随后的计算中需要从磁盘中读取上次计算的结果。由于从内存读取数据时
间比磁盘读取时间低两个数量级，这就造成了Hadoop的运行速度较慢，这种情况在迭代计算中尤为明显**。
2. **由于较复杂的数据计算任务需要多个步骤才能实现，且步骤之间具有依赖性。对于这些步骤之间，Hadoop
需要借助Oozie等工具进行处理。而Spark在执行任务前，可以将这些步骤根据依赖关系形成DAG图（有向无环
图），任务执行可以按图索骥，不需要人工干预，从而优化了计算路径，大大减少了I/O读取操作**。

## 2.Spark与MapReduce比较 
1. **Spark把中间数据放在内存中，迭代运算效率高**。MapReduce中的计算结果是保存在磁盘上，这亲势
必会影响整体的运行速度，而Spark支持DAG图的分布式并行计算的编程框架，减少了迭代过程中数据的落地，
提高了处理效率。
2. **Spark的容错性高**。Spark引进了 **弹性分布式数据集**（Resilient Distributed Dataset，
**RDD**）的概念，它是分布在一组节点中的只读对象集合，这些集合是弹性的，**如果数据集一部分丢失，
则可以根据“血缘”对它们进行重建**。另外，**在RDD计算时可以通过 CheckPoint来实现容错，而CheckPoint
有两种方式，即CheckPoint Data和Logging The Updates**，用户可以控制采用哪种方式来实现容错。
3. **Spark更加通用**。不像Hadoop只提供了Map和Reduce两种操作，Spark提供的数据集操作类型有很
多种，大致分为 **转换操作** 和 **行动操作** 两大类。转换操作包括Map、Filter、FlatMap、Sample、
GroupByKey、ReduceByKey、Union、Join、Cogroup、MapValues、Sort和PartionBy等多种操作类
型，行支操作包括Collect、Reduce、Lookou和Save等操作类型。另外，各个处理节点之间的通信模型不再
像Hadoop只有Shuffle一种模式，用户可以命名、物化、控制中间结果的存储、分区等。



