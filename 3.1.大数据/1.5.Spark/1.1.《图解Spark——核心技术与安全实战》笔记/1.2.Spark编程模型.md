Spark编程模型
================================================================================
## 1.RDD简介
Spark编程模型是 **弹性分布式数据集（Resilient Distributed Dataset, RDD）**，它是MapReduce
模型的扩展和延伸，但它 **解决了MapReduce的缺陷：在并行计算阶段高效地进行数据共享**。

相比以前集群容错处理模型，如：MapReduce、Dryad，它们 **将计算转换为一个有向无环图（DAG）的任
务集合**。这使在这些模型中能够 **有效地恢复DAG中故障和慢节点执行的任务**，但是这些模型中除了文
件系统外没有提供其他的存储方式，这就导致了在 **网络上进行频繁的数据复制而造成I/O压力**。由于RDD
提供一种基于粗粒度变换（如：map、filter和join等）的接口，该接口会将相同的操作应用到多个数据集上，
**这就使得它们可以记录创建数据集的“血缘”（Lineage），而不需要存储真正的数据**，从而达到高效的容
错性。**当某个RDD分区丢失的时候，RDD记录有足够的信息来重新计算，而且只需要计算该分区，这样丢失的
数据可以很快地恢复，不需要昂贵的复制代价**。

**基于RDD机制实现了多类模型计算**，包括多个现有的集群编程模型。这些模型包括以下几方面的内容：
1. **迭代计算**： 目前最常见的工作方式。比如应用于图处理、数据优化以及机器学习中的算法。
2. ***交互式SQL查询**： Spark的RDD不仅拥有很多常见数据库引擎的特性，达到可观的性能，而且Spark
SQL中提供完善的容错机制，能够在短查询和长查询中很好地处理故障和慢节点。
3. **MapReduceRDD**：通过提供MapReduce的 **超集**，能够高效地执行MapReduce程序。
4. **流式数据处理**：当前的模型没有解决在大规模集群中频繁出现 **慢节点的问题**，同时对故障解决
办法有限，需要大量的复制或浪费很长的恢复时间。**Spark提出了离散数据流（D-Stream）** 来解决这样
的问题。`D-Streams`把流式计算的执行当作一系列短而确定的批量计算的序列，

