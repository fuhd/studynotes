安装与配置Spark集群
================================================================================
## 1.下载Spark安装包
本书将使用Spark官方编译好的软件包进行安装。**在少数情况下，才会自行编译Spark源码或在编译之前根
据具体业务修改部分源码**。
+ 进入版本选择页面后，选择2.3.1版本，并单击“spark-2.3.1-bin-hadoop2.7.tgz”链接。
+ 将下载好的Spark压缩包上传至第1台虚拟机的“home/admin/softwares/installations”目录下。

## 2.检查与准备集群环境 

### 2.1.虚拟机的配置项
下表列出了三台虚拟机的核心配置项：
> 三台虚拟机的系统均为CentOS6.8版本，其与CentOS7.x系列版本的使用方式大同小异。

| 配置项 | linux01 | linux02 | linux03 |
|:------|:--------|:--------|:--------|
| 内存 | 4GB | 2GB | 2GB |
| CPU内核数 | 2 | 1 | 1 |
| 硬盘 | 50GB（单分区）| 50GB（单分区）|50GB（单分区）|
| 网络适配器 | NAT | NAT | NAT |

### 2.2.配置虚拟机系统
在部署Spark集群之前，需要确保三台虚拟机的软件环境一致的。

#### 2.2.1.禁用SELinux
分布式框架在进行端口通信时，**SELinux有可能阻塞其中的通信，因此暂且将其禁用**。在CentOS6.8中，
使用如下命令编辑SELinux配置文件：
```shell
vi /etc/sysconfig/selinux00000
```
并将SELINUX选项改为：
```ini
SELINUX:disable
```

#### 2.2.2.关闭防火墙
防火墙也可以导致分布式框架之间的通信被拒绝，因此暂且关闭防火墙。**在CentOS6.8系统中**，使用如下
命令关闭iptables防火墙。
```shell
service iptables stop
chkconfig iptables off
```
> 在CentOS7.X系统中，用firewalld替代了iptables。关闭防火墙的命令如下：
>
> systemctl stop firewalld.service
>
> systemctl disable firewalld.service 

#### 2.2.3.配置网卡文件
在CentOS6.8中编辑网卡配置文件的命令如下：
```shell
vi /etc/sysconfig/network-scripts/ifcfg-eth0
```
编辑内容如下：
```ini
#当前机器的IP地址
IPADDR=192.168.216.20
#固定的子网掩码地址
NETMASK=255.255.255.0
#网关地址
GATEWAY=192.168.216.2
#首选DNS，设置为网关的IP地址即可
DNS1=192.168.216.2
#备选DNS，设置为如下即可
DNS2=8.8.8.8
#设置为静态配置IP地址模式 
BOOTPROTO=static
#是否开机自动启动网卡
ONBOOT=yes
```
编辑完后，重启网卡：
```shell
service network restart
```
其余两台虚拟机网卡配置过程与上述过程基本一致。

#### 2.2.4.修改IP地址与主机名的映射关系 
```shell
vi /etc/hosts
```
将其修改为如下内容：
```ini
192.16.216.20 linux01
192.16.216.21 linux02
192.16.216.22 linux03
```

#### 2.2.5.实现3台虚拟机之间的无秘钥访问 
以其中一台虚拟机上的操作为例，首先，执行以下命令生成秘钥：
```shell
ssh-keygen -t rsa
```
然后，分发公钥至3台虚拟机，命令如下：
```shell
ssh-copy-id linux01;
ssh-copy-id linux02;
ssh-copy-id linux03;
```

#### 2.2.6.关闭桌面GUI
开发者几乎不会在Linux虚拟机中使用桌面UI，而是全部通过命令行进行系统操作，并且开启桌面UI会额外地 
增加系统的开销。因此可以考虑关闭GUI。

首先，在CentOS中关闭GUI需要 **编辑inittab文件**：
```shell
vi /etc/inittab
```
然后，**根据inittab文件中的描述，将其中的id修改为“2”**。

> 在CentOS7.X版本的系统中，关闭GUI的命令如下：
>
> systemctl set-default multi-user.target

#### 2.2.7.配置sudo权限
为方便普通用户临时使用更高的权限来执行命令，**需要编辑sudo文件来对普通用户进行授权**。首先编辑
sudo文件，具体命令如下：
```shell
visudo
```
然后在文件中增加如下内容：
```shell
admin ALL=(ALL) NOPASSWD: ALL
```

#### 2.2.8.配置时间服务器
三台虚拟机的系统时间应该是一致的。所以最好的方法是：**将第一台虚拟机配置为时间服务器，其余两台虚
拟 机通过ntpdate命令同步第一台虚拟机的时间，并将ntpdate命令配置到crontab脚本中，以使每隔若干
分钟同步一次**。具体操作这里不再详述。

#### 2.2.9.配置Java环境变量
系统变量与用户变量的配置文件所在位置分别如下所示：
+ 系统变量： /etc/profile
+ 用户变量：~/.bashrc

配置内容如下：
```shell
JAVA_HOME=/home/admin/modules/jdk1.8.0.121
export PATH=$PATH:$JAVA_HOME/bin
```

### 2.3.配置Windows中的hosts文件
在Windows中，为了可以通过虚拟机的主机名来访问虚拟机，最好将虚拟机的主机名与IP地址的映射关系保存
在Windows的“C:\Windows\System32\drivers\etc\hosts“文件中，示例内容如下：
```ini
192.168.216.20  linux01
192.168.216.21  linux02
192.168.216.22  linux03
```

## 3.了解目前集群中已经部署的框架服务 
**在生产环境中，完全独立地使用Spark的场景极其罕见**。大多数情况下Spark会与Hadoop及其他相关框
架配合使用。下表列出了目前在虚拟机中已部署的框架服务。

| 框架类别 | Linux01 | Linux02 | Linux03 |
|:--------|:--------|:--------|:--------|
| HDFS(Hadoop2.7.2) | NameNode, DataNode | DateNode | DataNode, SecondaryNameNode |
| YARN-MR(Hadoop2.7.2)|NodeManger, JobHistoryServer | ResourceManager, NodeManger | NodeManger |
| Zookeeper(Zookeeper3.4.x)|QuorumPeerMain | QuorumPeerMain | QuorumPeerMain |

## 4.部署Spark集群
下面详细列举了Spark目前支持的部署模式：
+ **Local模式**：在本地部署单个Spark服务。
+ **Standalone模式**：Spark自带的任务调度模式（**国内常用**）。
+ **YARN模式**：Spark使用Hadoop的YARN组件来进行资源与任务的调度（**国内常用**）。
+ **Mesos模式**：Spark使用Mesos平台进行资源与任务的调度。
+ **Kubernates模式**：自Spark2.3.x版本之后才开始支持Kubernates模式。

> 在Spark2.3.x之前的版本中，仅支持其它四种模式。

### 4.1.实例1：基于Standalone模式部署Spark集群
**在中小规模的集群中，基于Standalone模式部署Spark集群十分常见。利用Standalone模式自带的任务、
资源调度引擎，无须依赖其他调度平台即可正常运行Spark，减少了部署时的麻烦**。

> 提示：
>
> 在部署时，会涉及两个新的概念：Master服务与Worker服务。
>
> Master服务所在的虚拟机被称为：Spark主节点。
>
> Worker服务所在的虚拟机被称为：Spark从节点。

#### 4.1.1.解压缩 
用以下命令解压缩刚才下载好的Spark安装包：
```shell
tar -zxf /home/admin/softwares/installations/spark-2.3.1-bin-hadoop2.7.tgz -C /home/admin/modules/
```

#### 4.1.2.查看默认的配置文件
1. 执行如下命令，进入Spark配置文件所在目录：
```shell
cd spark-2.3.1-bin-hadoop2.7/conf
```
2. 通过`ll`或`ls -l`命令查看目录下的文件列表。所有文件的后缀后都是”template“，这是官方默认的
文件命名方式。如果此时直接启动Spark，则这些文件内的属性不会生效。

#### 4.1.3.去掉默认配置文件的后缀名
**将文件名中的后缀名”.template“去掉**，命令如下：
```shell
for i in *.template; do mv ${i} ${i%.*}; done
```
> 提示
>
> 这里使用shell语法中的 **for循环**，对当前目录中的所有文件进行重命名操作。**其中”`${i%.*}`"
> 的含义是：从右边截取字符串到第1个“.”字符出现的位置，并删除该字符右边的全部内容，保留该字符右边
> 的全部内容**。

#### 4.1.4.修改默认配置

1. **修改slaves文件**
**该文件描述了Spark有哪些虚拟机负责启动Worker服务**。先删除“localhost”所在的哪一行内容，然后
添加主机名称 （IP地址），配置如下：
```shell
vi slaves
```
添加：
```shell
linux01
linux02
linux03
```
> 在添加主机名时，请确保每一个主机名独占一行。

2. **继续修改spark-env.sh文件**
添加如下配置：
```ini
SPARK_MASTER_HOST=linux01
SPARK_MASTER_PORT=7077
```

3. **将配置好的Spark安装文件分发到其他集群**
```shell
scp -r spark-2.3.1-bin-hadoop2.7/ linux02:/home/admin/modules/
scp -r spark-2.3.1-bin-hadoop2.7/ linux03:/home/admin/modules/
```

#### 4.1.5.启动并查看服务 

1. **执行并查看服务**
进入第一台虚拟机的Spark安装目录，启动所有Spark节点的相关服务。
```shell
sbin/start-all.sh
```

2. **访问 Web页面，查看Spark的运行状态**
可以通过Web页面来查看Spark的运行状态。在本例中，在浏览器中输入：`http://linux01:8080`。


### 4.2.实例2：部署Spark的历史服务———Spark History Server
在实际开发过程中，**如果需要查看已执行任务的详细日志信息，则需要在运行任务之前启动Spark History 
Server服务**。

#### 4.2.1.配置Spark历史服务 
1. **进入Spark安装目录下的conf目录：**
```shell
cd /home/admin/modules/spark-2.3.1-bin-hadoop2.7/conf/
```

2. **编辑spark-default.conf文件**
添加如下内容：
```ini
#是否开启任务历史记录
spark.eventLog.enabled true
#在此目录中，会为每一个任务创建一个子目录，并在该目录中记录对应任务的日志信息。由于任务是分布式
#运行的，所以多台虚拟机需要一个共同的存储目录，在此选择HDFS作为日志存储方案，以便统计管理日志信息。
spark.eventLog.dir hdfs://linux01:8020/spark-logs
```

3. **在HDFS中创建spark-logs目录**
```shell
#必须提前在HDFS中创建“hdfs://linux01:8020/spark-logs”目录，否则“spark.eventLog.dir"属性不会生效。
bin/hadoop fs -mkdir /spark-logs
```

4. **在spark-env.sh文件中添加配置**
```shell
#-Dspark.history.retainedApplications:内存中允许存放的日志个数。如果超过该值，则旧的日志会被从内存中移除，但它并不会被从HDFS中删除
#-Dspark.history.fs.logDirectory：告诉Spark History Server哪个目录记录着任务的日志信息。
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://linux01:8020/spark-logs"
```

5. **将配置好的文件分发到其他虚拟机**
```shell
scp spark-defaults.conf linux02:/home/admin/modules/spark-2.3.1-bin-hadoop2.7/conf/
scp spark-env.sh linux02:/home/admin/modules/spark2.3.1-bin-hadoop2.7/conf/

scp spark-defaults.conf linux03:/home/admin/modules/spark-2.3.1-bin-hadoop2.7/conf/
scp spark-env.sh linux03:/home/admin/modules/spark2.3.1-bin-hadoop2.7/conf/
```

#### 4.2.2.启动并查看Spark历史服务 

1. **启动Spark的Master、Worker服务**
在第一台虚拟机上，进入Spark的安装目录后，执行如下命令：
```shell
sbin/start-all.sh
```

2. **启动Spark历史服务**
在第一台虚拟机上，进入Spark的安装目录后，执行如下命令：
```shell
sbin/start-history-server.sh
```

3. **访问历史服务页面**
可通过浏览器访问：`http://linux01:4000/`来查看历史任务页面。

### 4.3.实例3：基于Standalone模式部署高可用的Master服务 
到目前为止，Spark集群已经部署完毕并可以执行程序了。但是还有一个小问题————Master服务并不是高可
用的（可能会发生单点故障）。**在复杂的生产环境中，为了保障程序顺利运行，最好部署高可用的Master服
务**。

#### 4.3.1.节点分配
下表中列出了集群中Master服务与Worker服务的分布情况：

| 主机 | 服务 |
|:----|:-----|
| linux01 | master、worker |
| linux02 | master、worker |
| linux03 | worker |

#### 4.3.2.停止服务 
配置文件修改完毕后，需要重启服务才能使新的配置生效，因此在修改配置文件之前，建议先停止正在运行的
Spark相关服务。
> 注意，后续操作也是在第一台虚拟机中进行的。

1. **停止Spark历史服务**
进入Spark安装的根目录后，执行如下命令：
```shell
sbin/stop-history-server.sh
```

2. **停止Master服务及Worker服务**
进入Spark安装根目录后，执行如下命令：
```shell
sbin/stop-all.sh
```

#### 4.3.3.修改配置文件

1. **确保Zookeeper已安装且正常运行**
略

2. **修改spark-env.sh文件**
编辑该文件，**注释** 掉之前配置好的 **SPARK_MASTER_HOST属性**，如下所示：
```shell
#SPARK_MASTER_HOST=linux01
```
接着在该文件中添加如下内容：
```shell
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=linux01:2181,linux02:2181,linux03:2181 -Dspark.deploy.zookeeper.dir=/spark"
```

#### 4.3.4.分发配置文件
将配置完成的 **spark-env.sh文件** 分发到其余两台虚拟机上，具体命令如下：
```shell
scp spark-env.sh linux02:/home/admin/modules/spark-2.3.1-bin-hadoop2.7/conf/
scp spark-env.sh linux03:/home/admin/modules/spark-2.3.1-bin-hadoop2.7/conf/
```

#### 4.3.5.启动Spark服务 
1. 在第一台虚拟机中，进入Spark安装根目录，执行如下命令：
```shell
sbin/start-all.sh
```

2. 在第二台虚拟机中，进入Spark安装根目录，执行如下命令：
```shell
sbin/start-master.sh 
```

#### 4.3.6.查看服务 
通过浏览器访问Spark的管理页面来查看服务是否正常启动了。

1. 访问第一台虚拟机中的Spark管理页面：`http://linux01:8080/`。请留意”**Status: ALIVE**“，
这表示该节点目前牌 **活跃状态**。
2. 访问第二台虚拟机中的Spark管理页面：`http://linux02:8080/`。请留意”**Status: STANDBY**“，
这表示该节点目前处于 **待命状态**。

### 4.4.实例4：基于YARN模式部署Spark集群
在基于YARN模式部署Spark集群之前，**请确保集群中已经部署且已运行YARN服务**。

#### 4.4.1修改Hadoop配置文件
如果Hadoop处于运行中，则修改配置后需要 **重启Hadoop** 相关服务来保障配置生效。
> 接下来的操作依然在第一台虚拟机中进行。

1. 进入Hadoop安装根目录，执行如下命令 **编辑yarn-site.xml文件**：
```shell
vi etc/hadoop/yarn-site.xml
```

2. 修改yarn-site.xml文件，添加刀下配置内容：
```xml
<!-- 是否启动一个线程检查每个任务正使用的物理内存量。如果任务使用的内存量超出分配值，则会被中断。默认是true -->
<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>
<!-- 是否启动一个线程检查每个任务正使用的虚拟内存量。如果任务使用的内存量超出分配值，则会被中断。默认是true -->
<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
```
**这两个配置是为了确保在基于YARN调度Spark任务时，不会因为刚开始分配的资源不足导致任务被拒绝，或
被强制中断**。
> 提示：在YARN刚开始为当前任务分配资源时，任务可能无法被分配到它所需的全部资源。但随着其他任务执行完毕，释放出来的资源可以逐步被当前任务使用。

#### 4.4.2.分发Hadoop配置文件
在配置修改完毕后，为了让集群中的配置同步，需要 **向其他虚拟机分发yarn-site.xml文件**。具体命令
如下：
```shell
scp etc/hadoop/yarn-site.xml linux02:/home/admin/modules/hadoop-2.7.2/etc/hadoop/
scp etc/hadoop/yarn-site.xml linux03:/home/admin/modules/hadoop-2.7.2/etc/hadoop/
```

#### 4.4.3.修改spark配置文件
为了让Spark能够发现Hadoop的相关配置，需要修改`spark-env.sh`配置文件。

1. 进入Spark安装目录后执行命令：
```shell
vi conf/spark-env.sh
```

2. 在文件中 **添加** 如下内容：
```ini
HADOOP_CONF_DIR=/home/admin/modules/hadoop-2.7.2/etc/hadoop
YARN_CONF_DIR=/home/admin/modules/hadoop-2.7.2/etc/hadoop
```
> 提示：YARN属于Hadoop中的一个组件。基于YARN部署的Spark集群在运行任务时，任务的资源是由YARN分配的。添加上述内容，可以令Spark发现并读取Hadoop的相关配置。

#### 4.4.4.分发Spark配置文件
配置完成后，分发spark-env.sh文件到其他虚拟机，具体命令如下：
```shell
scp conf/spark-env.sh linux02:/home/admin/modules/spark-2.3.1-bin-hadoop2.7/conf/
scp conf/spark-env.sh linux03:/home/admin/modules/spark-2.3.1-bin-hadoop2.7/conf/
```
完成上述步骤后，**重启Hadoop及Spark服务**。

#### 4.4.5.停止Hadoop服务 
进入Hadoop安装目录下停止Hadoop相关服务，命令如下：
```shell
sbin/stop-all.sh
```

#### 4.4.6.停止Spark服务
进入到Spark安装根目录下，停止Spark相关服务。

1. 在第一台虚拟机中，**关闭主Master服务及所有Worker服务**，具体命令如下：
```shell
sbin/stop-all.sh
```

2. 在第二台虚拟机中，**关闭备用的Master服务**，具体命令如下：
```shell
sbin/stop-master.sh
```

#### 4.4.7.启动Hadoop相关服务 
进入Hadoop安装目录，启动Hadoop相关服务。命令如下：
```shell
sbin/start-all.sh
```

#### 4.4.8.启动Spark相关服务 
进入Spark安装根目录，启动Spark相关服务。命令如下：
```shell
sbin/start-all.sh
```
至此，已经成功基本YARN模式部署了Spark集群。


### 4.5.Standalone模式与YARN模式的特点

1. ***Standalone模式的特点**
**该模式自带完整的资源及任务调度引擎**，可以独立地部署到任何一个集群中。

2. **YARN模式的特点**
**该模式依赖Hadoop的YARN组件**。如果集群中除Spark任务外还有其他运行任务大量依赖于YARN组件，
那么Spark任务所需的资源也应该由YARN来调度，这样可以方便、集中地管理集群的运算资源。






