第一个Spark程序
================================================================================
## 1.运行第1个Spark程序
在运行第1个Spark程序之前，需要先了解提交并运行程序的流程。
1. **在提交一个Spark程序时，用于提交该程序的窗口被称为Client（客户）端。提交后的程序会向Master
服务申请资源，并启动一个Driver进程，其中包含该程序的全部代码**。
2. **每个Worker服务会启动Executor进程，该进程负责与Driver进程建立RPC通信，用于接收并运行
Driver进程派发过来的任务，并将任务的执行状态持续反馈给Driver进程**。

### 1.1.实例5：基于Standalone模式运行第1个Spark程序
使用蒙特卡洛算法求Pi的值。

#### 1.1.1.提交程序 
进入Spark的安装目录，通过脚本提交程序：
```shell
bin/spark-submit --class org.apache.spark.examples.SparkPi
    --master spark://linux01:7077
    --executor-memory 1G
    --total-executor-cores 3
    /home/admin/modules/spark-2.3.1-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.1.jar
    100
```
+ 通过`--class`参数设置jar包中入口类的类名。
+ 通过`--master`参数设置Master服务的通信地址，包含机器的IP地址和端口号（可以使用IP地址来代替
主机名）。
+ 通过`--executor-memory`参数设置每个Executor进程的可用内存，如果任务运行时所使用的内存超过
该值，则会出现虚拟机内存溢出错误。
    > 提示：每一个Worker节点都可以创建若干个Executor进程来执行具体任务，即Executor是具体执行任务的进程。
+ 通过`--total-executor-cores`参数设置集群中一共可以调度多少个CPU内核（CPU Cores）来执行任
务。一个CPU内核在同一时间 内只能运行一个任务（Task）。在设置该参数时，需要结合机器硬件资源的配置
情况。例如，分配到的CPU内核越多，则任务的并行度越高，任务完成的时间也越短。
+ 第五行代码指定jar包的位置。该jar包中包含要运行的程序。
+ 最后一行，100为程序参数，该参数会被传递到主方法中。该参数用于指定随机“点”的个数。

> 提示：在执行该任务时，如果没有启动Spark的历史服务，则任务执行完毕后无法查看任务日志。

#### 1.1.2.观察与总结
在上述操作中，即使没有明显的报错信息，也应该去看一下任务日志。下面将通过Spark历史服务（History 
Server）页面来查看任务日志。
1. **查看Spark的历史任务**
在任务成功运行后，可以看到Pi的粗略值已经被计算出来了：Pi is roughly 3.1417199141719916。此
时来到Spark历史服务页面`http://linux01:4000`，可以看到该任务日志。
2. **查看HDFS中的日志文件**
访问`http://linux01:50070/explorer.html#/spark-logs`，来到HDFS的“/spark-logs”目录下，
可以看到，产生了一个对应刚才任务的文件。该文件夹目录下的文件记录着刚才任务的日志信息。

### 1.2.实例6：基于YARN模式运行第1个Spark程序

#### 1.2.1.确认Hadoop集群已正常运行
> 提示：
>
> Hadoop的Job History Server并不是Spark的History Server，这是完全不同的两个服务。
>
> + Hadoop的Job History Server，为YARN中运行的程序提供历史日志服务。
>
> + Spark的History Server，为所有Spark程序提供历史日志服务。
>
> 基于Spark on YARN运行的程序，也可以通过 Spark History Server查看日志。

#### 1.2.2.用YARN-Client方式提交程序 
1. 提交程序 
进入Spark的安装目录，使用YARN-Client方式提交程序。具体命令如下：
```shell
bin/spark-submit 
    --class org.apache.spark.examples.SparkPi
    --master yarn
    --deploy-mode client
    --executor-memory 1G
    --total-executor-cores 3
    /home/admin/modules/spark-2.3.1-bin-hadoop2.7/ecamples/jars/spark-examples_2.11-2.3.1.jar
    100
```
2. 查看任务进度
在提交程序之后，可以进入YARN的Web页面查看任务度。Web页面 地址如下：
http://linux02:8088/cluster/apps 

> 当前我使用的端口是：8099，这是YARN的HA方案配置:
> ```xml
> <property>
>     <name>yarn.resourcemanager.webapp.address.rm005</name>
>     <value>bigdata005:8099</value>
> </property>
> <property>
>     <name>yarn.resourcemanager.webapp.address.rm008</name>
>     <value>bigdata008:8099</value>
> </property>
> ```

> 提示：
>
> 基于该方式运行的程序，Client客户端会与Driver进程绑定在一起，如果用于提交程序的Client客户端退出，则意味着Driver进程退出，程序将无法继续执行。

#### 1.2.3.用YARN-Cluster方式提交程序 
1. 提交程序 
进入Spark的安装目录后，执行如下命令：
```shell
bin/spark-submit
    --class org.apache.spark.examples.SparkPi
    --master yarn
    --deploy-mode cluster
    --executor-memory 1G
    --total-executor-cores 3
    /home/admin/modules/spark-2.3.1-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.1.jar 
    100
```

2. 查看任务记录
此时并不能在控制台直接看到输出结果，只是看到程序在进行。当程序运行完毕后，前往YARN的Web页面，找
到本次程序的运行记录，然后依次单击“History”链接——>"Logs"链接找开日志，在日志最后一行可以看到已
经求出了Pi的值。
> 提示：
>
> 基于该方式运行的程序，Client客户端不会与Driver进程绑定在一起。如果用于提交程序的Client客户端退出，程序依然可以继续执行。这种方式可以保障集群内Executor进程和Driver进程通信更加稳定。

#### 1.2.4.总结
总结如下：
1. 如果现有集群中没有部署YARN集群，则使用Standalone模式提交程序。
2. 如果在现在的集群中已经部署了YARN集群，则使用YARN模式提交程序。当基于YARN模式提交程序时：
    + 在正式的生产环境中，推荐使用YARN-Cluster方式提交程序。
    + 在调试环境中，推荐使用YARN-Client方式提交程序。

### 1.3.提交Spark程序时的参数规范

| 参数 | 解释 | 可选值举例 |
|:----|:-----|:---------|
| `--class` | Spark程序中包含主函数的类 |  |
| `--master` | Spark程序运行的模式 | `local[*]`、`spark://linux01:7077`、`yarn` |
| `--deploy-mode` | Spark程序运行的方式 | `client`、`cluster` |
| `--driver-memory` | 每一个Driver进程的内存 | 符合集群内存配置即可。具体情况具体分析 |
| `--executor-memory` | 每一个Executor进程的可用内存 | 符合集群内存配置即可。具体情况具体分析 |
| `--executor-cores` | 每一个Executor进程的可用CPU核数 |  |
| `--total-executor-cores` | 所有Executor进程可用的总CPU核数 |  |
| `<application-jar>` | 程序对应的jar包路径 |  |
| `[application-arguments]` | 该程序主方法所接受的参数 |  |

> 说明：
>
> 如果`--master yarn`，那么Hadoop集群位置由环境变量 **HADOOP_CONF_DIR** 决定。

## 2.使用spark-shell编写并运行WordCount程序 
`spark-shell`是Spark自带的交互式编程工具，开发者可以使用该工具快速验证代码的执行效果。
> 目前Spark交互式编程仅支持两种语言——Scala与Python。

### 2.1.实例7：启动spark-shell

#### 2.1.1.在单个Master情况下启动spark-shell
进入Spark的安装目录后，执行如下命令：
```shell
#如果master没有启动，可以到spark安装目录的sbin目录下执行：sh start-master.sh
bin/spark-shell --master spark://linux01:7077 --executor-memory 1g --total-executor-cores 3
```

#### 2.1.2.在多个Master情况下启动spark-shell
对于高可用的Master服务，启动方式也很简单。进入Spark的安装目录后，执行如下命令：
```shell
bin/spark-shell --master spark://linux01:7077,linux02:7077 --executor-memory 1g --total-executor-cores 3
```
> 提示：只需要在“--master”参数中用逗号分隔多个Master服务的通信地址即可。

### 2.2.实例8：在spark-shell中编写WordCount程序

#### 2.2.1.准备单词本
创建一个文件名为“woords.txt"的文本文件，文件的内容是以”空格“分割的若干个英文单词。它可以有很多
行，举例如下：
```
dog cat hadoop spark
dog hadoop kafka kylin
cat kafka spark
```

#### 2.2.2.上传单词本到HDFS中
```shell
/home/admin/modules/hadoop-2.7.2/bin/hadoop fs -put words.txt  /
```

#### 2.2.3.编写 WordCount代码
1. 读取words.txt中的内容，单词以空格分割。每一个单词都记为出现了1次，这是一个map过程。
2. 将相同单词对应的数字”1“聚合在一起，这是一个reduce过程。
3. 将词频统计结果降序排列，并输出保存到HDFS的output目录中。

```scala
sc.textFile("hdfs://linux01:8020/words.txt")
    .flatMap(_.split(" "))
    .map((_, 1))
    .reduceByKey(_ + _, 1)
    .sortBy(_._2, false)
    .saveAsTextFile("hdfs://linux01:8020/output")
```

#### 2.2.4.查看HDFS中的统计结果
在代码运行后，通过 **HDFS的Web页面**可以看到在output目录中有一个文件（http://linux01:50070）。

## 3.使用IDEA编写并运行WordCount程序

### 3.1.实例9：准备开发环境并构建代码工程

#### 3.1.1.准备开发环境
安装Scala SDK、下载安装IDEA、在IDEA中配置Scala开发环境，这些都略过了。这里以 **scala2.11.12
与java8** 为例。

#### 3.1.2.构建工程
1. 新建一个空的Java工程（基于Maven构建）。
2. 配置Maven工程的pom.xml文件
```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.example</groupId>
    <artifactId>spark-demo</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>2.11.12</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.4.0</version>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.hadoop</groupId>
                    <artifactId>hadoop-client</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.scala-lang</groupId>
                    <artifactId>scala-library</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>2.7.7</version>
        </dependency>
        <dependency>
            <groupId>com.github.jobserver.api</groupId>
            <artifactId>jobserver-api</artifactId>
            <version>1.0.0-SNAPSHOT</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.4.0</version>
            <scope>provided</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.scala-tools</groupId>
                <artifactId>maven-scala-plugin</artifactId>
                <version>2.15.2</version>
                <executions>
                    <execution>
                        <id>scala-compile-first</id>
                        <goals>
                            <goal>compile</goal>
                        </goals>
                        <configuration>
                            <includes>
                                <include>*/*.scala</include>
                            </includes>
                        </configuration>
                    </execution>
                    <execution>
                        <id>scala-test-compile</id>
                        <goals>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>

    <repositories>
        <repository>
            <id>1</id>
            <name>MAVEN-CENTRE</name>
            <url>http://central.maven.org/maven2/</url>
        </repository>
    </repositories>
</project>
```
4. 构建Scala工程的目录结构
+ src
    - main
        + java
        + resources
        + scala
    - test
        + java
        + resources
        + scala

最后，在刚才新建 的”scala"目录上单击鼠标右键，在弹出菜单中 **选择“Mark Directory as” -> 
"Sources root"** 命令。

### 3.2.实例10：使用IDEA编写WordCount程序 
编写WordCount代码:
```scala
package chapter3                                                                //1

import org.apache.spark.{SparkConf, SparkContext}                               //3

object Chapter3_3_2 {                                                           //5
  def main(args: Array[String]): Unit = {                                       //6
    //初始化SparkConf对象，设置基本任务参数                         
    val conf = new SparkConf()                                                  //8
      .setMaster("yarn-cluster") //设置目标Master通信地址                          //9
      .setAppName("WC") //设置任务名称                                            //10
    //实例化SparkContext,Spark的对外接口负责用户与Spark内部的交互通信
    val sc = new SparkContext(conf)                                             //12
    //读取文件并进行单词统计
    sc.textFile("hdfs:/ns/user/maxcompute/users/admin/text/abc.txt")            //14
      .flatMap(_.split(" "))                                                    //15
      .map((_, 1))                                                              //16
      .reduceByKey(_ + _, 1)                                                    //17
      .saveAsTextFile("hdfs:/ns/user/maxcompute/users/admin/text/abc_result")   //18
    sc.stop()                                                                   //19
  }
}
```
+ 代码第14行，将HDFS中的文本内容读取到RDD中。RDD的全称为“Resilient Distributed DataSets”
，即 **弹性的分布式数据集**，可以简单地将其理解为一个数据集合。
+ 代码第15行，**flatMap为RDD转换操作**。
+ 代码第16行，进行map操作。将每个单词映射为“(word,1)”这样的 **二元组** 结构。
+ 代码第17行，进行reduceByKey操作。该操作会把所有相同单词对应的数字“1”累加在一起。**在执行操
作前，可以指定分区个数，这里指定为1个分区，即Spark会将结果输出第一个文件中**。
> 提示：在reduceByKey操作中默认的分区规则是，对象的hash值与分区个数进行求模，根据结果将该对象
> 分配到对应的分区当中。在本示例中，“word.hashcode() % 1 = 0”，即在1个分区的情况下，所有的单
> 词都会被分区到0号分区进行reduce操作。一个分区对应一个输出文件。
