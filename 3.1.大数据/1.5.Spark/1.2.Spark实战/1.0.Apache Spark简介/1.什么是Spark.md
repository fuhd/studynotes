什么是Spark
===================================================================================
**Apache Spark通常被定义为一个快速、通用的分布式计算平台**。Spark能高效地使用内存，执行同样的工作，
它比Hadoop's MapReduce快10 ~ 100倍。最重要的是，**Spark的创造者设法将用户从正在处理的计算机集群中
抽象出来，向用户呈现一组基于集合的API。使用Spark的集合感觉好像在使用本地Scala、Java和Python的集合，
但是Spark的集合实际上引用分布在许多节点的数据。这些集合的操作被转换为复杂的并行程序**。



