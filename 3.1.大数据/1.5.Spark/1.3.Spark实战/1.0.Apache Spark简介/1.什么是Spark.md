什么是Spark
===================================================================================
**Apache Spark通常被定义为一个快速、通用的分布式计算平台**。Spark能高效地使用内存，执行同样的工作，
它比Hadoop's MapReduce快10 ~ 100倍。最重要的是，**Spark的创造者设法将用户从正在处理的计算机集群中
抽象出来，向用户呈现一组基于集合的API。使用Spark的集合感觉好像在使用本地Scala、Java和Python的集合，
但是Spark的集合实际上引用分布在许多节点的数据。这些集合的操作被转换为复杂的并行程序**。

Spark最初是在 **Berkeley** 的AMPLab由Matei Zaharia设计的，他与他的导师Ion Stoica以及Reynold Xin、
Patrick Wendell、Andy Konwinski和Ali Ghodsi共同创立了Databricks。**虽然Spark是开源的，但是Databricks
是Apache Spark的主要力量，它贡献了Spark的75%以上的代码**。它还提供了Databricks Cloud——一种基于
Apache Spark的大数据分析的商业产品。

通过支持 **Python、Java、Scala和R语言，Spark可以向广泛的用户开放：传统上倾向于Python和R的科学界、
仍然广泛使用的Java社区以及使用越来越流行的Scala的人们**。Spark在Java虚拟机（JVM）上提供了函数式编
程。

最后，**Spark将批处理、实时流计算功能、类似SQL的结构化数据处理、图算法和机器学习等类似MapReduce
的功能整合在一个框架中。这使它成为大多数大数据处理需求的一站式服务平台**。

**Spark并没有考虑到在线事务处理（OLTP）**，它更适合在线分析处理（OLAP）：批处理作业和数据挖掘等
方面起作有。

## Spark革命
**Spark可以解决Hadoop能解决的相同难题，但Spark却更高效**。下面将讨论Hadoop的缺点，以及Spark如
何解决这些问题。Hadoop解决了分布式数据处理工作面临的3个主要问题。
+ **并行化**：如何同时执行计算的子集。
+ **分发**：如何分发数据。
+ **容错**：如何处理组件故障。

## MapReduce的缺点
虽然Hadoop是当今大数据革命的基础，并被积极使用和维护，**但它仍然有缺点，并且这些缺点主要涉及它的
MapReduce组件。MapReduce作业结果需要存储在HDFS中，才能被其他作业使用。因此，MapReduce对于迭
代算法本身是不利的**。

此外，**MapReduce分为map和reduce两个阶段，许多类型的问题使用这两个阶段解决都不适合**，因为将每
一个问题都分解为这两个阶段都是有困难的，API有时也很麻烦。

## Spark带来了什么有价值的东西
**Spark的核心概念是内存中的执行模式，可以将内存中的作业数据缓存，而不是像MapReduce一样从磁盘中取
出**。与MapReduce中的相同工作相比，Spark可以将作业的执行速度提高100倍，**它对迭代算法（如机器学
习、图算法和需要重用数据的其他类型的工作负载）最有效果**。

2014年10月，Daytona Gray Sort比赛中Spark表现出色，它在1406秒内对100TB数据进行排序并创造了世界纪
录。

### 1.Spark的易用性
Spark API比传统的MapReduce API更容易使用。以附录A中的经典字数示例作为MapReduce作业来实现，需要3
个类：设置作业的主类、Mapper和Reduce。每个类有10行左右。

以下是用Scala语言编写的同一个Spark程序所需要的：
```scala
val spark = SparkSession.builder().appName("Spark wordcount")
val file = spark.sparkContext.textFile("hdfs://......")
val counts = file.flatMap(line => line.split(" ")).map(word => (word, 1)).countByKey()
counts.saveAsTextFile("hdfs://......")
```
**Spark支持Scala、Java、Python和R编程语言**，因此可以让更广泛的受众访问。**尽管支持Java，但Spark
可以利用Scala的多功能性、灵活性和函数式编程概念，这些概念更适合用于数据分析**。数据科学家都和科学
界人士广泛使用Python和R，这些用户的人数与Java和Scala开发人员的人数不相上下。

此外，**Spark shell**（读取 - 求值 - 输出 - 循环(REPL)）还提供了一个 **可用于实验和想法测试的交互式控制
台**。没有必要只是为了发现一些东西是否奏效而编译和部署（再次）。REPL甚至可用于在全套数据上启动作业。

**Spark可以运行在以下几种类型的集群上，Spark standalone集群、Hadoop的YARN（另一个资源协调者）和
Mesos**。这给了它更多的灵活性，并使其可供更大的用户群体访问。

### 2.Spark作为一个统一的平台
Spark的一个重要方面是将Hadoop生态系统中许多工具的功能组合成一个统一的平台。执行模型是足够通用的，
单个框架 **可以用于流数据处理、机器学习、类SQL操作、图计算和批处理**。许多角色可以在同一平台上协同
工作，这有助于弥补程序员、数据工程师和数据科学家之间的差距。Spark提供的函数列表还在继续增长。

### 3.Spark反面模式
**Spark不适合用于共享数据的异步更新（如在线事务处理），因为它是通过批处理分析创建的。（Spark Streaming
只是在时间窗口中应用于数据的批量分析。）**







