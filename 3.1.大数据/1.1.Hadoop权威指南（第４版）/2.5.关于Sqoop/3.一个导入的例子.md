一个导入的例子
================================================================================
在安装了Sqoop之后，**可以用它将数据导入（import）到Hadoop**。本章的所有示例中，我们都使用了支持
多种平台的易用数据库系统 **MySQL** 作为外部数据源。

假定我们已经创建好了数据库，现在我们再创建了一个名为widgets的新表。在本章后面的例子中我们都将使用
这个虚构的产品数据库与表。

在进行下一步之前，还需要 **下载MySQL的JDBC驱动的JAR文件，并将其存放在Sqoop的lib目录下**，从而使
之被添加到Sqoop的类路径中。

现在让我们 **使用Sqoop将这个表导入HDFS**：
```shell
$ sqoop import --connect jdbc:mysql://localhost/hadoopguide --table widgets -m 1
```
**Sqoop的import工具会运行一个MapReduce作业，该作业会连接MySQL数据库并读取表中的数据**。默认情况下，
该作业会并行使用 **4个map任务** 来加速导入过程。**每个任务都会将其所导入的数据写到一个单独的文件**，
但所有4个文件都位于同一个目录中。在本例中，由于我们知道只有很少的几行数据，**因此指定Sqoop只使用一个
map任务（-m 1）**，这样我们就只得到一个保存在HDFS中的文件。
```
本例中，使用的连接字符串（jdbc:mysql://localhost/hadoopguide）表明需要从本地机器上的数据库中读
取数据。如果使用了分布式Hadoop集群，则连接字符串中不能使用localhost，否则与数据库不在同一台机器上运行
的map任务都将无法连接到数据库。即使是从数据库服务器所在主机上运行Sqoop，也需要为数据库服务器指定完整
的主机名。
```
**在默认情况下，Sqoop会将我们导入的数据保存为逗号分隔的文本文件。如果导入数据的字段内容中存在分隔符，
我们可以另外指定分隔符、字段包围字符和转义字符。使用命令行参数可以指定分隔符、文件格式、压缩方式以及对
导入过程进行更细粒度的控制**。

### 文本和二进制文件格式
**Sqoop可以将数据导入成几种不同的格式。文本文件（默认）是一种人类可读的数据表示形式，并且是平台独立和
最简单的数据格式**。但文本文件不能保存二进制字段（例如数据库中类型为VARBINARY的列），并且在区分null值
和字符串null时可能会出现问题（尽管使用--null-string选项可以控制空值的表示方式）。

为了处理这些情况，**应该使用Sqoop的SequenceFile格式、Avro格式或Parquet文件。这些二进制格式能够为
导入的数据提供最精确的表示方式，同时还允许对数据进行压缩，并支持MapReduce并行处理同一文件的不同部分**。
然而，**Sqoop的目前版本还不能将Avro或SequenceFile文件加载到Hive中（尽管可以手动地Avro数据文件加载
到Hive中，而Parquet则可以通过Sqoop直接加载到Hive中）**。SequenceFile文件格式的另一个缺点是它只支持
Java语言。而Avro和Parquet数据文件却可以被很多种语言处理。





































ddd
