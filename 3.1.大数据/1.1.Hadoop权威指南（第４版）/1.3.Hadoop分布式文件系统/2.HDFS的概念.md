HDFS的概念
================================================================================
## 数据块
每个磁盘都有默认的数据块大小，这是磁盘进行数据读/写的最小单位。构建于单个磁盘之上的文件系统通过磁
盘块来管理该文件系统中的块，该文件系统块的大小可以是磁盘块的整数倍。**文件系统块一般为几千字节，
而磁盘块一般为512字节**。

**HDFS同样也有块（block）的概念，但是大得多，默认为128MB**。与单一磁盘上的文件系统相似，**HDFS
上的文件也被划分为块大小的多个分块（chunk），作为独立的存储单元**。但与面向单一磁盘的文件系统不
同的是，**HDFS中小于一个块大小的文件不会占据整个块的空间**（例如，当一个1MB的文件存储在一个128
MB的块中时，文件只使用1MB的磁盘空间，而不是128MB）。
```
HDFS中的块为什么这么大？
HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的
时间。因而，传输一个由多个块组成的大文件的时间取决于磁盘传输速率。

我们来做一个速算，如果寻址时间约为10ms，传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为
100MB。默认的块大小实际为128MB，但是很多情况下HDFS安装时使用更大的块。以后随着新一代磁盘驱动器传输速率的提升，块的
大小会被设计得更大。

但是这个参数也不会设置得过大。MapReduce中的map任务通常一次只处理一个块中的数据，因此如果任务数太少（少于集群中的节
点数量），作业的运行速度就会比较慢。
```
对分布式文件系统中的块进行抽象会带来很多好处。第一个最明显的好处是，**一个文件的大小可以大于网络
中任意一个磁盘的容量**。文件的所有块并不需要存储在同一个磁盘上，因此它们可以利用集群上的任意一个
磁盘进行存储。

第二个好处是，**使用抽象块而不是整个文件作为存储单元，大大简化了存储子系统的设计**。简化是所有系
统的目标，但是这对于故障各类繁多的分布式系统来说尤为重要。将存储子系统的对象设置成块，可简化存储
管理（**由于块的大小是固定的，因此计算单个磁盘能存储多少个块就相对容易**）。同时也消除了对元数据
的顾虑（**块只是要存储的大块数据，而文件的元数据，如权限信息，并不需要与块一同存储，这样一来，其
他系统就可以单独管理这些元数据**）。

不仅如此，**块还非常适合用于数据备份** 进而提供数据容错能力和提高可用性。将每个块复制到少数几个
物理上相互独立的机器上（**默认为3个**），可以确保在块、磁盘或机器发生故障后数据不会丢失。如果发
现一个块不可用，系统会从其它地方读取另一个复本，而这个过程对用户是透明的。**一个因损坏或机器故障
而丢失的块可以从其他候选地点复制到另一台可以正常运行的机器上，以保证复本的数量回到正常水平**。

与磁盘文件系统相似，**HDFS中fsck指令可以显示块信息**。例如，执行以下命令将列出文件系统中各个文
件由哪些块构成。
```shell
$ hdfs fsck / -files -blocks
```

## namenode和datanode
HDFS集群有 **两类节点** 以管理节点/工作节点模型运行，**即一个namenode（管理节点）和多个datanode
（工作节点）。namenode管理文件系统的命名空间。它维护着文件系统树及整棵树内所有的文件和目录**。这
些信息以 **两个文件** 形式永久保存在 **本地磁盘** 上：**命名空间镜像文件和编辑日志文件**。
namenode也记录着每个文件中各个块所在的数据节点信息，但它并不永久保存块的位置信息，因为这些信息会
在系统启动时根据数据节点信息重建。

客户端（client）代表用户通过与namenode和datanode交互来访问整个文件系统。客户端提供一个类似于
POSIX（可移植操作系统界面）的文件系统接口，因此用户在编程时无需知道namenode和datanode也可实现
其功能。

**datanode是文件系统的工作节点。它们根据需要存储并检索数据块（受客户端或namenode调度），并且定
期向namenode发送它们所存储的块的列表**。

**没有namenode，文件系统将无法使用。事实上，如果运行namenode服务的机器毁坏，文件系统上所有的
文件将会丢失，因为我们不知道如何根据datanode的块重建文件**。因此，**对namenode实现容错非常重
要，Hadoop为此提供两种机制**。

**第一种机制是备份那些组成文件系统元数据持久状态的文件**。Hadoop可以通过配置使namenode在 **多
个文件系统** 上保存元数据的持久状态。这些写操作是实时同步的，且是原子操作。一般的配置是，**将持
久状态写入本地磁盘的同时，写入一个远程挂载的网络文件系统（NFS）**。

另一种可行的方法是 **运行一个辅助namenode**，但它不能被用作namenode。这个辅助namenode的重要
作用是 **定期合并编程日志与命名空间镜像，以防止编辑日志过大。这个辅助namenode一般在另一台单独的
物理计算机上运行**，因为他需要占用大量CPU时间，并且需要与namenode一样多的内存来执行合并操作。
**它会保存合并后的命名空间镜像的副本，并在namenode发生故障时启用**。但是，**辅助namenode保存
的状态总是滞后于主节点，所以在主节点全部失效时，难免会丢失部分数据**。在这种情况下，一般把存储在
NFS上的namenode元数据复制到辅助namenode并作为新的主namenode运行（注意，**也可以运行热备份
namenode代替运行辅助namenode**）。

## 块缓存
通常datanode从磁盘中读取块，**但对于访问频繁的文件，其对应的块可能被显式地缓存在datanode的内存
中，以堆外块缓存**（`off-heap block cache`）的形式存在。 **默认情况下，一个块仅缓存在一个
datanode的内存中**，当然可以针对每个文件配置datanode的数量。**作业调度器（用于MapRduce、Spark
和其他框架的）通过在缓存块的datanode上运行任务，可以利用块缓存的优势提高读操作的性能**。

用户或应用通过在 **缓存池（cache pool）** 中增加一个 **cache directive** 来告诉namenode
需要缓存哪些文件及存多久。**缓存池是一个用于管理缓存权限和资源使用的管理性分组**。

## 联邦HDFS
**namenode在内存中保存文件系统中每个文件和每个数据块的引用关系**，这意味着对于一个拥有大量文件
的超大集群来说，**内存将成为限制系统横向扩展的瓶颈**。在 **2.x** 发行版本系列中引入的 **联邦
HDFS允许系统通过添加namenode实现扩展，其中每个namenode管理文件系统命名空间中的一部分**。

**在联邦环境下，每个namenode维护一个命名空间巻（namenode volume），由命名空间的元数据和一个
数据块池（block pool）组成，数据块池包含该命名空间下文件的所有数据块。命名空间卷之间是相互独立
的，两两之间并不相互通信，甚至其中一个namenode的失效也不会影响由其他namenode维护的命名空间的可
用性。数据块池不再进行切分，因此集群中的datanode需要注册到每个namenode，并且存储着来自多个数据
块池中的数据块**。

**要想访问联邦HDFS集群，客户端需要使用客户端挂载数据表将文件路径映射到namenode。该功能可以通过
ViewFileSystem和`viewfs://URI`进行配置和管理**。

## HDFS的高可用性
通过联合使用在多个文件系统中备份namenode的元数据和通过备用namenode创建监测点能防止数据丢失，但
是依旧无法实现文件系统的高可用性。namenode依旧存在 **单点失效**（`SOOF`,`single point of failure`）
问题。如果namenode失效了，那么所有的客户端，包括MapRduce作业，均无法读、写或列举（list）文件，
**因为namenode是唯一存储元数据与文件到数据块映射的地方**。在这一情况下，Hadoop系统无法提供服务
直到有新的namenode上线。

在这样的情况下，要想从一个失效的namenode恢复，系统管理员得启动一个拥有文件系统元数据副本的新的
namenode，并配置datanode和客户端以便使用这个新的namenode。**新的namenode直到满足以下情形才
能响应服务**：
1. **将命令空间的映像导入内存中**；
2. **重演编辑日志**；
3. **接收到足够多的来自datanode的数据块报告并退出安全模式。对于一个大型并拥有大量文件和数据块的
集群，namenode的冷启动需要30分钟，甚至更长时间**。

系统恢复时间太长，也会影响到日常维护。事实上，预期外的namenode失效出现概率很低，所以在现实中，计
划内的系统失败时间实际更为重要。

**Hadoop2针对上述问题增加了对HDFS高可用性（HA）的支持。在这一实现中，配置了一对活动-备用（active-standby）
namenode。当活动namenode失效，备用namenode就会接管它的任务并开始服务于来自客户端的请求，不会
有任何明显中断**。实现这一目标需要在架构上做如下修改：
+ **namenode之间需要通过高可用共享存储实现编辑日志的共享。当备用namenode接管工作之后，它将通
读共享编辑日志直到末尾，以实现与活动namenode的状态同步，并继续读取由活动namenode写入的新条目**。
+ **datanode需要同时向两个namenode发送数据块处理报告**，因为数据块的映射信息存储在namenode
的内存中，而非磁盘。
+ 客户端需要使用 **特定的机制** 来处理namenode的失效问题，这一机制对用户是透明的。
+ 辅助namenode的角色被备用namenode所包含，**备用namenode为活动的namenode命名空间设置周期性
检查点**。

可以从 **两种高可用性共享存储日志节点（journal node）的形式运行，每一次编辑必须写入多数日志节
点。**典型的，有三个journal节点，所以系统能够忍受其中任何一个的丢失**。这种安排与ZooKeeper的
工作方式类似，当然必须认识到，**QJM的实现并没有使用ZooKeeper**。（然而，值得注意的是，**HDFS
HA在选取活动的namenode时确实使用了ZooKeeper技术**）。

在活动namenode失效之后，**备用namenode能够快速（几十秒时间）实现任务接管**，因为最新的状态存
储在内存中：包括最新的编辑日志条目和最新的数据块映射信息。**实际观察到的失效时间略长一点（需要1分
钟左右），这是因为系统需要保守确定活动namenode是否真的失效了**。

在活动namenode失效且备用namenode也失效的情况下，当然这类情况发生的概率非常低，管理员依旧可以声
明一个备用namenode并实现冷启动。

### 故障切换与规避
系统中有一个称为 **故障转移控制器**（failover controller）的新实体，**管理者将活动namenode
转移为备用namenode的转换过程**。有多种故障转移控制器，但默认的一种是 **使用了ZooKeeper来确保
有且仅有一个活动namenode**。每一个namenode运行着一个轻量级的故障转移控制器，其工作就是监视宿主
namenode是否失效（通过一个简单的心跳机制实现）并在namenode失效时进行故障切换。

管理员也可以手动发起故障转移，例如在进行日常维护时。这称为“平稳的故障转换”（graceful failover），
因为故障转移控制器可以组织两个namenode有序地切换角色。

**但在非平稳故障转移的情况下**，无法确切知道失效namenode是否已经停止运行。例如，在网速非常慢或
者网络被分割的情况下，同样也可能激发故障转移，但是先前的活动namenode依然运行着并且依旧是活动
namenode。**高可用实现做了更进一步的优化，以确保先前活动的namenode不会执行危害系统并导致系统
崩溃的操作，该方法称为“规避”（fencing）**。

**同一时间QJM仅允许一个namenode向编程日志中写入数据。然而，对于先前的活动namenode而言，仍有可
能响应并处理客户过时的读请求，因此，设置一个SSH规避命令用于杀死namenode的进程是一个好主意。当使
用NFS过滤器实现共享编辑日志时，由于不可能同一时间只允许一个namenode写入数据（这也是为什么推荐
QJM的原因），因此需要更有力的规避方法**。规避机制包括：撤销namenode访问共享存储目录的权限（通常
使用供应商指定的NFS命令）、通过远程管理命令屏蔽相应的网络端口。诉诸的最后手段是，先前活动namenode
可以通过一个相当形象的称为“一枪爆头”的技术进行规避，该方法主要通过一个特定的供电单元对相应主机进
行断电操作。

**客户端的故障转移通过客户端类库实现透明处理**。最简单的实现是通过客户端的配置文件实现故障转移的
控制。**HDFS URI使用一个逻辑主机名，该主机名映射到一对namenode地址**（在配置文件中设置），客户
端类库会访问每一个namenode地址直至处理完成。
