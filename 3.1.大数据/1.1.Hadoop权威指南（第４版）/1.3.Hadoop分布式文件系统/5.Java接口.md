Java接口
================================================================================
我们要深入探索Hadoop的 **Filesystem类：它是与Hadoop的某一文件系统进行交互的API。虽然我们主要
聚焦于HDFS实例，即DistributedFileSystem**，但总体来说，还是应该集成Filesystem抽象类，并编
写代码，使其在不同文件系统中可移植。

## 1.从Hadoop URL读取数据
**要从Hadoop文件系统读取文件，最简单的方法是使用java.net.URL对象打开数据流，从中读取数据**。
具体格式如下：
```java
package com.hadoop.hdfs;

import org.apache.hadoop.fs.FsUrlStreamHandlerFactory;
import org.apache.hadoop.io.IOUtils;
import java.io.IOException;
import java.io.InputStream;
import java.net.URL;

public class TestHadoopURL {
    static {
        //注释A
        URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
    }
    public static void main(String[] args) {
        InputStream in = null;
        try {
            in = new URL("hdfs://localhost/quangle.txt").openStream();
            IOUtils.copyBytes(in, System.out, 4096, false);
        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            IOUtils.closeStream(in);
        }
    }
}
```
**让Java程序能够识别Hadoop的hdfs URL方案需要一些额外的工作。这里采用的方法是通过
FsUrlStreamHandlerFactory实例调用java.net.URL对象的setURLStreamHandlerFactory()方法**，
见上面的代码注释A。**每个Java虚拟机只能调用一次这个方法，因此通常在静态方法中调用。这个限制意味
着如果程序的其他组件（如不受你控制的第三方组件）已经声明一个FsUrlStreamHandlerFactory实例，
你将无法使用这种方法从Hadoop中读取数据**。

上面的范例展示的程序以标准输出方法显示Hadoop文件系统中的文件，类似于Unix中的cat命令。

**我们可以调用Hadoop中简洁的IOUtils类，并在finally子句中关闭数据流，同时也可以在输入流和输出
流之间复制数据（本例中为System.out）。copyBytes方法的最后两个参数，第一个设置用于复制的缓冲区
大小，第二个设置复制结束后是否关闭数据流。这里我们选择自行关闭输入流（自己编码来控制）**。

## 2.通过FileSystem API读取数据
**有时根本不可能在应用中设置FsUrlStreamHandlerFactory实例。在这种情况下，我们需要用
Filesystem API来打开一个文件的输入流**。

Hadoop文件系统中通过 **Hadoop Path对象**（而非`java.io.File`对象，因为它的语义与本地文件系
统联系太紧密）**来代表文件**。可以将路径视为一个Hadoop文件系统URL，
如`hdfs://localhost/user/fuhd/quangle.txt`。

FileSystem是一个通用的文件系统API，所以第一步是 **检索我们需要使用的文件系统实例，这里是HDFS。
获取FileSystem实例有下面这几个静态工厂方法**：
```java
public static FileSystem get(Configuration conf) throws IOException
public static FileSystem get(URI uri, Configuration conf) throws IOException
public static FileSystem get(URI uri, Configuration conf, String user) throws IOException
```
**Configuration对象** 封装了客户端或服务器的配置，通过设置 **配置文件读取类路径** 来实现（如：
`cet/hadoop/core-site.xml`）。**第一个方法返回的是默认文件系统（在core-site.xml中指定的，
如果没有指定，则使用默认的本地文件系统）。第二个方法通过给定的URI方案和权限来确定要使用的文件系统，
如果给定URI中没有指定方案，则返回默认文件系统。第三，作为给定用户来访问文件系统，对安全来说是至
关重要**。

在某些情况下，你可能希望 **获取本地文件系统的运行实例** ，此时你可以使用 **getLocal()** 方法
很方便地获取。
```java
public static LocalFileSystem getLocal(Configuration conf) throws IOException
```
有了FileSystem实例之后，我们 **调用open()函数来获取文件的输入流**：
```java
public FSDataInputStream open(Path f) throws IOException
public abstract FSDataInputStream open(Path f, int bufferSize) throws IOException
```
**第一个方法使用默认的缓冲区大小4KB**。

最后，我们重写上面的示例，得到如下示例。
```java
package com.hadoop.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

import java.io.IOException;
import java.io.InputStream;
import java.net.URI;

public class FileSystemCat {
    public static void main(String[] args) throws IOException {
        String uri = "hdfs://localhost/quangle2.txt";
        //获取FileSystem的实例
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(uri), conf);
        InputStream in = null;
        try {
            //获取文件输入流
            in = fs.open(new Path(uri));
            IOUtils.copyBytes(in, System.out, 4096, false);
        } finally {
            IOUtils.closeStream(in);
        }
    }
}
```

### FSDataInputStream对象
实际上，**FileSystem对象中的open方法返回的是FSDataInputStream对象**，而不是标准的`java.io`
类对象。**这个类是继承了java.io.DataInputStream的一个特殊类，并支持随机访问，由此可以从流的
任意位置读取数据**。
```java
package org.apache.hadoop.fs;

public class FSDataInputStream extends DataInputStream implements Seekable, PositionedReadable {
    //implementation elided
}
```
**Seekable接口支持在文件中找到指定位置，并提供一个查询当前位置相对于文件起始位置偏移量（getPos()）
的查询方法**：
```java
public interface Seekable {
    void seek(long pos) throws IOException;
    long getPos() throws IOException;
}
```
调用`seek()`来定位大于文件长度的位置会引发IOException异常。与java.io.InputStream的`skip()`
不同，**seek()可以移到文件中任意一个绝对位置，skip()则只能相对于当前位置定位到另一个新位置**。

下面的示例是对上面示例的再扩展，**它将一个文件写入标准输出两次：在一次写完之后，定位到文件的起始
位置再次以流方式读取该文件并输出**。
```java
package com.hadoop.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

import java.io.IOException;
import java.net.URI;

public class FileSystemDoubleCat {
    public static void main(String[] args) throws IOException {
        String uri = "hdfs://localhost/quangle2.txt";
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(uri), conf);
        FSDataInputStream in = null;
        try {
            in = fs.open(new Path(uri));
            IOUtils.copyBytes(in, System.out, 4096, false);
            //回到文件开始处
            in.seek(0);
            IOUtils.copyBytes(in, System.out, 4096, false);
        } finally {
            IOUtils.closeStream(in);
        }
    }
}
```
**FSDataInputStream类也实现了PositionedReadable接口，从一个指定偏移量处读取文件的一部分**：
```java
public interface PositionedReadable {
    public int read(long position, byte[] buffer, int offset, int length) throws IOException;
    public void readFully(long position, byte[] buffer, int offset, int length) throws IOException;
    public void readFully(long position, byte[] buffer) throws IOException;
}
```
**read()方法从文件的指定position处读取至多为length字节的数据并存入缓冲区buffer的指定偏移量
offset处。返回值是实际读到的字节数：调用者需要检查这个值，它有可能小于指定的length长度**。

`readFully()`方法将指定length长度的字节数数据读取到buffer中，除非已经读到文件末尾，这种情况
下将抛出EOFException异常。
```
read()方法与readFully()方法的区别？

read()方法实质是读取流上的字节直到流上没有字节为止，如果当声明的字节数组长度大于流上的数据长度时就提前返回，而
readFully()方法是读取流上指定长度的字节数组，也就是说如果声明了长度为len的字节数组，readFully()方法只有读取len长度
个字节的时候才返回，否则阻塞等待，如果超时，则会抛出异常 EOFException

那么当发送了长度为len的字节，那么为什么用read方法用户收不全呢，揪其原因发现消息在网络中传输是没那么理想的，我们发的那部
分字节数组在传送过程中可能在接受信息方的缓存当中或者在传输线路，极端情况下可能在发送方的缓存当中，这样就不在流上，所以
read方法提前返回了，这样就造成了各种错误
```
所有这些方法会保留文件当前偏移量，并且是线程安全的（**FSDataInputStream并不是为并发访问设计的，
因此最好为此新建多个实例**），因此它们提供了在读取文件的主体时，访问文件其他部分（可能是元数据）
的便利方法。

最后务必牢记，**seek()方法是一个相对高开销的操作，需要慎重使用**。建议用流数据来构建应用的访问
模式（比如使用MapReduce），而非执行大量`seek()`方法。

## 3.写入数据
**FileSystem类有一系列新建文件的方法。最简单的方法是给准备建的文件指定一个Path对象，然后返回一
个用于写入数据的输出流**：
```java
public FSDataInputStream create(Path f) throws IOException
```
此方法有 **多个重载版本**，允许我们指定 **是否需要强制覆盖现有文件、文件备份数量、写入文件时所用
缓冲区大小、文件块大小以及文件权限**。
```
说明：

create()方法能够为需要写入且当前不存在的文件创建父目录。尽管这样很方便，但有时并不希望这样。如果希望父目录不存在就导致文
件写入失败，则应该先调用exists()方法检查父目录是否存在。另一种方案是使用FileContext，允许你可以控制是否创建父目录。
```
还有一个重载方法 **有Progressable类型的入参，用于传递回调接口**，如此一来，**可以把数据写入
datanode的进度通知给应用**：
```java
package org.apache.hadoop.util;

public interface Progressable {
    public void progress();
}
```
另一种新建文件的方法是 **使用append()方法在一个现有文件末尾追加数据**（还有其他一些重载版本）：
```java
public FSDataInputStream append(Path f) throws IOException
```
这样的追加操作允许一个writer打开文件后 **在访问该文件的最后偏移量处追加数据**。有了这个API，某
些应用可以 **创建无边界文件**，例如，应用可以在关闭日志文件之后继续追加日志，该追加操作是可选的，
并非所有Hadoop文件系统都实现了该操作。例如，**HDFS支持追加**，但S3文件系统不支持。

下面的范例显示了如何将本地文件复制到Hadoop文件系统。**每次Hadoop调用progress()方法时，也就是
每次将64KB数据包写入datanode管线后，打印一个时间点来显示整个运行过程**。注意，这个操作并不是通
过API实现的，因此Hadoop后续版本能否执行该操作，取决于该版本是否修改过上述操作。API只是让你知道
“正在发生什么事情”。
```java
package com.hadoop.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import java.io.*;
import java.net.URI;

public class FileCopyWithProgress {
    public static void main(String[] args) throws IOException {
        String localSrc = "/home/fuhd/input/docs/hs_err.log";
        String dst = "hdfs://localhost/hs_err.log";
        InputStream in = new BufferedInputStream(new FileInputStream(localSrc));
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(dst), conf);
        //这里使用了Progressable对象作为参数
        OutputStream out = fs.create(new Path(dst), () -> {
            System.out.print(".");
        });
        //最后一个参数为true，表示由API关闭输入流与输入流，不用自己写代码处理
        IOUtils.copyBytes(in, out, 4096, true);
    }
}
```
目前，**其他Hadoop文件系统写入文件时均不调用progress()方法**。后面几章将展示进度对MapReduce
应用的重要性。

### FSDataOutputStream对象
FileSystem实例的`create()`方法返回 **FSDataOutputStream对象**，与FSDataInputStream类
相似，**它也有一个查询文件当前位置的方法**：
```java
package org.apache.hadoop.fs;

public class FSDataOutputStream extends DataOutputStream implements Syncable {
    public long getPos() throws IOException {
        //implementation elided
    }
    //implementation elided
}
```
但与FSDataInputStream类不同的是，**FSDataOutputStream类不允许在文件中定位。这是因为HDFS只
允许对一个已打开的文件顺序写入，或在现有文件的末尾追加数据**。换句话说，它不支持在除文件末尾之外
的其他位置进行写入，**因此，写入时定位就没有什么意义**。

## 4.目录
FileSystem实例提供了 **创建目录的方法**：
```java
public boolean mkdirs(Path f) throws IOException
```
**这个方法可以一次性新建所有必要但还没有的父目录**，就像`java.io.File`类的`mkdirs()`方法。
如果目录（以及所有父目录）都已经创建成功，则返回true。

**通常，你不需要显式创建一个目录，因为调用create()方法写入文件时会自动创建父目录**。

## 5.查询文件系统

### 5.1.文件元数据：FileStatus
任何文件系统的一个重要特征都是提供其目录结构浏览和检索它所存文件和目录相关信息的功能。**FileStatus
类封装了文件系统中文件和目录的元数据，包括文件长度、块大小、复本、修改时间、所有者以及权限信息**。

Filesystem的`getFileStatus()`方法用于获取文件或目录的FileStatus对象。示例：
```java
package com.fhd.hadoop;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hdfs.MiniDFSCluster;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.OutputStream;

import static org.hamcrest.CoreMatchers.is;
import static org.junit.Assert.assertThat;

public class ShowFileStatusTest {

    private MiniDFSCluster cluster;
    private FileSystem fs;

    @Before
    public void setUp() throws IOException {
        Configuration conf = new Configuration();
        if (System.getProperty("test.build.data") == null) {
            System.setProperty("test.build.data", "/tmp");
        }
        cluster = new MiniDFSCluster.Builder(conf).build();
        fs = cluster.getFileSystem();
        OutputStream out = fs.create(new Path("/dir/file"));
        out.write("content".getBytes("UTF-8"));
        out.close();
    }

    @After
    public void tearDown() throws IOException {
        if (fs != null) {
            fs.close();
        }
        if (cluster != null) {
            cluster.shutdown();
        }
    }

    @Test(expected = FileNotFoundException.class)
    public void throwsFileNotFoundForNonExistentFile() throws IOException {
        fs.getFileStatus(new Path("no-such-file"));
    }

    @Test
    public void fileStatusForFile() throws IOException {
        Path file = new Path("/dir/file");
        FileStatus stat = fs.getFileStatus(file);
        assertThat(stat.getPath().toUri().getPath(), is("/dir/file"));
        assertThat(stat.isDirectory(), is(true));
        assertThat(stat.getLen(), is(0L));
        assertThat(stat.getModificationTime(), is(lessThanOrEqualTo(System.currentTimeMillis())));
        assertThat(stat.getReplication(), is((short) 0));
        assertThat(stat.getBlockSize(), is(0L));
        assertThat(stat.getOwner(), is(System.getProperty("user.name")));
        assertThat(stat.getGroup(), is("supergroup"));
        assertThat(stat.getPermission().toString(), is("rwxr-xr-x"));
    }
}
```
如果文件或目录均不存在，会抛出一个`FileNotFoundException`异常。但如果只是想检查文件或目录是否
存在，那么调用`exists()`方法会更方便：
```java
public boolean exists(Path f) throws IOException
```

### 5.2.列出文件
查找一个文件或目录相关的信息很实用。但通常还需要能够 **列出目录中的内容**。这就是FileSystem的
`listStatus()`方法的功能：
```java
public FileStatus[] listStatus(Path f) throws IOException
public FileStatus[] listStatus(Path f, PathFilter filter) throws IOException
public FileStatus[] listStatus(Path[] files) throws IOException
public FileStatus[] listStatus(Path[] files, PathFilter filter) throws IOException
```
当传入的参数是一个文件时，它会简单转变成以数组方式返回长度为1的FileStatus对象。当传入的参数是一
个目录时，则返回0或多个FileStatus对象，表示此目录中包含的文件和目录。

它的重载方法允许使用PathFilter来限制匹配的文件和目录。最后，如果指定一组路径，其执行结果相当于
依次轮流传递每条路径并对其调用`listStatus()`方法，再将FileStatus对象数组累积存入同一数组中，
但该方法更为方便。在从文件系统树的不同分支构建输入文件列表时，这是很有用的。下面的示例，简单显示
了这个方法。注意Hadoop的FileUtil中`stat2Paths()`方法的使用，它将一个FileStatus对象数组转
换为一个Path对象数组。
```java
package com.fhd;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;

import java.net.URI;

public class hadoop {

    public static void main(String[] args) throws Exception {
        String uri = args[0];
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(uri), conf);

        Path[] paths = new Path[args.length];
        for (int i = 0; i < paths.length; i++) {
            paths[i] = new Path(args[i]);
        }
        FileStatus[] status = fs.listStatus(paths);
        Path[] listedPaths = FileUtil.stat2Paths(status);
        for (Path p : listedPaths) {
            System.out.println(p);
        }
    }
}
```
我们可以用这个程序来显示一组路径集目录列表的并集：
```shell
hadoop ListStatus hdfs://localhost/ hdfs://localhost/user/tom
```
```
hdfs://localhost/user
hdfs://localhost/user/tom/books
hdfs://localhost/user/tom/quangle.txt
```

### 5.3.文件模式  
在单个操作中处理一批文件是一个很常见的需求。例如，一个用于处理日常日志的MapReduce作业可能需要分
析一个月内包含在大量目录中的日志文件。**在一个表达式中使用通配符来匹配多个文件是比较方便的，无需
列举每个文件和目录来指定输入**，该操作称为“通配”（`globbing`）。Hadoop为执行通配提供了两个
FileSystem方法：
```java
public FileStatus[] globStatus(Path pathPattern) throws IOException
public FileStatus[] globStatus(Path pathPattern, PathFilter filter) throws IOException
```
**`globStatus()`方法返回路径格式与指定模式匹配的所有FileStatus对象组成的数组，并按路径排序。
PathFilter命令作为可选项可以进一步对匹配结果进行限制**。

Hadoop支持的通配符与`Unix bash shell`支持的相同：

| 通配符 | 名称 | 匹配 |
| :------------- | :------------- | :------------ |
| `*` | 星号 | 匹配0或多个字符 |
| `?` | 问号 | 匹配单一字符 |
| `[ab]` | 字符类 | 匹配{a,b}集合中的一个字符 |
| `[^ab]` | 非字符类 | 匹配非{a,b}集合中的一个字符 |
| `[a-b]` | 字符范围 | 匹配一个在{a...b}范围内的字符（包括ab），a在字典顺序上要小于或等于b |
| `[^a-b]` | 非字符范围 | 匹配一个不在{a...b}范围内的字符（包括ab），a在字典顺序上要小于或等于b |
| `{a,b}` | 或选择 | 匹配包含a或b中的一个的表达式 |
| `\c` | 转义字符 | 匹配元字符c |





















































aaaa
