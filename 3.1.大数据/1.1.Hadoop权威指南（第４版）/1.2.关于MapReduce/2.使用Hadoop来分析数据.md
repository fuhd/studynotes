使用Hadoop来分析数据
================================================================================
**为了充分利用Hadoop提供的并行处理优势，我们需要将查询表示成MapReduce作业**。完成某种本地端的
小规模测试之后，就可以把作业部署到在集群上运行。

## map和reduce
**MapReduce任务过程分为两个处理阶段。map阶段和reduce阶段**。每阶段都以 **键/值对** 作为输入
和输出，其类型由程序员来选择。程序员还需要写两个函数：**map函数** 和 **reduce函数**。

**map阶段的输入是NCDC原始数据**。我们选择 **文本格式** 作为输入格式，将数据集的 **每一行**
作为文本输入。键是某一行起始位置相对于文件起始位置的偏移量，不过我们不需要这个信息，所以将其忽略。

我们的map函数很简单。由于我们只对 **年份** 和 **气温** 属性感兴趣，所以只需要取出这两个字段数
据。**在本例中，map函数只是一个数据准备阶段**，通过这种方式来准备数据，使reduce函数能够继续对它
进行处理：即找出每年的最高气温。**map函数还是一个比较适合去除已损记录的地方**：此处，我们筛掉缺
失的、可疑的或错误的气温数据。

为了全面了解map的工作方式，我们考虑以下输入数据的示例数据（考虑到篇幅，去除了一些未使用的例，并用
省略号表示）：
```
0067011990999991950051507004...9999999N9+00001+99999999999...
0043011990999991950051512004...9999999N9+00221+99999999999...
0043011990999991950051518004...9999999N9-00111+99999999999...
......
```
这些行以键/值对的方式作为map函数的输入：
```
(0,0067011990999991950051507004...9999999N9+00001+99999999999...)
(106,0043011990999991950051512004...9999999N9+00221+99999999999...)
(212,0043011990999991950051518004...9999999N9-00111+99999999999...)
......
```
键（key）是文件中的行偏移量，map函数并不需要这个信息，所以将其忽略。map函数的功能仅限于提取年份
和气温信息，并将它们作为输出（气温值已用整数表示）：
```
(1950,0)
(1950,22)
(1950,-11)
......
```
**map函数的输出经由MapReduce框架处理后，最后发送到reduce函数。这个处理过程基于键来对键/值对进
行排序和分组**。因此，在这一示例中，reduce函数看到的是如下输入：
```
(1949,[111,78])
(1950,[0,22,-11])
```
每一年份后紧跟着一系列气温数据。reduce函数现在要做的是遍历整个列表并从中找出最大的读数：
```
(1949,111)
(1950,22)
```
这是最终输出结果，每一年的全球最高气温记录。

## Java MapReduce
下面写代码实现它，我们需要三样东西：一个map函数、一个reduce函数和一些用来运行作业的代码。**map
函数由Mapper类来表示，后者声明一个抽象的map()方法**。下面范例显示了我们的map函数实现：
```java
import java.io.IOException;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MaxTemperatureMapper extends MapReduceBase implements Mapper<LongWritable,Text,IntWritable> {

    private static final int MISSING = 9999;

    @Override
    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String year = line.substring(15, 19);
        int airTemperature;
        if(line.charAt(87) == '+') {
            airTemperature = Integer.parseInt(line.substring(88, 92));
        } else {
            airTemperature = Integer.parseInt(line.substring(87, 92));
        }
        String quality = line.substring(92, 93);
        if(airTemperature != MISSING && quality.matches("[01459]")) {
            context.write(new Text(year), new IntWritable(airTemperauture));
        }
    }
}
```
这个 **Mapper类是一个泛型类型**，它有 **四个形参类型**，分别指定map函数的 **输入键、输入值、
输出键和输出值** 的类型。就现在这个例子来说，输入键是一个长整数偏移量，输入值是一行文本，输出键是
年份，输出值是气温（整数）。**Hadoop本身提供了一套可优化网络序列化传输的基本类型**，而不直接使用
Java内嵌的类型。**这些类型都在`org.apache.hadoop.io`包中**。这里使用LongWritable类型（相
当于Java的Long类型）、Text类型（相当于Java中的String类型）和IntWritable类型（相当于Java的
Integer类型）。

map()方法的输入是一个键和一个值。我们首先将包含有一行输入的Text值转换成Java的String类型，之后
用substring()方法提取我们感兴趣的列。

**map()方法还提供Context实例用于输出内容的写入**。在这种情况下，我们将年份数据按Text对象进行
读/写（因为我们把年份当作键），将气温值封装在IntWritable类型中。只有气温数据不缺并且所对应质量
代码显示为正确的气温读数时，这些数据才会被写入输出记录中。

以类似方法用Reducer来定义reduce函数，范例如下：
```java
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class MaxTemperatureReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    @Override
    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int maxValue = Integer.MIN_VALUE;
        for(IntWritable value: values) {
            MaxValue = Math.max(maxValue, value.get());
        }
        context.write(key, new IntWritable(maxValue));
    }
}
```
同样，**reduce函数也有四个形式参数类型用于指定输入和输出类型。reduce函数的输入类型必须匹配map
函数的输出类型**：即Text类型和IntWritable类型。在这种情况下，reduce函数的输出类型也必须是Text
和IntWritable类型，分别输出年份及其最高气温。

第三部分代码负责运行MapReduce作业，范例如下：
```java
import java.io.IOException;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.input.FileOutputFormat;

public class MaxTemperature {
    public static void main(String[] args) throws Exception {
        if(args.length != 2) {
            System.err.println("Usage: MaxTemperature <input path> <output path>");
            System.exit(-1);
        }

        Job job = new Job();
        job.setJarByClass(MaxTemperature.class);
        job.setJobName("Max temperature");

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.setMapperClass(MaxTemperatureMapper.class);
        job.setReducerClass(MaxTemperatureReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```
**Job对象指定作业执行规范。我们可以用它来控制整个作业的运行**。我们在Hadoop集群上运行这个作业时，
**要把代码打包成一个JAR文件（Hadoop在集群上发布这个文件）。不必明确指定JAR文件的名称，在Job对
象的setJarByClass()方法中传递一个类即可，Hadoop利用这个类来查找包含它的JAR文件，进而找到相关
的JAR文件**。

构造Job对象之后，需要指定输入和输出数据的路径。**调用FileInputFormat类的静态方法addInputPath()
来定义输入数据的路径，这个路径可以是单个的文件、一个目录（此时，将目录下所有文件当作输入）或符合
特定文件模式的一系列文件**。由函数名可知，**可以多次调用addInputPath()来实现多路径的输入**。

**调用FileOutputFormat类中的静态方法setOutputPath()来指定输出路径（只能有一个输出路径）。
这个方法指定的是reduce函数输出文件的写入目录，在运行作业前该目录是不应该存在的，否则Hadoop会报
错并拒绝运行作业**。这种预防措施的目的是 **防止数据丢失**（长时间运行的作业如果结果被意外覆盖，
肯定是非常恼人的）。

接着，通过 **setMapperClass()** 和 **setReducerClass()** 方法指定要用的 **map类型** 和
**`reduce`类型**。

**setOutputKeyClass()和setOutputValueClass()方法控制reduce函数的输出类型，并且必须和Reducer
类产生的相匹配。map函数的输出类型默认情况下和reduce函数是相同的，因此如果mapper产生出和reducer
相同的类型时，不需要单独设置。但是，如果不同，则必须通过setMapOutputKeyClass()和
setMapOutputValueClass()方法来设置map函数的输出类型**。

**输入的类型通过输入格式来控制，我们的例子中没有设置，因为使用的是默认的TextInputFormat（文本
输入格式）**。

在设置定义map和reduce函数的类之后，可以开始运行作业。**Job中的waitForCompletion()方法提交
作业并等待执行完成。该方法唯一的参数是一个标识，指示是否已生成详细输出。当标识为true时，作业会把
其进度信息写到控制台**。

### 运行测试
写好MapReduce作业之后，**通常要拿一个小型数据集进行测试以排除代码问题**。首先，**以独立（本机）
模式安装Hadoop**，详细说明看安装一节。在这种模式下，Hadoop在本地文件系统上运行作业程序。然后使
用本书网站上的指令安装和编译示例。
```
% export HADOOP_CLASSPATH=hadoop-examples.jar
% hadoop MaxTemperature input/ncdc/sample.txt output
```
输出日志省略。

**如果调用hadoop命令的第一个参数是类名，Hadoop就会启动一个JVM来运行这个类。该Hadoop命令将Hadoop
库（及其依赖关系）添加到类路径中，同时也能获得Hadoop配置信息。为了将应用类添加到类路径中，我们定
义了一个HADOOP_CLASSPATH环境变量，然后由Hadoop脚本来执行相关操作**。
```
以本地（独立）模式运行时，本书中所有程序均假设已按照这种方式来设置HADOOP_CLASSPATH。
```
