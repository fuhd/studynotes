Hadoop配置
=================================================================================
有多个配置文件适用于Hadoop安装，这几个重要文件都在Hadoop分发包的 **etc/hadoop目录** 中。**配置目录
可以被重新安置在文件系统的其他地方（Hadoop安装路径的外面，以便于升级）**，只要启动守护进程时使用
**--config选项**（或等价的，使用 **HADOOP_CONF_DIR** 环境变量集）说明这个目录在本地文件系统的位置
就可以了。

Hadoop配置文件：

| 文件名称 | 格式 | 描述 |
| :------------- | :------------- | :------------------ |
| **hadoop-env.sh** | Bash脚本 | 脚本中要用到的环境变量，以运行Hadoop |
| **mapred-env.sh** | Bash脚本 | 脚本中要用到的环境变量，以运行MapReduce（**覆盖hadoop-env.sh中设置的变量**）|
| **yarn-env.sh** | Bash脚本 | 脚本中要用到的环境变量，以运行YARN（**覆盖hadoop-env.sh中设置的变量**）|
| **core-site.xml** | Hadoop配置XML | Hadoop Core的配置项，例如HDFS、MapReduce和YARN常用的I/O设置等 |
| **hdfs-site.xml** | Hadoop配置XML | Hadoop守护进程的配置项，包括namenode、辅助namenode和datanode等 |
| **mapred-site.xml** | Hadoop配置XML | MapReduce守护进程的配置项，包括作业历史服务器 |
| **yarn-site.xml** | Hadoop配置XML | YARN守护进程的配置项，包括资源管理器、web应用代理服务器和节点管理器 |
| **slaves** | 纯文本 | 运行datanode和节点管理器的机器列表（每行一个）|
| **hadoop-metrics2.properties** | Java属性文件 | 控制如何在Hadoop上发布度量的属性 |
| **log4j.properties** | Java属性文件 | 系统日志文件、namenode审计日志、任务JVM进程的任务日志的属性 |
| **hadoop-policy.xml** | Hadoop配置XML | 安全模式下运行Hadoop时的访问控制列表的配置项 |

### 配置管理
Hadoop并没有将所有配置信息放在一个单独的全局位置中。反之，**集群的每个Hadoop节点都各自保存一系列配置
文件，并由管理员完成这些配置文件的同步工作**。有 **并行shell工具** 帮助完成同步工作，诸如 **dsh或pdsh**。
在这方面，**Hadoop集群管理工具**，例如 **Cloudera Manager** 和 **Apache Ambari** 表现突出，
**因为在集群间传递修改信息是它们的关注点**。

**Hadoop也支持为所有master机器和worker机器采用同一套配置文件。这个做法的最大优势在于简单**，不仅体现在
理论上（仅需处理一套配置文件），也体现在可操作性上（使用Hadoop脚本就能进行管理）。

但是，**这种一体适用的配置模型并不合适某些集群**。以扩展集群为例，当试图为集群添加新机器，且新机器的硬件
规格与现有机器不同时，则需要新建一套配置文件，以充分利用新硬件的额外资源。

在这种情况下，需要引入“**机器类**”的概念，**为每一机器类维护单独的配置文件**。Hadoop没有提供执行这个
操作的工具。**需要借助外部工具** 来执行该配置操作，例如：**Chef、Puppet、CFEngine和Bcfg2等**。

对于任何规模的集群来说，**同步所有机器上的配置文件都极具挑战性**。因此，尽管用户能够使用控制脚本来管理
Hadoop，**仍然推荐使用控制管理工具管理集群**。使用这些工具也可以顺利完成日常维护，例如为安全漏洞打补丁，
升级系统包等。

### 环境设置
本节探讨如何 **设置hadoop-env.sh文件中的变量**。MapReduce和YARN（HDFS除外）都有类似的配置文件，
分别为 **mapred-env.sh** 和 **yarn-env.sh**，文件中的变量和组件相关，并且可以进行设置。**注意，
hadoop-env.sh文件里设置的值会被mapred-env.sh（MapReduce）和yarn-env.sh（YARN）文件覆盖**。

#### 1.Java
**需要设置Hadoop系统的Java安装的位置**。
+ 方法一：**是在hadoop-env.sh文件中设置JAVA_HOME项**；
+ 方法二：**是在shell中设置JAVA_HOME环境变量**；

相比之下，**方法一更好**，因为是需操作一次就能够保证整个集群使用同一版本的Java。

#### 2.内存堆大小
在默认情况下，Hadoop为各个守护进程分配 **1000MB（1GB）** 内存。该内存值由 **hadoop-env.sh** 文件的
**HADOOP_HEAPSIZE** 参数控制。也可以通过设置环境变量为单个守护进程修改堆大小。例如，在 **yarn-env.sh**
文件中设置 **YARN_RESOURCEMANAGER_HEAPSIZE**，即 **可覆盖资源管理器的堆大小**。

令人惊讶的是，尽管namenode分配更多的堆空间是很常见的事，**但对于HDFS守护进程而言并没有相应的环境变量**。
当然有别的途径可以设置namenode堆空间大小。

除了守护进程对内存的需求，节点管理器还需为应用程序分配容器（container），因此需要综合考虑上述因素来
计算一个工作机器的总体内存需求。
```
一个守护进程究竟需要多少内存？

由于namenode会在内存中维护所有文件的每个数据块的引用，因此namenode很可能会“吃光”分配给它的所有内存。很难套用一个公式来精确
计算内存需求量，因为内存需求量取决于多个因素，包括每个文件包含的数据块数、文件名称的长度、文件系统中的目录数等。此外，在不同
Hadoop版本下，namenode的内存需求也不相同。

1000MB内存（默认配置）通常足够管理数百万个文件。但是根据经验来看，保守估计需要为每100万个数据块分配1000MB内存空间。

以一个含200节点的集群为例，假设每个节点有24TB磁盘空间，数据块大小是128MB，复本数是3的话，则约有200万个数据块（甚至更多）：
200 * 24000000MB /(128MB * 3)。因此，在本例中，namenode的内存空间最好一开始设为12000MB。

也可以只增加namenode的内存分配量而不改变其他Hadoop守护进程的内存分配，即设置hadoop-env.sh文件的HADOOP_NAMENODE_OPTS
属性包含一个JVM选项以设定内存大小。HADOOP_NAMENODE_OPTS允许向namenode的JVM传递额外的选项。以Sun JVM为例，-Xmx2000m
选项表示为namenode分配2000MB内存空间。

由于辅助namenode的内存需求量和主namenode差不多，所以一旦更改namenode的内存分配的话还需对辅助namenode做相同更改（使用
HADOOP_SECONDARYNAMENODE_OPTS变量）。
```

#### 3.系统日志文件
默认情况下，Hadoop生成的系统日志文件存放在 **$HADOOP_HOME/logs** 目录之中，**也可以通过hadoop-env.sh
文件中的HADOOP_LOG_DIR来进行修改。建议修改默认设置，使之独立于Hadoop的安装目录**。这样的话，即使
Hadoop升级之后，安装路径发生变化，也不会影响日志文件的位置。**通常可以将日志文件存放在/var/log/hadoop
目录中**。实现方法很简单，就是在hadoop-env.sh中加入一行：
```shell
export HADOOP_LOG_DIR=/var/log/hadoop
```
**如果日志目录并不存在，则会首先创建该目录**（如果操作失败，请确认相关的Unix Hadoop用户是否有权创建该
目录）。运行在各台机器上的各个Hadoop守护进程会产生 **两类日志文件**。**第一类日志文件（以.log作为后缀
名）是通过log4j记录的。鉴于大部分应用程序的日志消息都写到该日志文件中，故障诊断的首要步骤即为检查该文件**。
标准的Hadoop log4j配置采用 **日常滚动文件追加方式（daily rolling file appender）** 来循环管理日
志文件。**系统不自动删除过期的日志文件，而是留待用户定期删除或存档，以节约本地磁盘空间**。

**第二类日志文件后缀名为.out，记录标准输出和标准错误日志。由于Hadoop使用log4j记录日志，所以该文件通常
只包含少量记录，甚至为空**。重启守护进程时，系统会创建一个新文件来记录此类日志。**系统仅保留最新的5个日志
文件**。旧的日志文件会附加一个介于1和5之间的数字后缀，5表示最旧的文件。

**日志文件的名称**（两种类型）包含运行守护进程的 **用户名称、守护进程名称和本地主机名等信息**。例如，
hadoop-hdfsdatanode-ip-10-45-174-112.log.2014-09-20就是一个日志文件的名称。这种命名方法保证集群
内所有机器的日志文件名称各不相同，从而可以将所有日志文件存到一个目录中。

**日志文件名称中的“用户名称”部分实际对应hadoop-env.sh文件中的HADOOP_IDENT_STRING项**。如果想采用其他
名称，可以修改HADOOP_IDENT_STRING项。

#### 4.SSH设置
**借助SSH协议，用户在主节点上使用控制脚本就能在（远程）工作节点上运行一系列指令**。自定义SSH设置会带来
诸多益处。例如，**减小连接超时设定（使用ConnectTimeout选项）** 可以避免控制脚本长时间等待宕机节点的响应。
当然，也不可设得过低，否则会导致繁忙节点被跳过。

**StrictHostKeyChecking** 也是一个很有用的SSH设置。**设置为no会自动将新主机键加到已知主机文件之中。
该项默认值是ask，提示用户确认是否已验证了“键指纹”（key fingerprint），因此不适合大型集群环境**。

**在hadoop-env.sh文件中定义HADOOP_SSH_OPTS环境变量还能够向SSH传递更多选项**。

### Hadoop守护进程的关键属性
Hadoop的配置属性之多简直让人眼花缭乱。**本节讨论对于真实的工作集群来说非常关键的一些属性**（或至少能
够理解默认属性的含义），这些属性分散在Hadoop的站点文件之中，包括 **core-site.xml、hdfs-site.xml和
yarn-site.xml**。

下面的 **3个范例** 分别列举了这些文件的典型实例（注意，这里没有列出MapReduce的站点文件。这是因为
**仅有的MapReduce守护进程就是作业历史服务器，对于它而言，默认属性值就够用了**）。
```xml
<!-- 典型的core-site.xml配置文件 -->
<?xml version="1.0"?>
<!-- core-site.xml -->
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode/</value>
    </property>
</configuration>
```
```xml
<!-- 典型的hdfs-site.xml配置文件 -->
<?xml version="1.0"?>
<!-- hdfs-site.xml -->
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/disk1/hdfs/name,/remote/hdfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/disk1/hdfs/data,/disk2/hdfs/data</value>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>/disk1/hdfs/namesecondary,/disk2/hdfs/namesecondary</vlaue>
    </property>
</configuration>
```
```xml
<!-- 典型的yarn-site.xml配置文件 -->
<?xml version="1.0"?>
<!-- yarn-site.xml -->
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>resourcemanager</value>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/disk1/nm-local-dir,/disk2/nm-local-dir</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce.shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>16384</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>16</value>
    </property>
</configuration>
```

#### 1.HDFS
**运行HDFS需要将一台机器指定为namenode**。在本例中，属性 **fs.defaultFS描述HDFS文件系统的URI，
其主机是namenode的主机名称或IP地址，端口是namenode监听RPC端口。如果没有指定，那么默认端口是8020**。

**属性fs.defaultFS也指定了默认文件系统，可以解析相对路径**。相对路径的长度更短，使用更便捷（不需要了解
特定namenode的地址）。例如，假设默认文件系统如上面的示例（core-site.xml）那样，则相对URI **/a/b解析
为hdfs://namenode/a/b**。

此外，还有其他一些关于HDFS的配置选项，包括namenode和datanode存储目录的属性。属性项 **dfs.namenode.name.dir
指定一系列目录来供namenode存储永久性的文件系统元素数据（编辑日志和文件系统映像）。这些元数据文件会同时备份
在所有指定目录中。通常情况下，通过配置dfs.namenode.name.dir属性可以将namenode元数据写到一两个本地磁盘
和一个远程磁盘（例如NFS挂载的目录）之中。这样的话，即使本地磁盘发生故障，甚至整个namenode发生故障，
都可以恢复元数据文件并且重构新的namenode。辅助namenode只是定期保存namenode的检查点，不维护namenode
的最新备份**。

属性 **dfs.datanode.data.dir可以设定datanode存储数据块的目录列表**。前面提到，dfs.namenode.name.dir
描述一系列目录，其目的是支持namenode进行冗余备份。**虽然dfs.datanode.data.dir也描述了一系列目录，
但是其目的是使datanode循环地在各个目录中写数据**。因此，**为了提高性能，最好分别为各个本地磁盘指定一个存储
目录。这样一来，数据块跨磁盘分布，针对不同数据块的读操作可以并发执行，从而提升读取性能**。

最后，**还需要指定辅助namenode存储文件系统的检查点的目录。属性dfs.namenode.checkpoint.dir指定一系列目录
来保存检查点。与namenode类似，检查点映像文件会分别存储在各个目录之中，以支持冗余备份**。

HDFS守护进程的关键属性：

| 属性名称 | 类型 | 默认值 | 说明 |
| :------------- | :------------- | :-------------- | :--------------- |
| **fs.defaultFS** | URI | file:/// | 默认文件系统。URI定义主机名称和namenode的RPC服务器工作的端口号，默认值是8020。本属性保存在core-site.xml中 |
| **dfs.namenode.name.dir** | 以逗号分隔的目录名称 | file://${hadoop.tmp.dir}/dfs/name | namenode存储永久性的元数据的目录列表。namenode在列表上的各个目录中均存放相同的元数据文件 |
| **dfs.datanode.data.dir** | 以逗号分隔的目录名称 | file://${hadoop.tmp.dir}/dfs/data | datanode存放数据块的目录列表。各个数据块分别存放于某一个目录中 |
| **dfs.namenode.checkpoint.dir** | 以逗号分隔的目录名称 | file://${hadoop.tmp.dir}/dfs/namesecondary | 辅助namenode存放检查点的目录列表。在所列每个目录中均存放一份检查点文件的副本 |

#### 2.YARN
为了运行YARN，**需要指定一台机器作为资源管理器**。最简单的做法是将属性 **yarn.resourcemanager.hostname
设置为用于运行资源管理器的机器的主机名或IP地址。资源管理器服务器的地址基本都可以从该属性获得**。例如，
**yarn.resourcemanager.address的格式为主机/端口对，yarn.resourcemanager.hostname表示默认主机**。
在MapReduce客户端配置中，需要通过RPC连接到资源管理器时，会用到这个属性。

**在执行MapReduce作业的过程中所产生的中间数据和工作文件被写到临时本地文件之中**。由于这些数据 **包括map任务
的输出数据，数据量可能非常大**，因此必须保证YARN容器本地临时存储空间（**由yarn.nodemanager.local-dirs
属性设置**）的容量足够大。**yarn.nodemanager.local-dirs属性使用一个逗号分隔的目录名称列表，最好将这些
目录分散到所有本地磁盘，以提升磁盘I/O操作的效率。通常情况下，YARN本地存储会使用与datanode数据块存储相同
的磁盘和分区（但是不同的目录）**。如前所述，datanode数据块存储目录由dfs.datanode.data.dir属性项指定。

与MapReduce 1 不同，**YARN** 没有tasktracker，**它依赖于shuffle句柄将map任务的输出送给reduce任务。
Shuffle句柄是长期运行于节点管理器的附加服务**。由于YARN是一个通用目的的服务，因此要通过将 **yarn-site.xml**
文件中的 **yarn.nodemanager.aux-services属性设置为mapreduce_shuffle来显式启用MapReduce的shuffle句柄**。

YARN守护进程的关键属性：

| 属性名称 | 类型 | 默认值 | 说明 |
| :------------- | :------------- | :------------- | :---------------- |
| **yarn.resourcemanager.hostname** | 主机名 | 0.0.0.0 | 运行资源管理器的机器主机名。以下缩写为${y.rm.hostname} |
| **yarn.resourcemanager.address** | 主机名和端口号 | ${y.rm.hostname}:8032 | 运行资源管理器的RPC服务器的主机名和端口 |
| **yarn.nodemanager.local-dirs** | 逗号分隔的目录名称 | ${hadoop.tmp.dir}/nm-local-dir | 目录列表，节点管理器允许容器将中间数据存于其中。当应用结束时，数据被清除 |
| **yarn.nodemanager.aux-services** | 逗号分隔的服务名称 | | 节点管理器运行的附加服务列表。每项服务由属性yarn.nodemanager.auxservices.servicename.class所定义的类实现。默认情况下，不指定附加服务 |
| **yarn.nodemanager.resource.memory-mb** | int | 8192 | 节点管理器运行的容器可以分配到的物理内存容量（单位是MB）|
| **yarn.nodemanager.vmem-pmem-ratio** | float | 2.1 | 容器所占的虚拟内存和物理内存之比。该值指示了虚拟内存的使用可以超过所分配内存的量 |
| **yarn.nodemanager.resource.cpuvcores** | int | 8 | 节点管理器运行的容器可以分配到的CPU核数目 |

#### 3.YARN和MapReduce中的内存设置
与MapReduce 1的基于 **slot** 的模型相比，**YARN以更精细化的方式来管理内存**。YARN不会立刻指定一个节点
上可以运行的map和reduce的slot的最大数目，相反，**它允许应用程序为一个任务请求任意规模的内存（在限制范围内）**。
在YARN模型中，节点管理器从一个内存池中分配内存，因此，**在一个特定节点上运行的任务数量取决于这些任务对内存
的总需求量，而不简单取决于固定的slot数量**。

计算为一个运行容器的节点管理器分配多少内存要取决于机器上的物理内存。每个Hadoop守护进程使用1000MB内存，
因此需要2000MB内存来运行1个datanode和1个节点管理器。**为机器上运行的其他进程留出足够的内存后，通过将配置
属性yarn.nodemanager.resource.memory-mb设置为总分配量（单位是MB），剩余的内存就可以被指定给节点管理
器的容器使用了。默认是8192MB**，对于大多数设置来说太低了。

接下来是确定如何 **为单个作业设置内存选项**。有两种主要控制方法：**一个是控制YARN分配的容器大小，另一个是控制
容器中运行的Java进程堆大小**。
```
MapReduce的内存控制都由客户端在作业配置中设置。YARN设置是集群层面的设置，客户端不能修改。
```
**容器大小由属性mapreduce.map.memory.mb和mapreduce.reduce.memory.mb决定，默认值都为1024MB**。
application master会使用这些设置以从集群中请求资源；此外，**节点管理器也会使用这些设置来运行、监控
任务容器。Java进程的堆大小由mapred.child.java.opts设置，默认是200MB。也可以单独为map和reduce
任务设置Java选项**。

MapReduce作业内存属性（**由客户端设置**）：

| 属性名称 | 类型 | 默认值 | 说明 |
| :---- | :---- | :-----| :----- |
| **mapreduce.map.memory.mb** | int | 1024 | map容器所用的内存容量 |
| **mapreduce.reduce.memor.mb** | int | 1024 | reduce容器所用的内存容量 |
| **mapred.child.java.opts** | String | -Xmx200m | JVM选项，用于启动运行map和reduce任务的容器进程。除了用于设置内存，该属性还包括JVM属性设置，以支持调试 |
| **mapreduce.map.java.opts** | String | -Xmx200m | JVM选项，针对运行map任务的子进程 |
| **mapreduce.reduce.java.opts** | String | -Xmx200m | JVM选项，针对运行reduce任务的子进程 |

例如，**假设mapred.child.java.opts被设为-Xmx800m, mapreduce.map.memory.mb被设置为默认值1024MB，
当map任务启动时，节点管理器会为该任务分配1个1024MB的容器**（在该任务运行期间，节点管理器的内存池也会相应
降低1024MB），**并启动配置为最大堆为800MB的JVM任务**。注意，**JVM进程的内存开销将比该堆的规模要大**，
开销依赖于所使用的本地库（native libraries）、永久生成空间（permanent generation space）等因素。
需要注意的是，**JVM进程（包括它创建的任何进程，如Streaming）所使用的物理内存必须不超出分配给它的内存大小
（1024MB）。如果一个容器使用的内存超过所分配的量，就会被节点管理器终止，并标记为失败**。

**YARN调试器会指定一个最小和最大内存分配量。默认情况下，最小内存分配量是1024MB（由yarn.scheduler.minimum-allocation-mb
设置），默认情况下，最大内存分配量是8192MB（由yarn.scheduler.maximum-allocation-mb设置）**。

**容器还需要满足对虚拟内存的限制。如果容器所使用的虚拟内存量超出预定系统和所分配的物理内存的乘积，则节点
管理器也会终止进程。该系统由yarn.nodemanager.vmem-pmem-ratio属性指定，默认值是2.1。在前面的例子中，
虚拟内存规模的上限值为2150MB，即2.1 * 1024MB**。

除了使用参数来配置内存使用之外，**还可以使用MapReduce任务计数器来监控任务执行过程中的真实内存消费量**。
这些计算器包括：**PHYSICAL_EMORY_BYTES、VIRTUAL_MEMORY_BYTES和COMMITTED_HEAP_BYTES**（见原书表9-2），
**分别描述了在某一时刻各种内存的使用情况，因此也适用于在任务尝试期间的观察**。

**Hadoop也提供了一些设置方法，用于控制MapReduce操作的内存使用。这些设置可以针对每个作业进行**（见原书7.3节）。

#### 4.YARN和MapReduce中的CPU设置
除了内存外，**YARN将CPU的使用作为一种资源进行管理，应用程序可以申请所需要的核数量。通过属性
yarn.nodemanager.resource.cpuvcores可以设置节点管理器分配给容器的核数量。应该设置为机器的总核数
减去机器上运行的每个守护进程（datanode、节点管理器和其他长期运行的进程）占用的核数（每个进程占用1个核）**。

**通过设置属性mapreduce.map.cpu.vcores和mapreduce.reduce.cpu.vcores，MapReduce作业能够控制分配
给map和reduce容器的核数量。两者的默认值均为1，适合通常的单线程MapReduce任务，因为这些任务使用单核就足够了**。
```
当调度过程中对核数量进行掌控后（这样，当机器没有空余核时，一个容器将不会分到核），节点管理器默认情况下将不会限
制运行中的容器对CPU的实际使用。这意味着一个容器可能会出现滥用配额的情况，例如使用超额的CPU，而这可能会饿死在同
一主机上运行的其他容器。YARN提供了基于Linux的cgroup技术的、强制实施CPU限制的手段。为此，节点管理器的容器执行类
（yarn.nodemanager.containerexecutor.class）必须被设置为使用LinuxContainerExecutor类，并且必须将
LinuxContainerExecutor类配置为使用cgroup。
```

### Hadoop守护进程的地址和端口
**Hadoop守护进程一般同时运行RPC和HTTP两个服务器，RPC服务器支持守护进程间的通信，HTTP服务器则提供与
用户交互的Web页面**。需要分别为各个服务器配置网络地址和监听端口号。**端口号0表示服务器会选择一个空闲的
端口号：但由于这种做法与集群范围的防火墙策略不兼容，所以我通常不推荐**。

RPC服务器的属性：

| 属性名称 | 默认值 | 说明 |
| :------ | :--- | :------ |
| **fs.defaultFS** | file:/// | 设为一个HDFS的URI时，该属性描述namenode的RPC服务器地址和端口。如果不指定，则默认的端口号是8020 |
| **dfs.namenode.rpc-bind-host** |  | namenode的RPC服务器将绑定的地址。如果没有设置（默认情况），绑定地址由fs.defaultFS决定。可以设为0.0.0.0，使得namenode可以监听所有接口 |
| **dfs.datanode.ipc.address** | 0.0.0.0:50020 | datanode的RPC服务器地址和端口 |
| **apreduce.jobhistory.address** | 0.0.0.0:10020 | 作业历史服务器的RPC服务器地址和端口，客户端（一般在集群外部）用于查询作业历史 |
| **mapreduce.jobhistory.bind-host** |  | 作业历史服务器的RPC和HTTP服务器将绑定的地址 |
| **yarn.resourcemanager.hostname** | 0.0.0.0 | 资源管理器运行所在的机器主机名。以下缩写为${y.rm.hostname} |
| **yarn.resourcemanager.bind-host** |  | 资源管理器的RPC和HTTP服务器将绑定的地址 |
| **yarn.resourcemanager.address** | ${y.rm.hostname}:8032 | 资源管理器的RPC服务器地址和端口。客户端（一般在集群外部）通过它与资源管理器通信 |
| **yarn.resourcemanager.admin.address** | ${y.rm.hostname}:8033 | 资源管理器的admin RPC服务器地址和端口。admin客户端（由yarn rmadmin调用，一般在集群外部）借此与资源管理器通信 |
| **yarn.resourcemanager.scheduler.address** | ${y.rm.hostname}:8030 | 资源管理器的调度器RPC服务器地址和端口。application master（在集群内部）借此与资源管理器通信 |
| **yarn.resourcemanager.resourcetracker.address** | ${y.rm.hostname}:8031 | 资源管理器的resource tracker的RPC服务器地址和端口。节点管理器（在集群内）借此与资源管理器通信 |
| **yarn.nodemanager.hostname** | 0.0.0.0 | 节点管理器运行所在的机器的主机名。以下缩写为${y.nm.hostname} |
| **yarn.nodemanager.bind-host** |  | 节点管理器的RPC和HTTP服务器将绑定的地址 |
| **yarn.nodemanager.address** | ${y.nm.hostname}:0 | 节点管理器的RPC服务器地址和端口。application master（在集群内部）借此与节点管理器通信 |
| **yarn.nodemanager.localizer.address** | ${y.nm.hostname}:8040 | 节点管理器的localizer的RPC服务器地址和端口 |

HTTP服务器的属性：

| 属性名称 | 默认值 | 说明 |
| :----- | :---- | :------- |
| **dfs.namenode.http-address** | 0.0.0.0:50070 | namenode的HTTP服务器地址和端口 |
| **dfs.namenode.http-bind-host** |  | namenode的HTTP服务器将绑定的地址 |
| **dfs.namenode.secondary.http-address** | 0.0.0.0:50090 | 辅助namenode的HTTP服务器地址和端口 |
| **dfs.datanode.http.address** | 0.0.0.0:50075 | datanode的HTTP服务器地址和端口。注意，属性名和namenode的属性名不一样 |
| **mapreduce.jobhistory.webapp.address** | 0.0.0.0:19888 | MapReduce作业历史服务器地址和端口。该属性在mapred-site.xml文件中设置 |

通常，用于设置服务器RPC和HTTP地址的属性担负着双重责任：一方面它们决定了 **服务器将绑定的网络接口**，另一
方面，**客户端或集群中的其他机器使用它们连接服务器**。例如，节点管理器使用yarn.resourcemanager.resourcetracker.address
属性来确定它们的资源管理器的地址。

**用户经常希望服务器同时可以绑定多个网络接口，将网络地址设为0.0.0.0可以达到这个目的，但是却破坏了上述
第二种情况，因为这个地址无法被客户端或集群中的其他机器解析**。一种解决䢍是将客户端和服务器的配置分开，
但是更好的一种方案是 **为服务器绑定主机。通过将yarn.resourcemanager.hostname设定为主机名或IP地址，
yarn.resourcemanager.bind-host设定为0.0.0.0，可以确保资源管理器能够与机器上的所有地址绑定，
且同时能为节点管理器和客户端提供可解析的地址**。

**除了RPC服务器之外，各个datanode还运行TCP/IP服务器以支持块传输。服务器地址和端口号由属性dfs.datanode.address设定，
默认值是0.0.0.0:50010**。

**有多个网络接口时，还可以为各个datanode选择某一个网络接口作为IP地址 (针对HTTP和RPC服务器)。相关属性
是dfs.datanode.dns.interface，默认值是default，表示使用默认的网络接口。可以修改该属性项来变更网络
接口的地址（例如，etho）**。

### Hadoop的其他属性

#### 1.集群成员
**为了便于在将来添加或移除节点，可以通过文件来指定一些允许作为datanode或节点管理器加入集群的经过认证
的机器。属性dfs.hosts记录允许作为datanode加入集群机器列表；属性yarn.resourcemanager.nodes.include-path记录
允许作为节点管理器加入集群的机器列表。与之相对应的，属性dfs.hosts.exclude和yarn.resourcemanager.nodes.exclude-path
所指定的文件分别包含待解除的机器列表**。

#### 2.缓冲区大小
**Hadoop使用一个4KB（4096字节）的缓冲区辅助I/O操作**。对于现代硬件和操作系统来说，这个容量实在过于保守了。
**增大缓冲区容量会显著提高性能，例如128KB（131072字节）更常用。可以通过core-site.xml文件中的io.file.buffer.size
属性来设置缓冲区大小（以字节为单位）**。

#### 3.HDFS块大小
**在默认情况下，HDFS块大小是128MB，但是许多集群把块大小设得更大（如256MB，268435456字节）以降低namenode
的内存压力，并向mapper传输更多数据。可以通过hdfs-site.xml文件中的dfs.blocksize属性设置块的大小（以字节
为单位）**。

#### 4.保留的存储空间
**默认情况下，datanode能够使用存储目录上的所有闲置空间。如果计划将部分空间留给其他应用程序（非HDFS）,
则需要设置dfs.datanode.du.reserved属性来指定待保留的空间大小（以字节为单位）**。

#### 5.回收站
**Hadoop文件系统也有回收站设施，被删除的文件并未真正删除，仅只转移到回收站（一个特定文件夹）。回收站中的
文件在被永久删除之前仍会至少保留一段时间。该信息由core-site.xml文件中的fs.trash.interval属性（以分钟
为单位）设置。默认情况下，该属性的值是0，表示回收站特性无效**。

与许多操作系统类似，**Hadoop的回收站设施是用户级特性，换句话说，只有由文件系统shell直接删除的文件才会
被放到回收站中，用程序删除的文件会被直接删除**。当然，也有例外的情况，如使用Trash类。构造一个Trash实例，
调用moveToTrash()方法会把指定路径的文件移到回收站中。如果操作成功，该方法返回一个值；否则，如果回收
站特性未被启动，或该文件已经在回收站中，该方法返回false。

**当回收站特性被启用时，每个用户都有独立的回收站目录**，即：**home目录下的.Trash目录**。恢复文件也很简易：
在.Trash的子目录中找到文件，并将其移出.Trash目录。

**HDFS会自动删除回收站中的文件，但是其他文件系统并不具备这项功能**。对于这些文件系统，必须定期手动删除。
执行以下命令可以删除已在回收站中 **超过最小时限的所有文件**：
```shell
$ hadoop fs -expunge
```
**Trash类的expunge()方法也具有相同的效果**。

#### 6.作业调度





















































ddd
