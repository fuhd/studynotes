Hadoop配置
=================================================================================
有多个配置文件适用于Hadoop安装，这几个重要文件都在Hadoop分发包的 **etc/hadoop目录** 中。**配置目录
可以被重新安置在文件系统的其他地方（Hadoop安装路径的外面，以便于升级）**，只要启动守护进程时使用
**--config选项**（或等价的，使用 **HADOOP_CONF_DIR** 环境变量集）说明这个目录在本地文件系统的位置
就可以了。

Hadoop配置文件：

| 文件名称 | 格式 | 描述 |
| :------------- | :------------- | :------------------ |
| **hadoop-env.sh** | Bash脚本 | 脚本中要用到的环境变量，以运行Hadoop |
| **mapred-env.sh** | Bash脚本 | 脚本中要用到的环境变量，以运行MapReduce（**覆盖hadoop-env.sh中设置的变量**）|
| **yarn-env.sh** | Bash脚本 | 脚本中要用到的环境变量，以运行YARN（**覆盖hadoop-env.sh中设置的变量**）|
| **core-site.xml** | Hadoop配置XML | Hadoop Core的配置项，例如HDFS、MapReduce和YARN常用的I/O设置等 |
| **hdfs-site.xml** | Hadoop配置XML | Hadoop守护进程的配置项，包括namenode、辅助namenode和datanode等 |
| **mapred-site.xml** | Hadoop配置XML | MapReduce守护进程的配置项，包括作业历史服务器 |
| **yarn-site.xml** | Hadoop配置XML | YARN守护进程的配置项，包括资源管理器、web应用代理服务器和节点管理器 |
| **slaves** | 纯文本 | 运行datanode和节点管理器的机器列表（每行一个）|
| **hadoop-metrics2.properties** | Java属性文件 | 控制如何在Hadoop上发布度量的属性 |
| **log4j.properties** | Java属性文件 | 系统日志文件、namenode审计日志、任务JVM进程的任务日志的属性 |
| **hadoop-policy.xml** | Hadoop配置XML | 安全模式下运行Hadoop时的访问控制列表的配置项 |

### 配置管理
Hadoop并没有将所有配置信息放在一个单独的全局位置中。反之，**集群的每个Hadoop节点都各自保存一系列配置
文件，并由管理员完成这些配置文件的同步工作**。有 **并行shell工具** 帮助完成同步工作，诸如 **dsh或pdsh**。
在这方面，**Hadoop集群管理工具**，例如 **Cloudera Manager** 和 **Apache Ambari** 表现突出，
**因为在集群间传递修改信息是它们的关注点**。

**Hadoop也支持为所有master机器和worker机器采用同一套配置文件。这个做法的最大优势在于简单**，不仅体现在
理论上（仅需处理一套配置文件），也体现在可操作性上（使用Hadoop脚本就能进行管理）。

但是，**这种一体适用的配置模型并不合适某些集群**。以扩展集群为例，当试图为集群添加新机器，且新机器的硬件
规格与现有机器不同时，则需要新建一套配置文件，以充分利用新硬件的额外资源。

在这种情况下，需要引入“**机器类**”的概念，**为每一机器类维护单独的配置文件**。Hadoop没有提供执行这个
操作的工具。**需要借助外部工具** 来执行该配置操作，例如：**Chef、Puppet、CFEngine和Bcfg2等**。

对于任何规模的集群来说，**同步所有机器上的配置文件都极具挑战性**。因此，尽管用户能够使用控制脚本来管理
Hadoop，**仍然推荐使用控制管理工具管理集群**。使用这些工具也可以顺利完成日常维护，例如为安全漏洞打补丁，
升级系统包等。

### 环境设置
本节探讨如何 **设置hadoop-env.sh文件中的变量**。MapReduce和YARN（HDFS除外）都有类似的配置文件，
分别为 **mapred-env.sh** 和 **yarn-env.sh**，文件中的变量和组件相关，并且可以进行设置。**注意，
hadoop-env.sh文件里设置的值会被mapred-env.sh（MapReduce）和yarn-env.sh（YARN）文件覆盖**。

#### 1.Java
**需要设置Hadoop系统的Java安装的位置**。
+ 方法一：**是在hadoop-env.sh文件中设置JAVA_HOME项**；
+ 方法二：**是在shell中设置JAVA_HOME环境变量**；

相比之下，**方法一更好**，因为是需操作一次就能够保证整个集群使用同一版本的Java。

#### 2.内存堆大小
在默认情况下，Hadoop为各个守护进程分配 **1000MB（1GB）** 内存。该内存值由 **hadoop-env.sh** 文件的
**HADOOP_HEAPSIZE** 参数控制。也可以通过设置环境变量为单个守护进程修改堆大小。例如，在 **yarn-env.sh**
文件中设置 **YARN_RESOURCEMANAGER_HEAPSIZE**，即 **可覆盖资源管理器的堆大小**。

令人惊讶的是，尽管namenode分配更多的堆空间是很常见的事，**但对于HDFS守护进程而言并没有相应的环境变量**。
当然有别的途径可以设置namenode堆空间大小。

除了守护进程对内存的需求，节点管理器还需为应用程序分配容器（container），因此需要综合考虑上述因素来
计算一个工作机器的总体内存需求。
```
一个守护进程究竟需要多少内存？

由于namenode会在内存中维护所有文件的每个数据块的引用，因此namenode很可能会“吃光”分配给它的所有内存。很难套用一个公式来精确
计算内存需求量，因为内存需求量取决于多个因素，包括每个文件包含的数据块数、文件名称的长度、文件系统中的目录数等。此外，在不同
Hadoop版本下，namenode的内存需求也不相同。

1000MB内存（默认配置）通常足够管理数百万个文件。但是根据经验来看，保守估计需要为每100万个数据块分配1000MB内存空间。

以一个含200节点的集群为例，假设每个节点有24TB磁盘空间，数据块大小是128MB，复本数是3的话，则约有200万个数据块（甚至更多）：
200 * 24000000MB /(128MB * 3)。因此，在本例中，namenode的内存空间最好一开始设为12000MB。

也可以只增加namenode的内存分配量而不改变其他Hadoop守护进程的内存分配，即设置hadoop-env.sh文件的HADOOP_NAMENODE_OPTS
属性包含一个JVM选项以设定内存大小。HADOOP_NAMENODE_OPTS允许向namenode的JVM传递额外的选项。以Sun JVM为例，-Xmx2000m
选项表示为namenode分配2000MB内存空间。

由于辅助namenode的内存需求量和主namenode差不多，所以一旦更改namenode的内存分配的话还需对辅助namenode做相同更改（使用
HADOOP_SECONDARYNAMENODE_OPTS变量）。
```

### 3.系统日志文件
默认情况下，Hadoop生成的系统日志文件存放在 **$HADOOP_HOME/logs** 目录之中，**也可以通过hadoop-env.sh
文件中的HADOOP_LOG_DIR来进行修改。建议修改默认设置，使之独立于Hadoop的安装目录**。这样的话，即使
Hadoop升级之后，安装路径发生变化，也不会影响日志文件的位置。**通常可以将日志文件存放在/var/log/hadoop
目录中**。实现方法很简单，就是在hadoop-env.sh中加入一行：
```shell
export HADOOP_LOG_DIR=/var/log/hadoop
```
**如果日志目录并不存在，则会首先创建该目录**（如果操作失败，请确认相关的Unix Hadoop用户是否有权创建该
目录）。运行在各台机器上的各个Hadoop守护进程会产生 **两类日志文件**。**第一类日志文件（以.log作为后缀
名）是通过log4j记录的。鉴于大部分应用程序的日志消息都写到该日志文件中，故障诊断的首要步骤即为检查该文件**。
标准的Hadoop log4j配置采用 **日常滚动文件追加方式（daily rolling file appender）** 来循环管理日
志文件。**系统不自动删除过期的日志文件，而是留待用户定期删除或存档，以节约本地磁盘空间**。

**第二类日志文件后缀名为.out，记录标准输出和标准错误日志。由于Hadoop使用log4j记录日志，所以该文件通常
只包含少量记录，甚至为空**。重启守护进程时，系统会创建一个新文件来记录此类日志。**系统仅保留最新的5个日志
文件**。旧的日志文件会附加一个介于1和5之间的数字后缀，5表示最旧的文件。

**日志文件的名称**（两种类型）包含运行守护进程的 **用户名称、守护进程名称和本地主机名等信息**。例如，
hadoop-hdfsdatanode-ip-10-45-174-112.log.2014-09-20就是一个日志文件的名称。这种命名方法保证集群
内所有机器的日志文件名称各不相同，从而可以将所有日志文件存到一个目录中。

**日志文件名称中的“用户名称”部分实际对应hadoop-env.sh文件中的HADOOP_IDENT_STRING项**。如果想采用其他
名称，可以修改HADOOP_IDENT_STRING项。

#### 4.SSH设置
**借助SSH协议，用户在主节点上使用控制脚本就能在（远程）工作节点上运行一系列指令**。自定义SSH设置会带来
诸多益处。例如，**减小连接超时设定（使用ConnectTimeout选项）** 可以避免控制脚本长时间等待宕机节点的响应。
当然，也不可设得过低，否则会导致繁忙节点被跳过。

**StrictHostKeyChecking** 也是一个很有用的SSH设置。**设置为no会自动将新主机键加到已知主机文件之中。
该项默认值是ask，提示用户确认是否已验证了“键指纹”（key fingerprint），因此不适合大型集群环境**。

**在hadoop-env.sh文件中定义HADOOP_SSH_OPTS环境变量还能够向SSH传递更多选项**。

### Hadoop守护进程的关键属性
Hadoop的配置属性之多简直让人眼花缭乱。**本节讨论对于真实的工作集群来说非常关键的一些属性**（或至少能
够理解默认属性的含义），这些属性分散在Hadoop的站点文件之中，包括 **core-site.xml、hdfs-site.xml和
yarn-site.xml**。

下面的 **3个范例** 分别列举了这些文件的典型实例（注意，这里没有列出MapReduce的站点文件。这是因为
**仅有的MapReduce守护进程就是作业历史服务器，对于它而言，默认属性值就够用了**）。
```xml
<!-- 典型的core-site.xml配置文件 -->
<?xml version="1.0"?>
<!-- core-site.xml -->
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode/</value>
    </property>
</configuration>
```
```xml
<!-- 典型的hdfs-site.xml配置文件 -->
<?xml version="1.0"?>
<!-- hdfs-site.xml -->
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/disk1/hdfs/name,/remote/hdfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/disk1/hdfs/data,/disk2/hdfs/data</value>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>/disk1/hdfs/namesecondary,/disk2/hdfs/namesecondary</vlaue>
    </property>
</configuration>
```
```xml
<!-- 典型的yarn-site.xml配置文件 -->
<?xml version="1.0"?>
<!-- yarn-site.xml -->
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>resourcemanager</value>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/disk1/nm-local-dir,/disk2/nm-local-dir</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce.shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>16384</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>16</value>
    </property>
</configuration>
```

#### 1.HDFS
**运行HDFS需要将一台机器指定为namenode**。在本例中，属性 **fs.defaultFS描述HDFS文件系统的URI，
其主机是namenode的主机名称或IP地址，端口是namenode监听RPC端口。如果没有指定，那么默认端口是8020**。

**属性fs.defaultFS也指定了默认文件系统，可以解析相对路径**。相对路径的长度更短，使用更便捷（不需要了解
特定namenode的地址）。例如，假设默认文件系统如上面的示例（core-site.xml）那样，则相对URI **/a/b解析
为hdfs://namenode/a/b**。

此外，还有其他一些关于HDFS的配置选项，包括namenode和datanode存储目录的属性。属性项 **dfs.namenode.name.dir
指定一系列目录来供namenode存储永久性的文件系统元素数据（编辑日志和文件系统映像）。这些元数据文件会同时备份
在所有指定目录中。通常情况下，通过配置dfs.namenode.name.dir属性可以将namenode元数据写到一两个本地磁盘
和一个远程磁盘（例如NFS挂载的目录）之中。这样的话，即使本地磁盘发生故障，甚至整个namenode发生故障，
都可以恢复元数据文件并且重构新的namenode。辅助namenode只是定期保存namenode的检查点，不维护namenode
的最新备份**。

属性 **dfs.datanode.data.dir可以设定datanode存储数据块的目录列表**。前面提到，dfs.namenode.name.dir
描述一系列目录，其目的是支持namenode进行冗余备份。**虽然dfs.datanode.data.dir也描述了一系列目录，
但是其目的是使datanode循环地在各个目录中写数据**。因此，**为了提高性能，最好分别为各个本地磁盘指定一个存储
目录。这样一来，数据块跨磁盘分布，针对不同数据块的读操作可以并发执行，从而提升读取性能**。

最后，**还需要指定辅助namenode存储文件系统的检查点的目录。属性dfs.namenode.checkpoint.dir指定一系列目录
来保存检查点。与namenode类似，检查点映像文件会分别存储在各个目录之中，以支持冗余备份**。

HDFS守护进程的关键属性：

| 属性名称 | 类型 | 默认值 | 说明 |
| :------------- | :------------- | :-------------- | :--------------- |
| **fs.defaultFS** | URI | file:/// | 默认文件系统。URI定义主机名称和namenode的RPC服务器工作的端口号，默认值是8020。本属性保存在core-site.xml中 |
| **dfs.namenode.name.dir** | 以逗号分隔的目录名称 | file://${hadoop.tmp.dir}/dfs/name | namenode存储永久性的元数据的目录列表。namenode在列表上的各个目录中均存放相同的元数据文件 |
| **dfs.datanode.data.dir** | 以逗号分隔的目录名称 | file://${hadoop.tmp.dir}/dfs/data | datanode存放数据块的目录列表。各个数据块分别存放于某一个目录中 |
| **dfs.namenode.checkpoint.dir** | 以逗号分隔的目录名称 | file://${hadoop.tmp.dir}/dfs/namesecondary | 辅助namenode存放检查点的目录列表。在所列每个目录中均存放一份检查点文件的副本 |

#### 2.YARN
为了运行YARN，**需要指定一台机器作为资源管理器**。最简单的做法是将属性 **yarn.resourcemanager.hostname
设置为用于运行资源管理器的机器的主机名或IP地址。资源管理器服务器的地址基本都可以从该属性获得**。例如，
**yarn.resourcemanager.address的格式为主机/端口对，yarn.resourcemanager.hostname表示默认主机**。
在MapReduce客户端配置中，需要通过RPC连接到资源管理器时，会用到这个属性。

**在执行MapReduce作业的过程中所产生的中间数据和工作文件被写到临时本地文件之中**。由于这些数据 **包括map任务
的输出数据，数据量可能非常大**，因此必须保证YARN容器本地临时存储空间（**由yarn.nodemanager.local-dirs
属性设置**）的容量足够大。**yarn.nodemanager.local-dirs属性使用一个逗号分隔的目录名称列表，最好将这些
目录分散到所有本地磁盘，以提升磁盘I/O操作的效率。通常情况下，YARN本地存储会使用与datanode数据块存储相同
的磁盘和分区（但是不同的目录）**。如前所述，datanode数据块存储目录由dfs.datanode.data.dir属性项指定。

与MapReduce 1 不同，**YARN** 没有tasktracker，**它依赖于shuffle句柄将map任务的输出送给reduce任务。
Shuffle句柄是长期运行于节点管理器的附加服务**。由于YARN是一个通用目的的服务，因此要通过将 **yarn-site.xml**
文件中的 **yarn.nodemanager.aux-services属性设置为mapreduce_shuffle来显式启用MapReduce的shuffle句柄**。

YARN守护进程的关键属性：

| 属性名称 | 类型 | 默认值 | 说明 |
| :------------- | :------------- | :------------- | :---------------- |
| **yarn.resourcemanager.hostname** | 主机名 | 0.0.0.0 | 运行资源管理器的机器主机名。以下缩写为${y.rm.hostname} |
| **yarn.resourcemanager.address** | 主机名和端口号 | ${y.rm.hostname}:8032 | 运行资源管理器的RPC服务器的主机名和端口 |
| **yarn.nodemanager.local-dirs** | 逗号分隔的目录名称 | ${hadoop.tmp.dir}/nm-local-dir | 目录列表，节点管理器允许容器将中间数据存于其中。当应用结束时，数据被清除 |
| **yarn.nodemanager.aux-services** | 逗号分隔的服务名称 | | 节点管理器运行的附加服务列表。每项服务由属性yarn.nodemanager.auxservices.servicename.class所定义的类实现。默认情况下，不指定附加服务 |
| **yarn.nodemanager.resource.memorymb** | int | 8192 | 节点管理器运行的容器可以分配到的物理内存容量（单位是MB）|

















































dd
