配置Hive
=================================================================================
### 新建一个空的hive-site.xml文件
在$HIVE_HOME/conf下新建空的hive-site.xml文件。
```shell
su - hadoop
$ cd /opt/apache-hive-1.2.2/conf/
$ touch hive-site.xml
```
**编辑hive-site.xml**，添加如下文本：
```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>

</configuration>
```

### 创建Hive的HDFS存储目录
```shell
su - hadoop
$ hadoop fs -mkdir -p /user/hive/warehouse  
$ hadoop fs -mkdir -p /tmp  
$ hadoop fs -chmod -R g+w /user/hive/warehouse  
$ hadoop fs -chmod -R g+w /tmp
```

### 添加连接MySQL的配置
编辑$HIVE_HOME/conf/hive-site.xml文件，添加：
```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://172.16.177.168:3306/hivedb?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>myhive</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>MyHive99@</value>
    </property>
</configuration>
```

### 复制MySQL驱动到$HIVE_HOME/lib目录
```shell
cp mysql-connector-java-5.1.38.jar $HIVE_HOME/lib
```

### 编辑$HIVE_HOME/bin/hive文件
修改hive执行文件，找到如下文本：
```shell
sparkAssemblyPath=`ls ${SPARK_HOME}/lib/spark-assembly-*.jar`
```
**将其修改为**：
```shell
sparkAssemblyPath=`ls ${SPARK_HOME}/jars/*.jar`
```
```
为什么要这么做？？

版本：
apache-hive-1.2.2-bin.tar.gz
spark-2.2.1-bin-hadoop2.7.tgz

1.问题陈述：
启动Hive的时候会说找不到spark-assembly相关的Jar包
cannot access /usr/local/spark/lib/spark-assembly-*.jar: No such file or directory

2.原因：
spark2以后，原有lib目录下的大JAR包被分散成多个小JAR包，原来的spark-assembly-*.jar已经不存在，所以hive没有办法找到这
个JAR包。

3.解决办法
进入hive安装路径下的bin目录下，编辑hive
找到下面这行shell脚本：
sparkAssemblyPath=`ls ${SPARK_HOME}/lib/spark-assembly-*.jar`  
将其修改为：sparkAssemblyPath=`ls ${SPARK_HOME}/jars/*.jar`
```
