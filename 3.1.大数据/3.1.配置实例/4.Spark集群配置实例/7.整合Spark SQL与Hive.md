整合Spark SQL与Hive
=================================================================================
```
说明：
该测试计划配置Hadoop集群，一共三个节点：
    node-master（172.16.177.166）
    node-slave01（172.16.177.167）
    node-slave02（172.16.177.168）

node-master：NameNode，DataNode，ResourceManager，NodeManager，worker（非进程）
node-slave01：DataNode，NodeManager，SecondaryNameNode，worker（非进程）
node-slave02: DataNode，NodeManager，worker（非进程）
```

### 复制$HIVE_HOME/conf/hive-site.xml到$SPARK_HOME/conf目录并编辑它
```shell
$ sudo cp $HIVE_HOME/conf/hive-site.xml $SPARK_HOME/conf
```
编辑该hive-site.xml文件，并添加：
```xml
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
</property>
```
添加该配置后的文件大概如下：
```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://172.16.177.168:3306/hivedb?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>myhive</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>MyHive99@</value>
    </property>
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>
</configuration>
```
**注：三个节点都要操作一遍**。

### 修改Spark的日志级别为WARN
```shell
su - hadoop
$ cd $SPARK_HOME/conf
$ cp log4j.properties.template log4j.properties
```
编辑log4j.properties文件：

修改：
```ini
log4j.rootCategory=INFO, console
```
为：
```ini
log4j.rootCategory=WARN, console
```
**注：三个节点都要操作一遍**。

### 把MySQL的jar包添加到Spark的类路径
跟前面一样，把jar包放到$SPARK_HOME/jars目录中，并添加到hdfs中。Spark-2.2.1使用的是
mysql-connector-java:5.1.38版本。
```shell
su - hadoop
$ cp mysql-connector-java-5.1.38.jar $SPARK_HOME/jars
$ hadoop fs -put $SPARK_HOME/jars/mysql-connector-java-5.1.38.jar /spark/jars
```

### 运行 Spark SQL CLI
**请注意，Spark SQL CLI不能与Thrift JDBC服务器通信**。
```shell
$ spark-sql --master yarn
```
**注：按ctrl+D结束进程！！！** 要不然spark-submit进程结束不了。

### 验证
```shell
$ spark-sql --master yarn

18/03/01 17:23:55 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
18/03/01 17:23:55 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
18/03/01 17:24:13 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
spark-sql>
```
**注：上面三个警告，我没有办法，好像就是这样！**

#### 1.创建库
```sql
spark-sql> create database db1;

18/03/01 18:03:37 WARN ObjectStore: Failed to get database db1, returning NoSuchObjectException
Time taken: 0.793 seconds
```

#### 2.创建表
```sql
spark-sql> use db1;
Time taken: 0.016 seconds

spark-sql> create table tab1(i int,str string);
18/03/01 18:05:02 WARN HiveMetaStore: Location: hdfs://node-master/user/hive/warehouse/db1.db/tab1 specified for non-external table:tab1
Time taken: 0.333 seconds

spark-sql> show tables;
db1	tab1	false
Time taken: 0.518 seconds, Fetched 1 row(s)
```
示例中，tab1表已经正确的创建在 **hdfs://node-master/user/hive/warehouse/db1.db/tab1位置**，
证明配置成功！！！！！
