编辑spark-defalut.conf配置文件
=================================================================================
### 在hdfs中创建必要的目录
```shell
$ su - hadoop
$ hadoop fs -mkdir -p /spark/logs
# spark程序需要的jar包都放这里来
$ hadoop fs -mkdir -p /spark/jars
```

**进入$SPARK_HOME/conf目录，copy模板文件spark-defaults.conf.template并重命名为spark-defaults.conf**：
```shell
$ cp spark-defaults.conf.template spark-defaults.conf
```
**编辑spark-defaults.conf文件**：
```
spark.eventLog.enabled                       true
spark.eventLog.dir                           hdfs://node-master/spark/logs
spark.yarn.jars                              hdfs://node-master/spark/jars
spark.yarn.appMasterEnv.SPARK_HOME           /opt/spark-2.2.1-bin-hadoop2.7
spark.yarn.appMasterEnv.SCALA_HOME           /opt/scala-2.11.8
spark.yarn.appMasterEnv.JAVA_HOME            /opt/jdk1.8.0_151
spark.yarn.appMasterEnv.HADOOP_HOME          /opt/hadoop-2.9.0
spark.yarn.appMasterEnv.HADOOP_CONF_DIR      /opt/hadoop-2.9.0/etc/hadoop
```
