编辑spark-defalut.conf配置文件
=================================================================================
```
说明：
该测试计划配置Hadoop集群，一共三个节点：
    node-master（172.16.177.166）
    node-slave01（172.16.177.167）
    node-slave02（172.16.177.168）

node-master：NameNode，DataNode，ResourceManager，NodeManager，worker（非进程）
node-slave01：DataNode，NodeManager，worker（非进程）
node-slave02: DataNode，NodeManager，SecondaryNameNode，worker（非进程）
```

### 在hdfs中创建必要的目录
**注：下面的操作只需要在hadoop的master上做一次**。
```shell
$ su - hadoop
$ hadoop fs -mkdir -p /spark/logs
$ hadoop fs -mkdir -p /spark/jars
$ cd /opt/spark-2.2.1-bin-hadoop2.7
$ hadoop fs -put jars/* /spark/jars
```

**进入$SPARK_HOME/conf目录，copy模板文件spark-defaults.conf.template并重命名为spark-defaults.conf**：
```shell
su - hadoop
$ cp spark-defaults.conf.template spark-defaults.conf
```
**编辑spark-defaults.conf文件**：
```
spark.eventLog.enabled                       true
spark.eventLog.dir                           hdfs://node-master/spark/logs
spark.yarn.jars                              hdfs://node-master/spark/jars/*
spark.yarn.appMasterEnv.SPARK_HOME           /opt/spark-2.2.1-bin-hadoop2.7
spark.yarn.appMasterEnv.SCALA_HOME           /opt/scala-2.11.8
spark.yarn.appMasterEnv.JAVA_HOME            /opt/jdk1.8.0_151
spark.yarn.appMasterEnv.HADOOP_HOME          /opt/hadoop-2.9.0
spark.yarn.appMasterEnv.HADOOP_CONF_DIR      /opt/hadoop-2.9.0/etc/hadoop
```
