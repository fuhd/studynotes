安装并配置Spark
=================================================================================
### 解压Spark
例如，解压到/opt目录下：
```shell
$ sudo tar xzvf spark-2.2.1-bin-hadoop2.7.tgz -C /opt/
```
```
注：
在/etc/profile中就不配置$SPARK_HOME与$PATH了，以免spark的脚本与hadoop的脚本冲突。
比如，都有一个start-all.sh的脚本。
```

### 编辑spark-env.sh文件








































ddd
