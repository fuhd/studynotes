安装并配置Spark
=================================================================================
### 解压Spark
例如，解压到/opt目录下：
```shell
$ sudo tar xzvf spark-2.2.1-bin-hadoop2.7.tgz -C /opt/
```
```
注：
在/etc/profile中就不配置$SPARK_HOME与$PATH了，以免spark的脚本与hadoop的脚本冲突。
比如，都有一个start-all.sh的脚本。
```

### 编辑spark-env.sh文件
假定Spark的安装目录为：$SPARK_HOME。**进入$SPARK_HOME/conf目录，copy模板文件spark-env.sh.template
并重命名为spark-env.sh**：
```shell
$ sudo cp spark-env.sh.template spark-env.sh
```
编辑spark-env.sh：
```shell

```






































ddd
