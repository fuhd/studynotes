编辑spark-env.sh配置文件
=================================================================================
```
说明：
该测试计划配置Hadoop集群，一共三个节点：
    node-master（172.16.177.166）
    node-slave01（172.16.177.167）
    node-slave02（172.16.177.168）

node-master：NameNode，DataNode，ResourceManager，NodeManager，worker（非进程）
node-slave01：DataNode，NodeManager，worker（非进程）
node-slave02: DataNode，NodeManager，SecondaryNameNode，worker（非进程）
```

**进入$SPARK_HOME/conf目录，copy模板文件spark-env.sh.template并重命名为spark-env.sh**：
```shell
su - hadoop
$ sudo cp spark-env.sh.template spark-env.sh
```
**编辑spark-env.sh**，添加如下配置项（**下面的配置可能是不需要的，先放这里**）：
```shell
# export SPARK_HOME=/opt/spark-2.2.1-bin-hadoop2.7
# export SCALA_HOME=/opt/scala-2.11.8
# export JAVA_HOME=/opt/jdk1.8.0_151
# export HADOOP_HOME=/opt/hadoop-2.9.0
# export HADOOP_CONF_DIR=/etc/hadoop

```
