编辑spark-env.sh配置文件
=================================================================================
```
说明：
该测试计划配置Hadoop集群，一共三个节点：
    node-master（172.16.177.166）
    node-slave01（172.16.177.167）
    node-slave02（172.16.177.168）

node-master：NameNode，DataNode，ResourceManager，NodeManager，worker（非进程）
node-slave01：DataNode，NodeManager，SecondaryNameNode，worker（非进程）
node-slave02: DataNode，NodeManager，worker（非进程）
```


**进入$SPARK_HOME/conf目录，copy模板文件spark-env.sh.template并重命名为spark-env.sh**：
```shell
su - hadoop
$ sudo cp spark-env.sh.template spark-env.sh
```
**编辑spark-env.sh**，添加如下配置项：
```shell
# 看情况是否要加这个配置！！！！！！！！！
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
```
```
说明
为什么要加 LD_LIBRARY_PATH=$HADOOP_HOME/lib/native 这个配置项？？？

原因
如果spark预先编译时的hadoop版本为32位，那么放在64位的机器上执行会报这个错误：
WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
      using builtin-java classes where applicable

当然了，如果没有这个警告信息，也不用加这个配置项。

解决办法
进入到$SPARK_HOME的conf目录下，在spark-env.sh配置文件中加入 LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
```
