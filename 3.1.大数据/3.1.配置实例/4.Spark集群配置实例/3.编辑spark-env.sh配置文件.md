编辑spark-env.sh配置文件
=================================================================================
```
说明：
该测试计划配置Hadoop集群，一共三个节点：
    node-master（172.16.177.166）
    node-slave01（172.16.177.167）
    node-slave02（172.16.177.168）

node-master：NameNode，DataNode，ResourceManager，NodeManager，worker（非进程）
node-slave01：DataNode，NodeManager，worker（非进程）
node-slave02: DataNode，NodeManager，SecondaryNameNode，worker（非进程）
```

**进入$SPARK_HOME/conf目录，copy模板文件spark-env.sh.template并重命名为spark-env.sh**：
```shell
su - hadoop
$ sudo cp spark-env.sh.template spark-env.sh
```
**编辑spark-env.sh**，添加如下配置项：
```shell
# export SPARK_HOME=/opt/spark-2.2.1-bin-hadoop2.7
# export SCALA_HOME=/opt/scala-2.11.8
# export JAVA_HOME=/opt/jdk1.8.0_151
# export HADOOP_HOME=/opt/hadoop-2.9.0
# export HADOOP_CONF_DIR=/etc/hadoop
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
```
```
说明
为什么要加 LD_LIBRARY_PATH=$HADOOP_HOME/lib/native 这个配置项？？？

原因
如果spark预先编译时的hadoop版本为32位，那么放在64位的机器上执行会报这个错误：
WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
      using builtin-java classes where applicable
      
当然了，如果没有这个警告信息，也不用加这个配置项。

解决办法
进入到$SPARK_HOME的conf目录下，在spark-env.sh配置文件中加入 LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
```
```
注意：
上面的配置项（除了 LD_LIBRARY_PATH=$HADOOP_HOME/lib/native），只有在 --deploy-mode client 时，才是需要的。
--deploy-mode cluster不在这里配置，在spark-default.conf中配置。
```
