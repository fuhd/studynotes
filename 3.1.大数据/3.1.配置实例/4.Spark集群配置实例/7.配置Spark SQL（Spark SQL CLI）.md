配置Spark SQL（Spark SQL CLI）
=================================================================================
```
说明：
该测试计划配置Hadoop集群，一共三个节点：
    node-master（172.16.177.166）
    node-slave01（172.16.177.167）
    node-slave02（172.16.177.168）

node-master：NameNode，DataNode，ResourceManager，NodeManager，worker（非进程）
node-slave01：DataNode，NodeManager，SecondaryNameNode，worker（非进程）
node-slave02: DataNode，NodeManager，worker（非进程）
```

### 复制$HIVE_HOME/conf/hive-site.xml到$SPARK_HOME/conf目录
```shell
$ sudo cp $HIVE_HOME/conf/hive-site.xml $SPARK_HOME/conf
```
**注：三个节点都要操作一遍**。

### 修改Spark的日志级别为WARN
```shell
su - hadoop
$ cd $SPARK_HOME/conf
$ cp log4j.properties.template log4j.properties
```
编辑log4j.properties文件：

修改：
```ini
log4j.rootCategory=INFO, console
```
为：
```ini
log4j.rootCategory=WARN, console
```

### 关闭Hive的版本验证
**因为Spark-SQL是hive的变种，Spark中的Hive版本号一般验证通不过，所以就不验证了！！目前好像不用加这个配置，
现在默认就是false!!**
```xml
......
<property>    
   <name>hive.metastore.schema.verification</name>    
   <value>false</value>    
</property>
......
```

### 把MySQL的jar包添加到Spark的类路径
跟前面一样，把jar包放到$SPARK_HOME/jars目录中，并添加到hdfs中。Spark-2.2.1使用的是
mysql-connector-java:5.1.38版本。
```shell
su - hadoop
$ cp mysql-connector-java-5.1.38.jar $SPARK_HOME/jars
$ hadoop fs -put $SPARK_HOME/jars/mysql-connector-java-5.1.38.jar /spark/jars
```

### 运行 Spark SQL CLI
请注意，Spark SQL CLI不能与Thrift JDBC服务器通信。
```shell
$ spark-sql --master yarn
```
