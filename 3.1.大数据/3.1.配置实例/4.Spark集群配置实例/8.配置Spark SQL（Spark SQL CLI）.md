配置Spark SQL（Spark SQL CLI）
=================================================================================
### 在$SPARK_HOME/conf下新建hive-site.xml配置文件
```shell
$ cd $SPARK_HOME/conf
$ touch hive-site.xml
```

### 配置Spark SQL在hdfs中的存储目录
执行创建 **hdfs目录** 的命令：
```shell
su - hadoop
$ hadoop fs -mkdir -p /spark/hive/warehouse
$ hadoop fs -chmod -R g+w /spark/hive/warehouse
```
编辑 **$SPARK_HOME/conf/hive-site.xml** 文件，添加如下配置：
```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>spark.sql.warehouse.dir</name>
        <value>/spark/hive/warehouse</value>
    </property>
</configuration>
```

### 编辑MySQL相关的配置（元数据存储）
编辑 **$SPARK_HOME/conf/hive-site.xml** 文件，添加如下配置：
```xml
......
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://172.16.177.167:3306/hivedb?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>myhive</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive123</value>
</property>
......
```





































zzz
