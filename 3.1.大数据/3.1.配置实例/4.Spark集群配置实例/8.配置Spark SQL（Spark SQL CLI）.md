配置Spark SQL（Spark SQL CLI）
=================================================================================
```
说明：
该测试计划配置Hadoop集群，一共三个节点：
    node-master（172.16.177.166）
    node-slave01（172.16.177.167）
    node-slave02（172.16.177.168）

node-master：NameNode，DataNode，ResourceManager，NodeManager，worker（非进程）
node-slave01：DataNode，NodeManager，SecondaryNameNode，worker（非进程）
node-slave02: DataNode，NodeManager，worker（非进程）
```

### 在$SPARK_HOME/conf下新建hive-site.xml配置文件
```shell
$ cd $SPARK_HOME/conf
$ touch hive-site.xml
```

### 配置Spark SQL在hdfs中的存储目录
执行创建 **hdfs目录** 的命令：
```shell
su - hadoop
$ hadoop fs -mkdir -p /spark/hive/warehouse
$ hadoop fs -chmod -R g+w /spark/hive/warehouse
```
编辑 **$SPARK_HOME/conf/hive-site.xml** 文件，添加如下配置：
```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>spark.sql.warehouse.dir</name>
        <value>/spark/hive/warehouse</value>
    </property>
</configuration>
```

### 编辑MySQL相关的配置（元数据存储）
编辑 **$SPARK_HOME/conf/hive-site.xml** 文件，添加如下配置：
```xml
......
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://172.16.177.168:3306/hivedb?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>myhive</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>MyHive99@</value>
</property>
......
```
**注：编辑完成的hive-site.xml文件分发到其它节点上**。

### 关闭Hive的版本验证
**因为Spark-SQL是hive的变种，Spark中的Hive版本号一般验证通不过，所以就不验证了！！**
```xml
......
<property>    
   <name>hive.metastore.schema.verification</name>    
   <value>false</value>    
</property>
......
```

### 把MySQL的jar包添加到Spark的类路径
跟前面一样，把jar包放到$SPARK_HOME/jars目录中，并添加到hdfs中。Spark-2.2.1使用的是
mysql-connector-java:5.1.38版本。
```shell
su - hadoop
$ cp mysql-connector-java-5.1.38.jar $SPARK_HOME/jars
$ hadoop fs -put $SPARK_HOME/jars/mysql-connector-java-5.1.38.jar /spark/jars
```

### 运行 Spark SQL CLI
请注意，Spark SQL CLI不能与Thrift JDBC服务器通信。
```shell
$ spark-sql --master yarn
```
