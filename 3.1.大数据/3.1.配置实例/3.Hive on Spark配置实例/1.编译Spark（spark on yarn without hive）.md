编译Spark（spark on yarn without hive）
=================================================================================
下面的操作都是在本地机器上完成。

### 安装Java
略

### 安装Scala
首先，把Scala安装包解压到你想安装的目录，如："/opt/scala-2.11.8"。然后在 **/etc/profile** 中新
增配置：
```shell
export SCALA_HOME=/opt/scala-2.11.8
# 这里隐藏了其他配置
export PATH=$SCALA_HOME/bin:$PATH
```
接下来，执行命令：
```shell
$ source /etc/profile
```

### 安装Maven
把Maven安装包解压到你想安装的目录，如："/opt/apache-maven-3.5.0"。然后在 **/etc/profile** 中新
增配置：
```shell
export MVN_HOME=/opt/apache-maven-3.5.0
# 这里隐藏了其他配置
export PATH=$MVN_HOME/bin:$PATH
```
接下来，执行命令：
```shell
$ source /etc/profile
```

### 编译Spark
解压Spark源码包，进入spark源代码根目录执行下面程序进行编译（**必须是spark2.0+**）：
```shell
$ ./dev/make-distribution.sh --name "hadoop2-without-hive" --tgz "-Pyarn,hadoop-provided,hadoop-2.7,parquet-provided"
```
**编译完成后会在根目录生成spark-2.0.0-bin-hadoop2-without-hive.tgz**。
