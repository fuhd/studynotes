Sqoop2的安装与配置
=================================================================================
```
说明：
该测试计划配置Hadoop集群，一共三个节点：
    node-master（172.16.177.166）
    node-slave01（172.16.177.167）
    node-slave02（172.16.177.168）

node-master：NameNode，DataNode，ResourceManager，NodeManager，worker（非进程）
node-slave01：DataNode，NodeManager，SecondaryNameNode，worker（非进程），sqoop2 server
node-slave02: DataNode，NodeManager，worker（非进程），Thrift服务
```

### 解压并安装
```shell
su - hadoop
$ tar xzvf sqoop-1.99.7-bin-hadoop200.tar.gz
$ mv sqoop-1.99.7-bin-hadoop200 sqoop-1.99.7
$ sudo mv sqoop-1.99.7 /opt/
$ sudo chown -R hadoop:hadoop sqoop-1.99.7/
```

### 编辑/etc/profile
确保有 **$HADOOP_HOME** 环境变量（前面配置过了）。

### 编辑Hadoop的core-site.xml文件
添加如下配置：
```xml
<property>
    <name>hadoop.proxyuser.hadoop.hosts</name>
    <value>*</value>
</property>
<property>
    <name>hadoop.proxyuser.hadoop.groups</name>
    <value>*</value>
</property>
```
```
注意：Hadoop所有节点都需要配置！！！
```

### 创建第三方Jar包目录并配置环境变量
```shell
$ sudo mkdir -p /opt/sqoop2_extra_lib
# 复制mysql驱动到目录（我前面在$SPARK_HOME/jars中放置了MySQL驱动的）
$ sudo cp $SPARK_HOME/jars/mysql-connector-java-5.1.38.jar /opt/sqoop2_extra_lib/
$ sudo chown -R hadoop:hadoop sqoop2_extra_lib/
```
编程/etc/profile文件，添加：
```shell
#################### JAVA ####################
export JAVA_HOME=/opt/jdk1.8.0_151
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin

#################### SCALA ###################
export SCALA_HOME=/opt/scala-2.11.8
export PATH=$PATH:$SCALA_HOME/bin

################### HADOOP ###################
export HADOOP_HOME=/opt/hadoop-2.9.0
export HADOOP_CONF_DIR=/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

#################### SPARK ##################
export SPARK_HOME=/opt/spark-2.2.1-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

#################### sqoop2 #################
# 这个配置，好像也可以在sqoop.properties文件中配置
export SQOOP_SERVER_EXTRA_LIB=/opt/sqoop2_extra_lib
```

### 配置PATH
编辑/etc/profile文件，添加：
```shell
#################### JAVA ####################
export JAVA_HOME=/opt/jdk1.8.0_151
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin

#################### SCALA ###################
export SCALA_HOME=/opt/scala-2.11.8
export PATH=$PATH:$SCALA_HOME/bin

################### HADOOP ###################
export HADOOP_HOME=/opt/hadoop-2.9.0
export HADOOP_CONF_DIR=/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

#################### SPARK ##################
export SPARK_HOME=/opt/spark-2.2.1-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

#################### sqoop2 #################
# 这个配置，好像也可以在sqoop.properties文件中配置
export SQOOP_SERVER_EXTRA_LIB=/opt/sqoop2_extra_lib
export SQOOP_HOME=/opt/sqoop-1.99.7
export PATH=$PATH:$SQOOP_HOME/bin
```
执行：
```shell
$ source /etc/profile
```

### 编辑sqoop.properties文件
```shell
# 编辑该属性，因为我们的配置在/etc/hadoop目录下
org.apache.sqoop.submission.engine.mapreduce.configuration.directory=/etc/hadoop

org.apache.sqoop.log4j.appender.file.File=/opt/sqoop-1.99.7/logs/sqoop.log
org.apache.sqoop.log4j.appender.audit.File=/opt/sqoop-1.99.7/logs/audit.log
org.apache.sqoop.repository.sysprop.derby.stream.error.file=/opt/sqoop-1.99.7/logs/derbyrepo.log

org.apache.sqoop.repository.jdbc.url=jdbc:derby:/opt/sqoop-1.99.7/repository/db;create=true
```

### 存储库初始化
```shell
su - hadoop

```



































ddd
