在idea中调试spark代码（基于yarn-client）
================================================================================
代码：
```scala
package chapter3

import org.apache.spark.{SparkConf, SparkContext}

object Chapter3_3_2 {
  def main(args: Array[String]): Unit = {
    System.setProperty("HADOOP_USER_NAME", "admin")
    val conf = new SparkConf()
      .setMaster("yarn-client")
      .setJars(List("/home/fuhd/work/workspace/scala/spark-demo/target/spark-demo-1.0-SNAPSHOT.jar"))
      .setIfMissing("spark.driver.host", "192.168.4.188")
      .set("spark.cores.max", "3")
      .set("spark.executor.memory", "1g")
      .set("spark.yarn.jars", "hdfs://ns/user/maxcompute/yarn_jars/spark_2.4.0/*.jar")
      .setAppName("WC")
    val sc = new SparkContext(conf)
    sc.setLogLevel("DEBUG")
    //如果hdfs-site.xml,core-site.xml,yarn-site.xml在classpath中，hdfs://ns前缀会自动添加到下面的路径中
    sc.textFile("/user/maxcompute/users/admin/text/abc.txt")
      .flatMap(_.split(" "))
      .map((_, 1))
      .reduceByKey(_ + _, 1)
      .sortBy(_._2, false)
      //如果hdfs-site.xml,core-site.xml,yarn-site.xml在classpath中，hdfs://ns前缀会自动添加到下面的路径中
      .saveAsTextFile("/user/maxcompute/users/admin/text/results")
    sc.stop()
  }
}
```
pom.xml:
```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.example</groupId>
    <artifactId>spark-demo</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
       <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>2.11.12</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.4.0</version>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.hadoop</groupId>
                    <artifactId>hadoop-client</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.scala-lang</groupId>
                    <artifactId>scala-library</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>2.7.7</version>
        </dependency>
        <dependency>
            <groupId>org.apache.httpcomponents</groupId>
            <artifactId>httpclient</artifactId>
            <version>4.5.12</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.scala-tools</groupId>
                <artifactId>maven-scala-plugin</artifactId>
                <version>2.15.2</version>
                <executions>
                    <execution>
                        <id>scala-compile-first</id>
                        <goals>
                            <goal>compile</goal>
                        </goals>
                        <configuration>
                            <includes>
                                <include>*/*.scala</include>
                            </includes>
                        </configuration>
                    </execution>
                    <execution>
                        <id>scala-test-compile</id>
                        <goals>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>

    <repositories>
        <repository>
            <id>1</id>
            <name>MAVEN-CENTRE</name>
            <url>http://central.maven.org/maven2/</url>
        </repository>
    </repositories>
</project>
```

## 异常1
```
org.apache.spark.SparkException: Could not parse Master URL: 'yarn'
```

### 原因
没有添加`spark-yarn_2.11`包。

### 解决
添加：
```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-yarn_2.11</artifactId>
    <version>2.4.0</version>
</dependency>
```

## 异常2
一直重复打印：
```
Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s); 
retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
```

### 原因
没有把hdfs-site.xml、core-site.xml、yarn-site.xml这三个hadoop的配置文件copy到工程的
resources资源目录中来。

### 解决
去hadoop集群中导出这三个文件放到resources目录中。

## 异常3
错误日志：
```
Exit code: 50
Stack trace: ExitCodeException exitCode=50: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:585)
	at org.apache.hadoop.util.Shell.run(Shell.java:482)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```
一直查不出来原因。最后去hdfs上找application_xxxxxxxxx_xxxx日志，发现：
```
20/09/21 19:28:44 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.io.InvalidClassException: org.apache.spark.executor.InputMetrics; local class incompatible: 
stream classdesc serialVersionUID = -2738058365267886524, local class serialVersionUID = -1517808254557426315
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
```

### 原因 
spark相关jar的版本不对（**采用yarn-client模式运行的**），本地jar依赖与hdfs中的jars版本不匹配。

### 解决
替换为hdfs中相同版本的jar包。
