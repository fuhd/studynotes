Spark jar如何在D-Lite数据中台中使用
================================================================================
## scala版本示例
**实现SparkJob**：
```scala
package chapter3

import com.github.jobserver.api.{SparkJob, SparkJobLogger}
import org.apache.spark.sql.SparkSession

class Chapter3_3_3 extends SparkJob {
  override def runJob(sparkSession: SparkSession, sparkJobLogger: SparkJobLogger, args: Array[String]): Unit = {
    val sc = sparkSession.sparkContext
    sc.textFile(args(0))
      .flatMap(_.split(" "))
      .map((_, 1))
      .reduceByKey(_ + _, 1)
      .saveAsTextFile(args(1))
  }
}
```

## Java版本示例
**实现SparkJob**：
```java
package org.why;

import com.github.jobserver.api.SparkJob;
import com.github.jobserver.api.SparkJobLogger;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.sql.SparkSession;
import scala.Tuple2;

public final class WordCount8 implements SparkJob {

    public void runJob(SparkSession sparkSession, SparkJobLogger logger, String[] args) throws Exception {
        JavaSparkContext sc = new JavaSparkContext(sparkSession.sparkContext());
        JavaRDD<String> rdd1 = sc.textFile(args[0]);
        JavaRDD<String> rdd2 = rdd1.flatMap(new FlatMapFunction<String, String>() {
            public Iterator<String> call(String s) throws Exception {
                List<String> list = new ArrayList();
                String[] arr = s.split(" ");
                String[] var4 = arr;
                int var5 = arr.length;

                for(int var6 = 0; var6 < var5; ++var6) {
                    String ss = var4[var6];
                    list.add(ss);
                }

                return list.iterator();
            }
        });
        JavaPairRDD<String, Integer> rdd3 = rdd2.mapToPair(new PairFunction<String, String, Integer>() {
            public Tuple2<String, Integer> call(String s) throws Exception {
                return new Tuple2(s, 1);
            }
        });
        JavaPairRDD<String, Integer> rdd4 = rdd3.reduceByKey(new Function2<Integer, Integer, Integer>() {
            public Integer call(Integer v1, Integer v2) throws Exception {
                return v1 + v2;
            }
        });
        Iterator var9 = rdd4.collect().iterator();

        while(var9.hasNext()) {
            Object o = var9.next();
            System.out.println(o);
        }

        rdd4.saveAsTextFile(args[1]);
    }
}
```