问题1
================================================================================
## 问题描述 
执行PySpark代码，如：
```python
#/bin/python3 
# 注意：请尽量不要在作业中访问大数据平台外部外部网络，包含http接口和mysql等
def main(sparkSession):
	spark = SparkSession.builder.appName("pySpark-test").getOrCreate()
	spark.sql("SELECT * FROM tythin01.dwd_dmp_assets_01_dt where ds = ${bizdate}").show()
```
有时候会报如下错误，并且会不定时的出现：
```
py4j.protocol.Py4JError: org.apache.spark.api.python.PythonUtils... does not exist in the JVM
```

## 原因 
没有找到找到安装的spark的环境。


## 解决
在从pyspark导入SparkConf之前先执行下面的语句：
```python
import findspark
findspark.init()
```
如上例：
```python
#/bin/python3 
# 注意：请尽量不要在作业中访问大数据平台外部外部网络，包含http接口和mysql等

import findspark
findspark.init()

def main(sparkSession):
	spark = SparkSession.builder.appName("pySpark-test").getOrCreate()
	spark.sql("SELECT * FROM tythin01.dwd_dmp_assets_01_dt where ds = ${bizdate}").show()
```