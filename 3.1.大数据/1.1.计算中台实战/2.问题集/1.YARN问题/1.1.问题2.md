问题2
================================================================================
## 问题描述1
Hadoop集群环境中，yarn采用的HA方案，关键配置如下：
```xml
...
<property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value>true</value>
</property>
<property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>rmcluster</value>
</property>
<property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value>rm005,rm008</value>
</property>
<property>
    <name>yarn.resourcemanager.hostname.rm005</name>
    <value>bigdata005</value>
</property>
<property>
    <name>yarn.resourcemanager.hostname.rm008</name>
    <value>bigdata008</value>
</property>

...

<property>
    <name>yarn.resourcemanager.address.rm005</name>
    <value>bigdata005:8032</value>
</property>
<property>
    <name>yarn.resourcemanager.scheduler.address.rm005</name>
    <value>bigdata005:8030</value>
</property>
<property>
    <name>yarn.resourcemanager.resource-tracker.address.rm005</name>
    <value>bigdata005:8031</value>
</property>
<property>
    <name>yarn.resourcemanager.address.rm008</name>
    <value>bigdata008:8032</value>
</property>
<property>
    <name>yarn.resourcemanager.scheduler.address.rm008</name>
    <value>bigdata008:8030</value>
</property>
<property>
    <name>yarn.resourcemanager.resource-tracker.address.rm008</name>
    <value>bigdata008:8031</value>
</property>
```
在运行官方提供的Pi示例时报如下错误：
```shell
bin/spark-submit 
    --class org.apache.spark.examples.SparkPi 
    --master yarn  
    --deploy-mode client 
    --executor-memory 1G 
    --total-executor-cores 3 
    /usr/local/spark-2.4.0-bin-2.7.7/examples/jars/spark-examples_2.11-2.4.0.jar 
    100
```
首先报错如下：
```
Exception in thread "main" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
        at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:657)
        at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:290)
        at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:251)
        at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:120)
        at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$1.<init>(SparkSubmit.scala:911)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:911)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:81)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```
**没有设置环境变量：HADOOP_CONF_DIR 或者 YARN_CONF_DIR**。

## 解决方案1
1. 可以在控制台直接设置：
```shell
export HADOOP_CONF_DIR=xxxxxxxxxxxxx
#这个配置好像不是必须的
export YARN_CONF_DIR=xxxxxxxxxxxxxxxxx
```
2. 也可以修改 **`spark-env.sh`** 文件，添加：
```shell
export HADOOP_CONF_DIR=xxxxxxxxxxxxx
#这个配置好像不是必须的
export YARN_CONF_DIR=xxxxxxxxxxxxxxxxx
```

## 问题描述2
连续输出大量的：
```
......
Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s)
Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s)
......
```
发现是我的 **HADOOP_CONF_DIR环境变量配置错误了**，我直接指向了`$HADOOP_HOME`目录，应该指向
**`$HADOOP_HOME/etc/hadoop`**

## 解决方案2
修改如下：
```shell
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
#这个配置好像不是必须的
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
```
