flink-sql安装
================================================================================
**在bigdata005上操作**

## 1.安装jar到仓库
```shell
#flinkStreamSql的jars子目录下
mvn install:install-file -DgroupId=com.cloudera.impala.jdbc -DartifactId=ImpalaJDBC41 -Dversion=2.6.12 -Dpackaging=jar -Dfile=ImpalaJDBC41.jar
```

## 2.修改配置信息
编辑`\xxx\flinkStreamSql\launcher\src\main\resources\application.yml`，修改为当前配
置值。
```yaml
flink:
  flinkConfig: "/usr/local/flink-1.7.2/conf/"                     #flink的配置文件路径
  yarnConfig: "/etc/hadoop/conf"                                  #hadoop的配置文件路径
  yarnModel: "yarn"                                               #flink-sql提交模式
  flinkPluginRoot: "/usr/local/flink-sql/plugins"                 #flink-sql编译后的jar路径
```

## 3.本地编译打包
**在父pom中禁用pl.project13.maven:git-commit-id-plugin:2.2.6插件**
```shell
#进入本地flink-sql目录，执行以下命令编译打包
mvn clean package -Dmaven.test.skip
```

## 4.服务部署
打包后将 **bin,lib,plugins** 放到 **bigdata005** 上的 **/usr/local/flink-sql/**。
```shell
mkdir -p /usr/local/flink-sql
cp -r plugins/ /usr/local/flink-sql/
cp -r lib/ /usr/local/flink-sql/
cp -r bin/ /usr/local/flink-sql/
chown -R admin:admin flink-sql/
```

## 5.config参数配置
```shell
#在config页面添加alphax配置参数
#打开 http://bigdata005:8181/config 页面，appname选择maxcompute，profile选择生产环境，添加以下内容
#以下为示例数据，请根据自身配置改动
dc.flinkStream.apiUrl=http://bigdata005:8484/
dc.flink.sql.path=/tmp/
dc.flink.localSqlPluginPath=/usr/local/flink-sql/plugins
dc.flink.flinkconf=/usr/local/flink-1.7.2/conf
dc.flink.yarnconf=/etc/hadoop/conf
dc.flink.remoteSqlPluginPath=/usr/local/flink-sql/plugins
dc.flink.flinkJarPath=/usr/local/flink-1.7.2/lib
dc.flink.mode=yarn
dc.flink.pluginLoadMode=shipfile
dc.flink.overview.url=http://bigdata005
```

## 6.启动服务
```shell
#使用admin帐号
cd /usr/local/flink-sql/bin
sh flink_sql_submit.sh
```

