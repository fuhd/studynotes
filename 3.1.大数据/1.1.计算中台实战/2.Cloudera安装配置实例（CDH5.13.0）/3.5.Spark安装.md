Spark安装
================================================================================
## 1.spark源码打包
```shell
#cd /e/alpha/gitlab/spark2.4.0   在自己本地项目路径
dev/make-distribution.sh --tgz  -Pyarn -Phive-thriftserver -Phive-1.1.0  -Dhadoop.version=2.6.0 -Dyarn.version=2.6.0 -DskipTests
```

## 2.安装spark-core jar到仓库
```shell
mvn install:install-file -DgroupId=org.apache.spark -DartifactId=spark-core_2.11 -Dversion=2.4.0 -Dpackaging=jar -Dfile=/home/fuhd/work/workspace/datacenter/spark2.4/core/target/spark-core_2.11-2.4.0.jar -Dmaven.test.skip  -Dcheckstyle.skip=true -DpomFile=/home/fuhd/work/workspace/datacenter/spark2.4/core/pom.xml  
```

## 3.服务部署
**放到bigdata008服务器上**：
```shell
#将本地打包好的spark-2.4.0-bin-2.6.0.tgz放到bigdata008服务器，进行解压安装
tar -zxvf spark-2.4.0-bin-2.6.0.tgz -C /usr/local/
cd /usr/local
chown -R admin:admin spark-2.4.0-bin-2.6.0
```

## 4.创建相关文件夹
**bigdata008服务器上**：
```shell
cd /home/admin/
mkdir jobinstancelogs
mkdir -p jobserver-config/spark-2.4
mkdir jobserver-jars
```
**在三个节点都要创建**：
```shell
cd /home/admin/
mkdir eventlogs
mkdir jars
##在CDH中最好给/home/admin目录755的权限（目录所有者当然是admin），因为yarn用户要读这个目录，要不然没有权限
chmod -R 777 /home/admin
#从maxcompute的biz子项目的pom.xml中找到aspectjweaver-1.8.10.jar放到/home/admin/jars/
```

## 5.相关jar上传到hdfs
```shell
#将spark相关jar放入hdfs
su - admin
hdfs dfs -mkdir -p /user/maxcompute/yarn_jars/spark_2.4.0/
hdfs dfs -put /usr/local/spark-2.4.0-bin-2.6.0/jars/* /user/maxcompute/yarn_jars/spark_2.4.0/
hdfs dfs -put /home/admin/jars/aspectjweaver-1.8.10.jar /user/maxcompute/yarn_jars/
```

## 6.软链接设置
```shell
su - admin
#软链接hadoop和hive的配置文件
ln -s /etc/hadoop/conf/hdfs-site.xml /usr/local/spark-2.4.0-bin-2.6.0/conf/hdfs-site.xml 
#hive-site.xml必须从apache-hive-1.2.1-bin/conf下拷贝过来，再修改
cp /etc/hive/conf/hive-site.xml /usr/local/spark-2.4.0-bin-2.6.0/conf/
```
**修改** 如下：
```xml
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <!-- 如果hive库不存在，则创建 -->
        <value>jdbc:mysql://bigdata005:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>MyPass99@</value>
    </property>
    <!-- 显示当前使用的数据库 -->
    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://bigdata005:9083</value>
    </property>
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>hdfs://ns/user/hive/warehouse/</value>
    </property>
    <property>
        <name>hive.exec.scratchdir</name>
        <value>/bigdata/hive/tmp</value>
    </property>
    <property>
        <name>hive.querylog.location</name>
        <value>/var/log/hive</value>
    </property>
</configuration>
```
注意：创建/bigdata/hive/tmp目录: 
```shell
mkdir -p /bigdata/hive/tmp
cd /bigdata/
chown -R admin:admin hive
chmod -R 777 hive
```

## 7.修改环境变量
```shell
su - admin
cd /usr/local/spark-2.4.0-bin-2.6.0/conf
cp spark-env.sh.template spark-env.sh
vi /spark-env.sh
```
添加：
```shell
export SPARK_LOCAL_IP=bigdata008
export SPARK_HOME=/usr/local/spark-2.4.0-bin-2.6.0
```

## 8.启动thriftserver
```shell
/usr/local/spark-2.4.0-bin-2.6.0/sbin/start-thriftserver.sh
```
> 启动后可能会有异常信息：No data or no sasl data in the stream
>
> No data or no sasl data in the stream是一个对HiveServer2服务没有影响的ERROR日志，该日志主要是HiveServer2服务上的负载均衡器进行Tcp检查引起的。

9.**复制安装好的Spark目录到bigdata005,bigdata006节点上（如果需要使用pyspark作业类型）**
使用scp命令即可完成，最后设置其所有者为：`admin:admin`。

