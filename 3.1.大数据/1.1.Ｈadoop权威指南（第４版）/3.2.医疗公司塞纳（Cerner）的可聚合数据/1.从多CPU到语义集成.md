从多CPU到语义集成
=============================================================================
一直以来，健康医疗信息技术所做的事情是 **将现有流程用自动化的方式来实现**。然而这一切正在发生改变。随着人们对 **提高治疗质量
和控制医疗成本** 的诉求日益增长，迫切需要有更好的系统来支撑这些目标。接下来，我们将看`Cerner`公司是如何用`Hadoop`生态系统
来理解健康医疗的概念并构建方案来解决这些问题的。

`Cerner`公司长期致力于将技术应用于健康医疗，并在很长的一段时间里将重点放在 **电子医疗记录** 上。然而，新的问题需要一种更宽泛的方法，
这驱使我们去研究`Hadoop`。

`2009`年时，我们需要为医疗记录建立 **更好的搜索索引**。由此而引出的处理需求无法通过其他架构简单解决。**搜索索引需要以高昂的代价
处理临床文档**：从文档中抽取术语，并解析其与其他术语的关系。例如，如果用户键入“心脏病”一词，我们希望返回的是探讨心肌梗塞的文档。
这一处理，代价相当高，比如大文档，它需要占用数秒钟的`CPU`时间，而我们想要将其应用于数以百万计的文档。简而言之，我们需要投入许多`CPU`
来处理这个问题，并且希望处理过程能够经济合算。

在其他可选方案中，我们 **曾经考虑过用一种基于分段式事件驱动架构**（`SEDA，staged event-driven architecture`）的方法来成规模
地抽取文档。但是 **`Hadoop`** 在满足一项重要需求方面脱颖而出：**我们需要在数小时或者更快的时间内频繁地反复处理数以百万计的文档**。
面向临床文档的知识抽取的逻辑在快速改进中，我们需要将这种改进迅速推向世界。**在`Hadoop`中，这仅仅意味着在已有的数据之上运行一个新版本
的`MapReduce`作业，然后处理文档被载入一个`Apache Solr`服务器集群以支持应用查询**。

这些早期的成功为后期更多涉足的项目打好了基础。这一系统类型及其数据可被用作经验基础，**以帮助控制成本以及改善全部人口的医疗水平**。由于健康
医疗数据经常 **以碎片化的形式** 分布在各系统和机构中，我们需要 **首先收集所有这些数据** 并理解其含义。

当有了大量的数据源和格式，甚至标准化的数据模型有待解析时，我们面临着一个 **庞大的语义集成** 问题。最大的挑战并非来自数据的规模————
众所周知`Hadoop`可以根据需要扩展————而是来自于为满足我们的需求，**对数据进行清理、管理和转换所带来的极端复杂性**。我们需要更高级的工具来
管理这一复杂性。









