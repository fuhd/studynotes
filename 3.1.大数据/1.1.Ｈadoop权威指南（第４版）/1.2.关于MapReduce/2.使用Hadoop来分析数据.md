使用Hadoop来分析数据
===============================================================================
**为了充分利用`Hadoop`提供的并行处理优势，我们需要将查询表示成`MapReduce`作业**。完成某种本地端的小规模测试之后，
就可以把作业部署到在集群上运行。

### map和reduce
**`MapReduce`任务过程分为两个处理阶段。`map`阶段和`reduce`阶段**。每阶段都以 **键/值对** 作为输入和输出，其类型
由程序员来选择。程序员还需要写两个函数：**`map`函数** 和 **`reduce`函数**。

**`map`阶段的输入是`NCDC`原始数据**。我们选择 **文本格式** 作为输入格式，将数据集的 **每一行** 作为文本输入。键是
某一行起始位置相对于文件起始位置的偏移量，不过我们不需要这个信息，所以将其忽略。

我们的`map`函数很简单。由于我们只对 **年份** 和 **气温** 属性感兴趣，所以只需要取出这两个字段数据。**在本例中，`map`函数
只是一个数据准备阶段**，通过这种方式来准备数据，使`reduce`函数能够继续对它进行处理：即找出每年的最高气温。**`map`函数还是一
个比较适合去除已损记录的地方**：此处，我们筛掉缺失的、可疑的或错误的气温数据。

为了全面了解`map`的工作方式，我们考虑以下输入数据的示例数据（考虑到篇幅，去除了一些未使用的例，并用省略号表示）：
```
0067011990999991950051507004...9999999N9+00001+99999999999...
0043011990999991950051512004...9999999N9+00221+99999999999...
0043011990999991950051518004...9999999N9-00111+99999999999...
......
```
这些行以键/值对的方式作为`map`函数的输入：
```
(0,0067011990999991950051507004...9999999N9+00001+99999999999...)
(106,0043011990999991950051512004...9999999N9+00221+99999999999...)
(212,0043011990999991950051518004...9999999N9-00111+99999999999...)
......
```
键（`key`）是文件中的行偏移量，`map`函数并不需要这个信息，所以将其忽略。`map`函数的功能仅限于提取年份和气温信息，
并将它们作为输出（气温值已用整数表示）：
```
(1950,0)
(1950,22)
(1950,-11)
......
```
**`map`函数的输出经由`MapReduce`框架处理后，最后发送到`reduce`函数。这个处理过程基于键来对键/值对进行排序和分组**。
因此，在这一示例中，`reduce`函数看到的是如下输入：
```
(1949,[111,78])
(1950,[0,22,-11])
```
每一年份后紧跟着一系列气温数据。`reduce`函数现在要做的是遍历整个列表并从中找出最大的读数：
```
(1949,111)
(1950,22)
```
这是最终输出结果，每一年的全球最高气温记录。

### Java MapReduce
下面写代码实现它，我们需要三样东西：一个`map`函数、一个`reduce`函数和一些用来运行作业的代码。**`map`函数由`Mapper`
类来表示，后者声明一个抽象的`map()`方法**。下面范例显示了我们的`map`函数实现：
```java
import java.io.IOException;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MaxTemperatureMapper extends MapReduceBase implements Mapper<LongWritable,Text,IntWritable> {

    private static final int MISSING = 9999;

    @Override
    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String year = line.substring(15, 19);
        int airTemperature;
        if(line.charAt(87) == '+') {
            airTemperature = Integer.parseInt(line.substring(88, 92));
        } else {
            airTemperature = Integer.parseInt(line.substring(87, 92));
        }
        String quality = line.substring(92, 93);
        if(airTemperature != MISSING && quality.matches("[01459]")) {
            context.write(new Text(year), new IntWritable(airTemperauture));
        }
    }
}
```
这个 **`Mapper`类是一个泛型类型**，它有 **四个形参类型**，分别指定`map`函数的 **输入键、输入值、输出键和输出值** 的类型。
就现在这个例子来说，输入键是一个长整数偏移量，输入值是一行文本，输出键是年份，输出值是气温（整数）。**`Hadoop`本身提供了一套可
优化网络序列化传输的基本类型**，而不直接使用`Java`内嵌的类型。**这些类型都在`org.apache.hadoop.io`包中**。这里使用`LongWritable`
类型（相当于`Java`的`Long`类型）、`Text`类型（相当于`Java`中的`String`类型）和`IntWritable`类型（相当于`Java`的`Integer`类型）。

`map()`方法的输入是一个键和一个值。我们首先将包含有一行输入的`Text`值转换成`Java`的`String`类型，之后用`substring()`
方法提取我们感兴趣的列。

**`map()`方法还提供`Context`实例用于输出内容的写入**。在这种情况下，我们将年份数据按`Text`对象进行读/写（因为我们把年份当作键），
将气温值封装在`IntWritable`类型中。只有气温数据不缺并且所对应质量代码显示为正确的气温读数时，这些数据才会被写入输出记录中。

以类似方法用`Reducer`来定义`reduce`函数，范例如下：
```java
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class MaxTemperatureReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    @Override
    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int maxValue = Integer.MIN_VALUE;
        for(IntWritable value: values) {
            MaxValue = Math.max(maxValue, value.get());
        }
        context.write(key, new IntWritable(maxValue));
    }
}
```
同样，**`reduce`函数也有四个形式参数类型用于指定输入和输出类型。`reduce`函数的输入类型必须匹配`map`函数的输出类型**：即`Text`类型和
`IntWritable`类型。在这种情况下，`reduce`函数的输出类型也必须是`Text`和`IntWritable`类型，分别输出年份及其最高气温。

第三部分代码负责运行`MapReduce`作业，范例如下：
```java
import java.io.IOException;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.input.FileOutputFormat;

public class MaxTemperature {
    public static void main(String[] args) throws Exception {
        if(args.length != 2) {
            System.err.println("Usage: MaxTemperature <input path> <output path>");
            System.exit(-1);
        }

        Job job = new Job();
        job.setJarByClass(MaxTemperature.class);
        job.setJobName("Max temperature");

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        
        job.setMapperClass(MaxTemperatureMapper.class);
        job.setReducerClass(MaxTemperatureReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```
**`Job`对象指定作业执行规范。我们可以用它来控制整个作业的运行**。我们在`Hadoop`集群上运行这个作业时，**要把代码打包成一个`JAR`文件
（`Hadoop`在集群上发布这个文件）。不必明确指定`JAR`文件的名称，在`Job`对象的`setJarByClass()`方法中传递一个类即可，`Hadoop`利用
这个类来查找包含它的`JAR`文件，进而找到相关的`JAR`文件**。

构造`Job`对象之后，需要指定输入和输出数据的路径。**调用`FileInputFormat`类的静态方法`addInputPath()`来定义输入数据的路径，
这个路径可以是单个的文件、一个目录（此时，将目录下所有文件当作输入）或符合特定文件模式的一系列文件**。由函数名可知，**可以多次调用
`addInputPath()`来实现多路径的输入**。

**调用`FileOutputFormat`类中的静态方法`setOutputPath()`来指定输出路径（只能有一个输出路径）。这个方法指定的是`reduce`函数输出
文件的写入目录，在运行作业前该目录是不应该存在的，否则`Hadoop`会报错并拒绝运行作业**。这种预防措施的目的是 **防止数据丢失**（长时间运行
的作业如果结果被意外覆盖，肯定是非常恼人的）。

接着，通过 **`setMapperClass()`** 和 **`setReducerClass()`** 方法指定要用的 **`map`类型** 和 **`reduce`类型**。

**`setOutputKeyClass()`和`setOutputValueClass()`方法控制`reduce`函数的输出类型，并且必须和`Reducer`类产生的相匹配。
`map`函数的输出类型默认情况下和`reduce`函数是相同的，因此如果`mapper`产生出和`reducer`相同的类型时，不需要单独设置。但是，
如果不同，则必须通过`setMapOutputKeyClass()`和`setMapOutputValueClass()`方法来设置`map`函数的输出类型**。

**输入的类型通过输入格式来控制，我们的例子中没有设置，因为使用的是默认的`TextInputFormat`（文本输入格式）**。

在设置定义`map`和`reduce`函数的类之后，可以开始运行作业。**`Job`中的`waitForCompletion()`方法提交作业并等待执行完成。
该方法唯一的参数是一个标识，指示是否已生成详细输出。当标识为`true`时，作业会把其进度信息写到控制台**。





















