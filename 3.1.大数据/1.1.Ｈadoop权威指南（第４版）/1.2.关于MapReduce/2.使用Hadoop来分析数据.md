使用Hadoop来分析数据
===============================================================================
**为了充分利用`Hadoop`提供的并行处理优势，我们需要将查询表示成`MapReduce`作业**。完成某种本地端的小规模测试之后，
就可以把作业部署到在集群上运行。

### map和reduce
**`MapReduce`任务过程分为两个处理阶段。`map`阶段和`reduce`阶段**。每阶段都以 **键/值对** 作为输入和输出，其类型
由程序员来选择。程序员还需要写两个函数：**`map`函数** 和 **`reduce`函数**。

**`map`阶段的输入是`NCDC`原始数据**。我们选择 **文本格式** 作为输入格式，将数据集的 **每一行** 作为文本输入。键是
某一行起始位置相对于文件起始位置的偏移量，不过我们不需要这个信息，所以将其忽略。

我们的`map`函数很简单。由于我们只对 **年份** 和 **气温** 属性感兴趣，所以只需要取出这两个字段数据。**在本例中，`map`函数
只是一个数据准备阶段**，通过这种方式来准备数据，使`reduce`函数能够继续对它进行处理：即找出每年的最高气温。**`map`函数还是一
个比较适合去除已损记录的地方**：此处，我们筛掉缺失的、可疑的或错误的气温数据。

为了全面了解`map`的工作方式，我们考虑以下输入数据的示例数据（考虑到篇幅，去除了一些未使用的例，并用省略号表示）：
```
0067011990999991950051507004...9999999N9+00001+99999999999...
0043011990999991950051512004...9999999N9+00221+99999999999...
0043011990999991950051518004...9999999N9-00111+99999999999...
......
```
这些行以键/值对的方式作为`map`函数的输入：
```
(0,0067011990999991950051507004...9999999N9+00001+99999999999...)
(106,0043011990999991950051512004...9999999N9+00221+99999999999...)
(212,0043011990999991950051518004...9999999N9-00111+99999999999...)
......
```
键（`key`）是文件中的行偏移量，`map`函数并不需要这个信息，所以将其忽略。`map`函数的功能仅限于提取年份和气温信息，
并将它们作为输出（气温值已用整数表示）：
```
(1950,0)
(1950,22)
(1950,-11)
......
```
**`map`函数的输出经由`MapReduce`框架处理后，最后发送到`reduce`函数。这个处理过程基于键来对键/值对进行排序和分组**。
因此，在这一示例中，`reduce`函数看到的是如下输入：
```
(1949,[111,78])
(1950,[0,22,-11])
```
每一年份后紧跟着一系列气温数据。`reduce`函数现在要做的是遍历整个列表并从中找出最大的读数：
```
(1949,111)
(1950,22)
```
这是最终输出结果，每一年的全球最高气温记录。

### Java MapReduce
下面写代码实现它，我们需要三样东西：一个`map`函数、一个`reduce`函数和一些用来运行作业的代码。**`map`函数由`Mapper`
类来表示，后者声明一个抽象的`map()`方法**。下面范例显示了我们的`map`函数实现：
```java
import java.io.IOException;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MaxTemperatureMapper extends MapReduceBase implements Mapper<LongWritable,Text,IntWritable> {

    private static final int MISSING = 9999;

    @Override
    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String year = line.substring(15, 19);
        int airTemperature;
        if(line.charAt(87) == '+') {
            airTemperature = Integer.parseInt(line.substring(88, 92));
        } else {
            airTemperature = Integer.parseInt(line.substring(87, 92));
        }
        String quality = line.substring(92, 93);
        if(airTemperature != MISSING && quality.matches("[01459]")) {
            context.write(new Text(year), new IntWritable(airTemperauture));
        }
    }
}
```
这个 **`Mapper`类是一个泛型类型**，它有 **四个形参类型**，分别指定`map`函数的 **输入键、输入值、输出键和输出值** 的类型。
就现在这个例子来说，输入键是一个长整数偏移量，输入值是一行文本，输出键是年份，输出值是气温（整数）。**`Hadoop`本身提供了一套可
优化网络序列化传输的基本类型**，而不直接使用`Java`内嵌的类型。**这些类型都在`org.apache.hadoop.io`包中**。这里使用`LongWritable`
类型（相当于`Java`的`Long`类型）、`Text`类型（相当于`Java`中的`String`类型）和`IntWritable`类型（相当于`Java`的`Integer`类型）。

`map()`方法的输入是一个键和一个值。我们首先将包含有一行输入的`Text`值转换成`Java`的`String`类型，之后用`substring()`
方法提取我们感兴趣的列。

**`map()`方法还提供`Context`实例用于输出内容的写入**。在这种情况下，我们将年份数据按`Text`对象进行读/写（因为我们把年份当作键），
将气温值封装在`IntWritable`类型中。只有气温数据不缺并且所对应质量代码显示为正确的气温读数时，这些数据才会被写入输出记录中。

以类似方法用`Reducer`来定义`reduce`函数，范例如下：
```java
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class MaxTemperatureReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    @Override
    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int maxValue = Integer.MIN_VALUE;
        for(IntWritable value: values) {
            MaxValue = Math.max(maxValue, value.get());
        }
        context.write(key, new IntWritable(maxValue));
    }
}
```
同样，**`reduce`函数也有四个形式参数类型用于指定输入和输出类型。`reduce`函数的输入类型必须匹配`map`函数的输出类型**：即`Text`类型和
`IntWritable`类型。在这种情况下，`reduce`函数的输出类型也必须是`Text`和`IntWritable`类型，分别输出年份及其最高气温。

第三部分代码负责运行`MapReduce`作业，范例如下：
```java

```








