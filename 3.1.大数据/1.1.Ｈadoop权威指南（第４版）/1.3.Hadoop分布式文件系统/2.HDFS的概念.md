HDFS的概念
========================================================================
### 数据块
每个磁盘都有默认的数据块大小，这是磁盘进行数据读/写的最小单位。构建于单个磁盘之上的文件系统通过
磁盘块来管理该文件系统中的块，该文件系统块的大小可以是磁盘块的整数倍。**文件系统块一般为几千字节，
而磁盘块一般为`512`字节**。

**`HDFS`同样也有块（`block`）的概念，但是大得多，默认为`128MB`**。与单一磁盘上的文件系统相似，
**`HDFS`上的文件也被划分为块大小的多个分块（`chunk`），作为独立的存储单元**。但与面向单一磁盘的文件
系统不同的是，**`HDFS`中小于一个块大小的文件不会占据整个块的空间**（例如，当一个`1MB`的文件存储在
一个`128MB`的块中时，文件只使用`1MB`的磁盘空间，而不是`128MB`）。
```
HDFS中的块为什么这么大？
HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的
时间。因而，传输一个由多个块组成的大文件的时间取决于磁盘传输速率。

我们来做一个速算，如果寻址时间约为10ms，传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为
100MB。默认的块大小实际为128MB，但是很多情况下HDFS安装时使用更大的块。以后随着新一代磁盘驱动器传输速率的提升，块的
大小会被设计得更大。

但是这个参数也不会设置得过大。MapReduce中的map任务通常一次只处理一个块中的数据，因此如果任务数太少（少于集群中的节
点数量），作业的运行速度就会比较慢。
```
对分布式文件系统中的块进行抽象会带来很多好处。第一个最明显的好处是，**一个文件的大小可以大于网络中
任意一个磁盘的容量**。文件的所有块并不需要存储在同一个磁盘上，因此它们可以利用集群上的任意一个磁盘
进行存储。

第二个好处是，**使用抽象块而不是整个文件作为存储单元，大大简化了存储子系统的设计**。简化是所有系
统的目标，但是这对于故障各类繁多的分布式系统来说尤为重要。将存储子系统的对象设置成块，可简化存储
管理（**由于块的大小是固定的，因此计算单个磁盘能存储多少个块就相对容易**）。同时也消除了对元数据
的顾虑（**块只是要存储的大块数据，而文件的元数据，如权限信息，并不需要与块一同存储，这样一来，其他
系统就可以单独管理这些元数据**）。

不仅如此，**块还非常适合用于数据备份** 进而提供数据容错能力和提高可用性。将每个块复制到少数几个物理
上相互独立的机器上（**默认为`3`个**），可以确保在块、磁盘或机器发生故障后数据不会丢失。如果发现
一个块不可用，系统会从其它地方读取另一个复本，而这个过程对用户是透明的。**一个因损坏或机器故障而丢失
的块可以从其他候选地点复制到另一台可以正常运行的机器上，以保证复本的数量回到正常水平**。

与磁盘文件系统相似，**`HDFS`中`fsck`指令可以显示块信息**。例如，执行以下命令将列出文件系统中各个文件
由哪些块构成。
```shell
$ hdfs fsck / -files -blocks
```

### namenode和datanode
`HDFS`集群有 **两类节点** 以管理节点/工作节点模型运行，**即一个`namenode`（管理节点）和多个`datanode`
（工作节点）。`namenode`管理文件系统的命名空间。它维护着文件系统树及整棵树内所有的文件和目录**。
这些信息以 **两个文件** 形式永久保存在 **本地磁盘** 上：**命名空间镜像文件和编辑日志文件**。
`namenode`也记录着每个文件中各个块所在的数据节点信息，但它并不永久保存块的位置信息，因为这些信息
会在系统启动时根据数据节点信息重建。

客户端（`client`）代表用户通过与`namenode`和`datanode`交互来访问整个文件系统。客户端提供一个
类似于`POSIX`（可移植操作系统界面）的文件系统接口，因此用户在编程时无需知道`namenode`和`datanode`
也可实现其功能。

**`datanode`是文件系统的工作节点。它们根据需要存储并检索数据块（受客户端或`namenode`调度），并且
定期向`namenode`发送它们所存储的块的列表**。

**没有`namenode`，文件系统将无法使用。事实上，如果运行`namenode`服务的机器毁坏，文件系统上所有的
文件将会丢失，因为我们不知道如果根据`datanode`的块重建文件**。因此，**对`namenode`实现容错非常重要，
`Hadoop`为此提供两种机制**。

**第一种机制是备份那些组成文件系统元数据持久状态的文件**。`Hadoop`可以通过配置使`namenode`在
**多个文件系统** 上保存元数据的持久状态。这些写操作是实时同步的，且是原子操作。一般的配置是，**将持
久状态写入本地磁盘的同时，写入一个远程挂载的网络文件系统（`NFS`）**。

另一种可行的方法是 **运行一个辅助`namenode`**，但它不能被用作`namenode`。这个辅助`namenode`的重要
作用是 **定期合并编程日志与命名空间镜像，以防止编辑日志过大。这个辅助`namenode`一般在另一台单独
的物理计算机上运行**，因为他需要占用大量`CPU`时间，并且需要与`namenode`一样多的内存来执行合并操作。
**它会保存合并后的命名空间镜像的副本，并在`namenode`发生故障时启用**。但是，**辅助`namenode`保存
的状态总是滞后于主节点，所以在主节点全部失效时，难免会丢失部分数据**。在这种情况下，一般把存储在`NFS`
上的`namenode`元数据复制到辅助`namenode`并作为新的主`namenode`运行（注意，**也可以运行热备份`namenode`
代替运行辅助`namenode**）。

### 块缓存
通常`datanode`从磁盘中读取块，**但对于访问频繁的文件，其对应的块可能被显式地缓存在`datanode`
的内存中，以堆外块缓存**（`off-heap block cache`）的形式存在。 **默认情况下，一个块仅缓存在一
个`datanode`的内存中**，当然可以针对每个文件配置`datanode`的数量。**作业调度器（用于`MapRduce`
、`Spark`和其他框架的）通过在缓存块的`datanode`上运行任务，可以利用块缓存的优势提高读操作的性能**。

用户或应用通过在 **缓存池（`cache pool`）** 中增加一个 **`cache directive`** 来告诉`namenode`
需要缓存哪些文件及存多久。**缓存池是一个用于管理缓存权限和资源使用的管理性分组**。

### 联邦HDFS
**`namenode`在内存中保存文件系统中每个文件和每个数据块的引用关系**，这意味着对于一个拥有大量文
件的超大集群来说，**内存将成为限制系统横向扩展的瓶颈**。在 **`2.x`** 发行版本系列中引入的
**联邦`HDFS`允许系统通过添加`namenode`实现扩展，其中每个`namenode`管理文件系统命名空间中的一部分**。

**在联邦环境下，每个`namenode`维护一个命名空间巻（`namenode volume`），由命名空间的元数据和一个
数据块池（`block pool`）组成，数据块池包含该命名空间下文件的所有数据块。命名空间卷之间是相互独立的，
两两之间并不相互通信，甚至其中一个`namenode`的失效也不会影响由其他`namenode`维护的命名空间的可用性。
数据块池不再进行切分，因此集群中的`datanode`需要注册到每个`namenode`，并且存储着来自多个数据块
池中的数据块**。

**要想访问联邦`HDFS`集群，客户端需要使用客户端挂载数据表将文件路径映射到`namenode`。该功能可以通过
`ViewFileSystem`和`viewfs://URI`进行配置和管理**。

### HDFS的高可用性
通过联合使用在多个文件系统中备份`namenode`的元数据和通过备用`namenode`创建监测点能防止数据丢失，
但是依旧无法实现文件系统的高可用性。`namenode`依旧存在 **单点失效**（`SOOF`,`single point of failure`）
问题。如果`namenode`失效了，那么所有的客户端，包括`MapRduce`作业，均无法读、写或列举（`list`）
文件，**因为`namenode`是唯一存储元数据与文件到数据块映射的地方**。在这一情况下，`Hadoop`系统无
法提供服务直到有新的`namenode`上线。

在这样的情况下，要想从一个失效的`namenode`恢复，系统管理员得启动一个拥有文件系统元数据副本的新的
`namenode`，并配置`datanode`和客户端以便使用这个新的`namenode`。**新的`namenode`直到满足以下
情形才能响应服务**：
1. **将命令空间的映像导入内存中**；
2. **重演编辑日志**；
3. **接收到足够多的来自`datanode`的数据块报告并退出安全模式。对于一个大型并拥有大量文件和数据块的集群，
`namenode`的冷启动需要`30`分钟，甚至更长时间**。

系统恢复时间太长，也会影响到日常维护。事实上，预期外的`namenode`失效出现概率很低，所以在现实中，
计划内的系统失败时间实际更为重要。

**`Hadoop2`针对上述问题增加了对`HDFS`高可用性（`HA`）的支持。在这一实现中，配置了一对活动-备用
（`active-standby`）`namenode`。当活动`namenode`失效，备用`namenode`就会接管它的任务并开
始服务于来自客户端的请求，不会有任何明显中断**。实现这一目标需要在架构上做如下修改：
+ **`namenode`之间需要通过高可用共享存储实现编辑日志的共享。当备用`namenode`接管工作之后，它将通
读共享编辑日志直到末尾，以实现与活动`namenode`的状态同步，并继续读取由活动`namenode`写入的新条目**。
+ **`datanode`需要同时向两个`namenode`发送数据块处理报告**，因为数据块的映射信息存储在`namenode`
的内存中，而非磁盘。
+ 客户端需要使用 **特定的机制** 来处理`namenode`的失效问题，这一机制对用户是透明的。
+ 辅助`namenode`的角色被备用`namenode`所包含，**备用`namenode`为活动的`namenode`命名空间设置
周期性检查点**。

可以从 **两种高可用性共享存储** 做出选择：**`NFS`过滤器** 或 **群体日志管理器（`QJM`，
`quorum journal manager`）。`QJM`是一个专用的`HDFS`实现，为提供一个高可用的编辑日志而设计，
被推荐用于大多数`HDFS`部署中**。`QJM`以一组日志节点（`journal node`）的形式运行，每一次编辑必须
写入多数日志节点。**典型的，有三个`journal`节点，所以系统能够忍受其中任何一个的丢失**。这种安排与
`ZooKeeper`的工作方式类似，当然必须认识到，**`QJM`的实现并没有使用`ZooKeeper`**。（然而，值得
注意的是，**`HDFS HA`在选取活动的`namenode`时确实使用了`ZooKeeper`技术**）。

在活动`namenode`失效之后，**备用`namenode`能够快速（几十秒时间）实现任务接管**，因为最新的状态存储
在内存中：包括最新的编辑日志条目和最新的数据块映射信息。**实际观察到的失效时间略长一点（需要1分钟左右），
这是因为系统需要保守确定活动`namenode`是否真的失效了**。

在活动`namenode`失效且备用`namenode`也失效的情况下，当然这类情况发生的概率非常低，管理员依旧可以
声明一个备用`namenode`并实现冷启动。

#### 故障切换与规避
系统中有一个称为 **故障转移控制器**（`failover controller`）的新实体，**管理者将活动`namenode`
转移为备用`namenode`的转换过程**。有多种故障转移控制器，但默认的一种是 **使用了`ZooKeeper`来确保
有且仅有一个活动`namenode`**。每一个`namenode`运行着一个轻量级的故障转移控制器，其工作就是监视
宿主`namenode`是否失效（通过一个简单的心跳机制实现）并在`namenode`失效时进行故障切换。

管理员也可以手动发起故障转移，例如在进行日常维护时。这称为“平稳的故障转换”（`graceful failover`），
因为故障转移控制器可以组织两个`namenode`有序地切换角色。

但在非平稳故障转移的情况下，无法确切知道失效`namenode`是否已经停止运行。例如，在网速非常慢或者网络
被分割的情况下，同样也可能激发故障转移，但是先前的话动`namenode`依然运行着并且依旧是活动`namenode`。
高可用实现做了更进一步的优化，以确保先前活动的`namenode`不会执行危害系统并导致系统崩溃的操作，
该方法称为“规避”（`fencing`）。



















































w
