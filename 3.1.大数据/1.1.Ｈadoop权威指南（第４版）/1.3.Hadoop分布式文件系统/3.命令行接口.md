命令行接口
==========================================================================
现在我们通过命令行交互来进一步认识`HDFS`。`HDFS`还有很多其他接口，但命令行是最简单的，同时也是许多
开发者最熟悉的。

在我们设置为 **分布配置** 时，有两个属性项需要进一步解释。**第一项是`fs.defaultFS`**，设置为
`hdfs://localhost/`，**用于设置`Hadoop`的默认文件系统**。文件系统是由`URI`指定的，这里我们
已使用`hdfs URI`来配置`HDFS`为`Hadoop`的默认文件系统。**`HDFS`的守护程序通过该属性来确定
`HDFS namenode`的主机及端口**。我们将在`localhost`默认的`HDFS`端口 **`8020`** 上运行
`namenode`。这样一来，`HDFS`客户端可以通过该属性得知`namenode`在哪里运行进而连接到它。

**第二个属性`dfs.replication`，我们设为`1`，这样一来，`HDFS`就不会按默认设置将文件系统块复本
设置为`3`。在单独一个`datanode`上运行时，`HDFS`无法将块复制到`3`个`datanode`上，所以会持续
给出块复本不足的警告**。设置这个属性之后，上述问题就不会在出现了。

### 文件系统的基本操作
至此，文件系统已经可以使用了，我们 **可以执行所有常用的文件系统操作**，例如，读取文件，新建目录，
移动文件，删除数据，列出目录，等等。可以输入命令：
```shell
$ hadoop fs -help
```
**获取每个命令的详细帮助文件**。

首先 **从本地文件系统将一个文件复制到`HDFS`**：
```shell
$ hadoop fs -copyFromLocal ~/input/docs/quangle.txt hdfs://localhost/quangle.txt
```
该命令调用`hadoop`文件系统的`shell`命令 **`fs`**，后者提供了一系列子命令，在这个例子中，我们
执行的是 **`-copyFromLocal`**。本地文件`quangle.txt`被复制到运行在`localhost`上的`HDFS`
实例中，路径为：`hdfs://localhost/quangle.txt`。事实上，**我们可以简化命令格式以省略主机的`URL`
并使用默认设置，即省略`hdfs://localhost`，因为该项已在`core-site.xml`中指定**。如下命令：
```shell
$ hadoop fs -copyFromLocal ~/input/docs/quangle.txt /quangle.txt
```
**我们也可以使用相对路径，并将文件复制到`HDFS`的`home`目录中（注意：没有用/作为开始）**，本例中
为`/user/fuhd`：
```shell
$ hadoop fs -copyFromLocal ~/input/docs/quangle2.txt quangle2.txt
# 上例与下面命令相等
# hadoop fs -copyFromLocal ~/input/docs/quangle2.txt /user/fuhd/quangle2.txt
```
我们 **把文件复制回本地文件系统，并检查是否一致**：
```shell
$ hadoop fs -copyToLocal /quangle.txt ~/quangle.copy.txt
$ md5sum ~/input/docs/quangle.txt ~/quangle.copy.txt

b537dc015ca9316e9a052ce01edeef96  /home/fuhd/input/docs/quangle.txt
b537dc015ca9316e9a052ce01edeef96  /home/fuhd/quangle.copy.txt
```


































































































ss
