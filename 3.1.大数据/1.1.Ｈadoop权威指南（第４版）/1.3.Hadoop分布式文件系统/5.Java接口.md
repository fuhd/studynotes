Java接口
===============================================================================
我们要深入探索`Hadoop`的 **`Filesystem`类：它是与`Hadoop`的某一文件系统进行交互的`API`。
虽然我们主要聚焦于`HDFS`实例，即`DistributedFileSystem`**，但总体来说，还是应该集成`Filesystem`
抽象类，并编写代码，使其在不同文件系统中可移植。

### 从Hadoop URL读取数据
**要从`Hadoop`文件系统读取文件，最简单的方法是使用`java.net.URL`对象打开数据流，从中读取数据**。
具体格式如下：
```java
package com.hadoop.hdfs;

import org.apache.hadoop.fs.FsUrlStreamHandlerFactory;
import org.apache.hadoop.io.IOUtils;
import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.net.URL;

public class TestHadoopURL {
    static {
        //注释A
        URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
    }
    public static void main(String[] args) {
        InputStream in = null;
        try {
            in = new URL("hdfs://localhost/quangle.txt").openStream();
            BufferedReader reader = new BufferedReader(new InputStreamReader(in));
            reader.lines().forEach(data -> System.out.printf("data: %s\n", data));
        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            IOUtils.closeStream(in);
        }
    }
}
```
**让`Java`程序能够识别`Hadoop`的`hdfs URL`方案需要一些额外的工作。这里采用的方法是通过
`FsUrlStreamHandlerFactory`实例调用`java.net.URL`对象的`setURLStreamHandlerFactory()`
方法**，见上面的代码注释A。**每个`Java`虚拟机只能调用一次这个方法，因此通常在静态方法中调用。这个
限制意味着如果程序的其他组件（如不受你控制的第三方组件）已经声明一个`FsUrlStreamHandlerFactory`
实例，你将无法使用这种方法从`Hadoop`中读取数据**。

下面的范例展示的程序以标准输出方法显示`Hadoop`文件系统中的文件，类似于`Unix`中的`cat`命令。
```java
package com.hadoop.hdfs;

import org.apache.hadoop.fs.FsUrlStreamHandlerFactory;
import org.apache.hadoop.io.IOUtils;
import java.io.IOException;
import java.io.InputStream;
import java.net.URL;

public class TestHadoopURL2 {
    static {
        URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
    }
    public static void main(String[] args) throws IOException {
        InputStream in = null;
        try {
            in = new URL("hdfs://localhost/quangle2.txt").openStream();
            IOUtils.copyBytes(in, System.out, 4096, false);
        } finally {
            IOUtils.closeStream(in);
        }
    }
}
```
**我们可以调用`Hadoop`中简洁的`IOUtils`类，并在`finally`子句中关闭数据流，同时也可以在输入流和
输出流之间复制数据（本例中为`System.out`）。`copyBytes`方法的最后两个参数，第一个设置用于复制
的缓冲区大小，第二个设置复制结束后是否关闭数据流。这里我们选择自行关闭输入流（自己编码来控制）**。

### 通过FileSystem API读取数据
**有时根本不可能在应用中设置`FsUrlStreamHandlerFactory`实例。在这种情况下，我们需要用
`Filesystem API`来打开一个文件的输入流**。

`Hadoop`文件系统中通过 **`Hadoop Path`对象**（而非`java.io.File`对象，因为它的语义与本地
文件系统联系太紧密）**来代表文件**。可以将路径视为一个`Hadoop`文件系统`URL`，
如`hdfs://localhost/user/fuhd/quangle.txt`。









































aaaa
