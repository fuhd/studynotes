Java接口
===============================================================================
我们要深入探索`Hadoop`的 **`Filesystem`类：它是与`Hadoop`的某一文件系统进行交互的`API`。
虽然我们主要聚焦于`HDFS`实例，即`DistributedFileSystem`**，但总体来说，还是应该集成`Filesystem`
抽象类，并编写代码，使其在不同文件系统中可移植。

### 从Hadoop URL读取数据
**要从`Hadoop`文件系统读取文件，最简单的方法是使用`java.net.URL`对象打开数据流，从中读取数据**。
具体格式如下：
```java
package com.hadoop.hdfs;

import org.apache.hadoop.fs.FsUrlStreamHandlerFactory;
import org.apache.hadoop.io.IOUtils;
import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.net.URL;

public class TestHadoopURL {
    static {
        //注释A
        URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
    }
    public static void main(String[] args) {
        InputStream in = null;
        try {
            in = new URL("hdfs://localhost/quangle.txt").openStream();
            BufferedReader reader = new BufferedReader(new InputStreamReader(in));
            reader.lines().forEach(data -> System.out.printf("data: %s\n", data));
        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            IOUtils.closeStream(in);
        }
    }
}
```
**让`Java`程序能够识别`Hadoop`的`hdfs URL`方案需要一些额外的工作。这里采用的方法是通过
`FsUrlStreamHandlerFactory`实例调用`java.net.URL`对象的`setURLStreamHandlerFactory()`
方法**，见上面的代码注释A。**每个`Java`虚拟机只能调用一次这个方法，因此通常在静态方法中调用。这个
限制意味着如果程序的其他组件（如不受你控制的第三方组件）已经声明一个`FsUrlStreamHandlerFactory`
实例，你将无法使用这种方法从`Hadoop`中读取数据**。

下面的范例展示的程序以标准输出方法显示`Hadoop`文件系统中的文件，类似于`Unix`中的`cat`命令。
```java
package com.hadoop.hdfs;

import org.apache.hadoop.fs.FsUrlStreamHandlerFactory;
import org.apache.hadoop.io.IOUtils;
import java.io.IOException;
import java.io.InputStream;
import java.net.URL;

public class TestHadoopURL2 {
    static {
        URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
    }
    public static void main(String[] args) throws IOException {
        InputStream in = null;
        try {
            in = new URL("hdfs://localhost/quangle2.txt").openStream();
            IOUtils.copyBytes(in, System.out, 4096, false);
        } finally {
            IOUtils.closeStream(in);
        }
    }
}
```
**我们可以调用`Hadoop`中简洁的`IOUtils`类，并在`finally`子句中关闭数据流，同时也可以在输入流和
输出流之间复制数据（本例中为`System.out`）。`copyBytes`方法的最后两个参数，第一个设置用于复制
的缓冲区大小，第二个设置复制结束后是否关闭数据流。这里我们选择自行关闭输入流（自己编码来控制）**。

### 通过FileSystem API读取数据
**有时根本不可能在应用中设置`FsUrlStreamHandlerFactory`实例。在这种情况下，我们需要用
`Filesystem API`来打开一个文件的输入流**。

`Hadoop`文件系统中通过 **`Hadoop Path`对象**（而非`java.io.File`对象，因为它的语义与本地
文件系统联系太紧密）**来代表文件**。可以将路径视为一个`Hadoop`文件系统`URL`，
如`hdfs://localhost/user/fuhd/quangle.txt`。

`FileSystem`是一个通用的文件系统`API`，所以第一步是 **检索我们需要使用的文件系统实例，这里是
`HDFS`。获取`FileSystem`实例有下面这几个静态工厂方法**：
```java
public static FileSystem get(Configuration conf) throws IOException
public static FileSystem get(URI uri, Configuration conf) throws IOException
public static FileSystem get(URI uri, Configuration conf, String user) throws IOException
```
**`Configuration`对象** 封装了客户端或服务器的配置，通过设置 **配置文件读取类路径** 来实现（如：
`cet/hadoop/core-site.xml`）。**第一个方法返回的是默认文件系统（在`core-site.xml`中指定的，
如果没有指定，则使用默认的本地文件系统）。第二个方法通过给定的`URI`方案和权限来确定要使用的文件系统，
如果给定`URI`中没有指定方案，则返回默认文件系统。第三，作为给定用户来访问文件系统，对安全来说是至
关重要**。

在某些情况下，你可能希望 **获取本地文件系统的运行实例** ，此时你可以使用 **`getLocal()`** 方法
很方便地获取。
```java
public static LocalFileSystem getLocal(Configuration conf) throws IOException
```
有了`FileSystem`实例之后，我们 **调用`open()`函数来获取文件的输入流**：
```java
public FSDataInputStream open(Path f) throws IOException
public abstract FSDataInputStream open(Path f, int bufferSize) throws IOException
```
**第一个方法使用默认的缓冲区大小`4KB`**。

最后，我们重写上面的示例，得到如下示例。
```java
package com.hadoop.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

import java.io.IOException;
import java.io.InputStream;
import java.net.URI;

public class FileSystemCat {
    public static void main(String[] args) throws IOException {
        String uri = "hdfs://localhost/quangle2.txt";
        //获取FileSystem的实例
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(uri), conf);
        InputStream in = null;
        try {
            //获取文件输入流
            in = fs.open(new Path(uri));
            IOUtils.copyBytes(in, System.out, 4096, false);
        } finally {
            IOUtils.closeStream(in);
        }
    }
}
```

#### FSDataInputStream对象
实际上，**`FileSystem`对象中的`open`方法返回的是`FSDataInputStream`对象**，而不是标准的
`java.io`类对象。**这个类是继承了`java.io.DataInputStream`的一个特殊类，并支持随机访问，
由此可以从流的任意位置读取数据**。
```java
package org.apache.hadoop.fs;

public class FSDataInputStream extends DataInputStream implements Seekable, PositionedReadable {
    //implementation elided
}
```
**`Seekable`接口支持在文件中找到指定位置，并提供一个查询当前位置相对于文件起始位置偏移量（`getPos()`）
的查询方法**：
```java
public interface Seekable {
    void seek(long pos) throws IOException;
    long getPos() throws IOException;
}
```
调用`seek()`来定位大于文件长度的位置会引发`IOException`异常。与`java.io.InputStream`的`skip()`
不同，**`seek()`可以移到文件中任意一个绝对位置，`skip()`则只能相对于当前位置定位到另一个新位置**。

下面的示例是对上面示例的再扩展，**它将一个文件写入标准输出两次：在一次写完之后，定位到文件的起始位置
再次以流方式读取该文件并输出**。
```java
package com.hadoop.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

import java.io.IOException;
import java.net.URI;

public class FileSystemDoubleCat {
    public static void main(String[] args) throws IOException {
        String uri = "hdfs://localhost/quangle2.txt";
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(uri), conf);
        FSDataInputStream in = null;
        try {
            in = fs.open(new Path(uri));
            IOUtils.copyBytes(in, System.out, 4096, false);
            //回到文件开始处
            in.seek(0);
            IOUtils.copyBytes(in, System.out, 4096, false);
        } finally {
            IOUtils.closeStream(in);
        }
    }
}
```
**`FSDataInputStream`类也实现了`PositionedReadable`接口，从一个指定偏移量处读取文件的一部分**：
```java
public interface PositionedReadable {
    public int read(long position, byte[] buffer, int offset, int length) throws IOException;
    public void readFully(long position, byte[] buffer, int offset, int length) throws IOException;
    public void readFully(long position, byte[] buffer) throws IOException;
}
```
**`read()`方法从文件的指定`position`处读取至多为`length`字节的数据并存入缓冲区`buffer`的指定偏移
量`offset`处。返回值是实际读到的字节数：调用者需要检查这个值，它有可能小于指定的`length`长度**。

`readFully()`方法将指定`length`长度的字节数数据读取到`buffer`中，除非已经读到文件末尾，这种情况
下将抛出`EOFException`异常。
```
read()方法与readFully()方法的区别？

read()方法实质是读取流上的字节直到流上没有字节为止，如果当声明的字节数组长度大于流上的数据长度时就提前返回，而readFully()
方法是读取流上指定长度的字节数组，也就是说如果声明了长度为len的字节数组，readFully()方法只有读取len长度个字节的时候才返
回，否则阻塞等待，如果超时，则会抛出异常 EOFException

那么当发送了长度为len的字节，那么为什么用read方法用户收不全呢，揪其原因发现消息在网络中传输是没那么理想的，我们发的那部分
字节数组在传送过程中可能在接受信息方的缓存当中或者在传输线路，极端情况下可能在发送方的缓存当中，这样就不在流上，所以read方
法提前返回了，这样就造成了各种错误
```
所有这些方法会保留文件当前偏移量，并且是线程安全的（**`FSDataInputStream`并不是为并发访问设计的，
因此最好为此新建多个实例**），因此它们提供了在读取文件的主体时，访问文件其他部分（可能是元数据）的便利
方法。

最后务必牢记，**`seek()`方法是一个相对高开销的操作，需要慎重使用**。建议用流数据来构建应用的访问模式
（比如使用`MapReduce`），而非执行大量`seek()`方法。

### 写入数据
**`FileSystem`类有一系列新建文件的方法。最简单的方法是给准备建的文件指定一个`Path`对象，然后返回
一个用于写入数据的输出流**：
```java
public FSDataInputStream create(Path f) throws IOException
```
此方法有 **多个重载版本**，允许我们指定 **是否需要强制覆盖现有文件、文件备份数量、写入文件时所用
缓冲区大小、文件块大小以及文件权限**。
```
说明：
create()方法能够为需要写入且当前不存在的文件创建父目录。尽管这样很方便，但有时并不希望这样。如果希望父目录不存在就导致文件写入失败，
则应该先调用exists()方法检查父目录是否存在。另一种方案是使用FileContext，允许你可以控制是否创建父目录。
```














































aaaa
