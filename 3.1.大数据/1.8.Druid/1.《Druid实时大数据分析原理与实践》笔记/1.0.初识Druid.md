初识Druid
================================================================================
## 1.Druid是什么
Druid单词来源于西方古罗马的神话人物，中文常常翻译成 **德鲁伊**。

本书介绍的Druid是一个 **分布式的支持实时分析的数据存储系统**。美国广告技术公司MetaMarkets于
2011年创建了Druid项目，并且于2012年晚期开源了Druid项目。Druid设计之初的想法就是为分析而生，
**它在处理数据的规模、数据处理的实时性方面，比传统化的OLAP系统有了显著的性能改进，而且拥抱主流的
开源生态，包括Hadoop等**。

## 2.大数据分析和Druid
Hadoop设计之初就是为了批量处理大数据，但数据处理实时性经常是它的弱点，无法满足很多数据分析师所期
望的秒级返回查询结果的分析需求。

**为了解决数据实时性的问题**，大部分公司都有一个经历，将数据分析变成更加实时的可交互方案。其中，
涉及新软件的引入、数据流的改进等。整个数据分析的基础架构通常分为以下几类：
1. 使用Hadoop/Spark的MR分析。
2. 将Hadoop/Spark的结果注入RDBMS中提供实时分析。
3. 将结果注入到容量更大的NoSQL中，例如HBase等。
4. 将数据源进行流式处理，对接流式计算框架，如Storm，结果落在RDBMS/NoSQL中。
5. **将数据源进行流式处理，对接分析数据库，例如Druid、Vertica等**。

## 3.Druid的产生

### 3.1.MetaMarkets简介
Druid诞生于MetaMarkets公司，简单介绍这家公司有助于更好地理解Druid创建的背景。**这家公司致力
于为在线媒体公司提供数据分析服务**，客户包括在线广告公司、在线游戏开发商和社交媒体等。根据
MetaMarkets数据，它们的数据分析平台的请求事件的峰值超过3百万/秒。另外，广告分析对于实时查询的
性能要求非常高，Druid就是MetaMarkets的核心数据处理平台，能够保证99%的数据查询在1秒内返回结果。
在这个业务背景下，MetaMarkets的工程师在早期的数据分析平台的设计上，经过两个阶段的努力后，最终决
定自主开发Druid系统来满足业务需求。

#### 3.1.1.第一阶段，基于RDBMS的查询分析 
MetaMarkets最开始使用了GreenPlum社区版数据库，分析系统运行在亚马逊的云服务器上。采有这种方式
后发现以下两个明显问题。
+ **很多全表扫描操作响应特别慢。例如对于一个3300万行的数据，计算总行数需要3秒时间**。
+ 所有列的处理方式相同。这意味着时间、维度、指标不做任何区分。因此在使用这些数据的时候，需要管理
哪些是维度列，哪些是指标列。

对于第二个问题，可能只是增加了少量的系统管理成本，**但是第一个问题对于分析平台确实是致命的**。

#### 3.1.2.第二阶段，预计算结果放入NoSQL中
经过第一阶段的尝试，开发人员得出一个结论：**常规的关系型数据库管理系统（`RDBMS`）并不能满足实时
大规模数据分析的性能要求**。因此，他们在第二阶段的工作中，将 **NoSQL**作为数据落地时的存储，利
用NoSQL的高性能、可扩展特性解决数据分析的实时问题。**这种方法在维度比较少的时候，计算量不大，将
数据放在NoSQL中，访问速度很快。但是，随着维度的增加，预计算的计算量越来越大，当维度达到11个时，
计算时间甚至超过了24小时。在后面的性能优化过程中，开发人员不得不限制预计算的维度，例如规定不能超
过5个维度，只预先计算最重要的维度。但是随着业务的发展，维度还在不断继续增加，计算量再次成为瓶颈**。

### 3.2.失败总结
在经历两次深刻的失败后，MetaMarkets决定创建一个分布式的内存OLAP系统，用于解决以下两个核心问题。
+ RDBMS的查询太慢。
+ 支持灵活的查询分析能力。

## 4.Druid的三个设计原则
在设计之初，开发人员确定了三个设计原则。
1. **快速查询：部分数据的聚合 + 内存化 ＋索引**。
2. **水平扩展能力：分布式数据 + 并行化查询**。
3. **实时分析： 不可变的过去，只追加的未来**。

### 4.1.快速查询
对于数据分析场景，**大部分情况下，我们只关心一定粒度聚合的数据，而非每一行原始数据的细节情况**。
因此，数据聚合粒度可以是1分钟、5分钟、1小时或1天等。部分数据聚合给Druid争取了很大的性能优化空间。
**数据内存化也是提高查询速度的杀手锏**。内存和硬盘的访问速度相差近百倍，但内存的大小是非常有限的，
因此在内存使用方面要精细设计，**比如Druid里面使用了Bitmap和各种压缩技术**。另外，为了支持
Drill-Down某些维度，**Druid维护了一些倒排索引**。这种方式可以加快AND和OR等计算操作。

### 4.2.水平扩展能力
Druid查询性能在很大程度上依赖于内存的优化使用。**数据可以分布在多个节点的内存中，因此当数据增长
的时候，可以通过简单增加机器的方式进行扩容。为了保持平衡，Druid按照时间范围把聚合数据进行分区处
理。对于高基数的维度，只按照时间切分有时候是不够的（Druid的每个Segment不超过2000万行），故
Druid还支持对Segment进一步分区**。

**历史Segment数据可以保存在深度存储系统中，存储系统可以是本地磁盘、HDFS或远程的云服务**。如果
某些节点同现故障，则可借助Zookeeper协调其他节点重新构造数据。

Druid的查询模块能够感知和处理集群的状态变化，查询总是在有效的集群架构中进行。集群上的查询可以进
行灵活的水平扩展。Druid内置提供了一些容易并行化的聚合操作，例如`Count`、`Mean`、`Variance`
和其他查询统计。对于一些无法并行化的操作，例如：`Median`、`Druid`暂时不提供支持。在支持直方图
（`Histogram`）方面，Druid也是通过一些近似计算的方法进行支持，以保证Druid整体的查询性能，这些
近似计算方法还包括`HyperLoglog`、`DataSketches`的一些基数计算。

### 4.3.实时分析
Druid提供了包含基于时间维度数据的存储服务，并且任何一行数据都是历史真实发生的事件，**因此在设计
之初就约定事件一但进入系统，就不能再改变**。

**对于历史数据druid以Segment数据文件的方式组织，并且将它们存储到深度存储系统中，例如文件系统或
亚马逊的S3等。当需要查询这些数据的时候，Druid再从深度存储系统中将它们装载到内存供查询使用**。

## 5.Druid的技术特点
+ 数据吞吐量大。
+ 支持流式数据摄入和实时。
+ 查询灵活且快。
+ 社区支持力度大。

## 6.Druid的Hello,World

### 6.1.Druid的部署环境
Druid系统用Java编写，**建议直接使用Java8版本安装Druid**。在操作系统方面，支持主流Linux和
Mac OS。内存配置越大越好，建议8GB以上。如果只用于测试功能，4GB内存也可以运行。

### 6.2.Druid的基本概念

#### 6.2.1.数据格式
Druid在数据摄入之前，首先需要定义一个数据源（`DataSource`），**这个DataSource有些类似数据库
中表的概念**。每个数据集合包含三个部分：**时间列（`TimeStamp`）、维度列（`Dimemsion`）和指标
列（`Metric`）**。

**时间列**: 每个数据集合都必须有时间列，**这个列是数据聚合的重要维度**，Druid会将时间很近的一
些数据行聚合在一起。

**维度列**：维度列的字段为 **字符串类型**。

**指标列**：指标列字段通常为 **数值类型**，计算操作通常包括：`Count`、`Sum`和`Meam`等。

数据格式样例：

| 时间（时间列）|广告主（维度列）|广告位置（维度列）|是否点击（维度列）|展现次数（维度列）|展现价格（指标列）|
|:----- |:----- |:----- |:----- |:----- |:----- |
| 2011-01-01T01:01:352 | A | 位置1 | Yes | 20 | 20 |


#### 6.2.2.数据摄入
Druid提供两种数据摄入方式，**一种是实时数据摄入；另一种是批处理数据摄入**。
+ 实时摄入：**Kafka**
+ 批处理摄入：**HDFS、CSV、Etc**

#### 6.2.3.数据查询
在数据查询方面，**Druid原生查询是采用JSON格式**，通过HTTP传送。**Druid不支持标准的SQL语言查
询，因为有些SQL语言查询并不适用于Druid现在的设计**。随着Druid应用越来越广泛，支持标准SQL的需求
变得越来越重要，一些开源生态项目正在向这个方面努力，例如`Imply.io`的 **PlyQL** 等。

对于Druid的查询访问，除了原生Java客户端支持外，也出现了很多支持不同语言客户端访问的开源项目，例
如Python、R、JavaScript和Ruby等。

下面是一个用JSON表达的查询例子，该查询中指定了 **时间范围、聚合粒度、数据源等**。
```json
{
    "queryType": "timeseries",
    "dataSource": "sample_datasource",
    "granularity": "day",
    "aggregations": [
        {
            "type": "longSum",
            "name": "result_name",
            "fieldName": "field_name"
        }
    ],
    "intervals": ["2012-01-01T00:00:00.000/2012-01-04T00:00:00.000"],
    "context": {"skipEmptyBuckets": "true"}
}
```








