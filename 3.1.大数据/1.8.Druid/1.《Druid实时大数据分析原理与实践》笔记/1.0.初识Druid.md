初识Druid
================================================================================
## 1.Druid是什么
Druid单词来源于西方古罗马的神话人物，中文常常翻译成 **德鲁伊**。

本书介绍的Druid是一个 **分布式的支持实时分析的数据存储系统**。美国广告技术公司MetaMarkets于
2011年创建了Druid项目，并且于2012年晚期开源了Druid项目。Druid设计之初的想法就是为分析而生，
**它在处理数据的规模、数据处理的实时性方面，比传统化的OLAP系统有了显著的性能改进，而且拥抱主流的
开源生态，包括Hadoop等**。

## 2.大数据分析和Druid
Hadoop设计之初就是为了批量处理大数据，但数据处理实时性经常是它的弱点，无法满足很多数据分析师所期
望的秒级返回查询结果的分析需求。

**为了解决数据实时性的问题**，大部分公司都有一个经历，将数据分析变成更加实时的可交互方案。其中，
涉及新软件的引入、数据流的改进等。整个数据分析的基础架构通常分为以下几类：
1. 使用Hadoop/Spark的MR分析。
2. 将Hadoop/Spark的结果注入RDBMS中提供实时分析。
3. 将结果注入到容量更大的NoSQL中，例如HBase等。
4. 将数据源进行流式处理，对接流式计算框架，如Storm，结果落在RDBMS/NoSQL中。
5. **将数据源进行流式处理，对接分析数据库，例如Druid、Vertica等**。

## 3.Druid的产生

### 3.1.MetaMarkets简介
Druid诞生于MetaMarkets公司，简单介绍这家公司有助于更好地理解Druid创建的背景。**这家公司致力
于为在线媒体公司提供数据分析服务**，客户包括在线广告公司、在线游戏开发商和社交媒体等。根据
MetaMarkets数据，它们的数据分析平台的请求事件的峰值超过3百万/秒。另外，广告分析对于实时查询的
性能要求非常高，Druid就是MetaMarkets的核心数据处理平台，能够保证99%的数据查询在1秒内返回结果。
在这个业务背景下，MetaMarkets的工程师在早期的数据分析平台的设计上，经过两个阶段的努力后，最终决
定自主开发Druid系统来满足业务需求。

#### 3.1.1.第一阶段，基于RDBMS的查询分析 
MetaMarkets最开始使用了GreenPlum社区版数据库，分析系统运行在亚马逊的云服务器上。采有这种方式
后发现以下两个明显问题。
+ **很多全表扫描操作响应特别慢。例如对于一个3300万行的数据，计算总行数需要3秒时间**。
+ 所有列的处理方式相同。这意味着时间、维度、指标不做任何区分。因此在使用这些数据的时候，需要管理
哪些是维度列，哪些是指标列。

对于第二个问题，可能只是增加了少量的系统管理成本，**但是第一个问题对于分析平台确实是致命的**。

#### 3.1.2.第二阶段，预计算结果放入NoSQL中
经过第一阶段的尝试，开发人员得出一个结论：**常规的关系型数据库管理系统（`RDBMS`）并不能满足实时
大规模数据分析的性能要求**。因此，他们在第二阶段的工作中，将 **NoSQL**作为数据落地时的存储，利
用NoSQL的高性能、可扩展特性解决数据分析的实时问题。**这种方法在维度比较少的时候，计算量不大，将
数据放在NoSQL中，访问速度很快。但是，随着维度的增加，预计算的计算量越来越大，当维度达到11个时，
计算时间甚至超过了24小时。在后面的性能优化过程中，开发人员不得不限制预计算的维度，例如规定不能超
过5个维度，只预先计算最重要的维度。但是随着业务的发展，维度还在不断继续增加，计算量再次成为瓶颈**。

### 3.2.失败总结
在经历两次深刻的失败后，MetaMarkets决定创建一个分布式的内存OLAP系统，用于解决以下两个核心问题。
+ RDBMS的查询太慢。
+ 支持灵活的查询分析能力。

## 4.Druid的三个设计原则
在设计之初，开发人员确定了三个设计原则。
1. **快速查询：部分数据的聚合 + 内存化 ＋索引**。
2. **水平扩展能力：分布式数据 + 并行化查询**。
3. **实时分析： 不可变的过去，只追加的未来**。

### 4.1.快速查询
对于数据分析场景，**大部分情况下，我们只关心一定粒度聚合的数据，而非每一行原始数据的细节情况**。
因此，数据聚合粒度可以是1分钟、5分钟、1小时或1天等。部分数据聚合给Druid争取了很大的性能优化空间。
**数据内存化也是提高查询速度的杀手锏**。内存和硬盘的访问速度相差近百倍，但内存的大小是非常有限的，
因此在内存使用方面要精细设计，**比如Druid里面使用了Bitmap和各种压缩技术**。另外，为了支持
Drill-Down某些维度，**Druid维护了一些倒排索引**。这种方式可以加快AND和OR等计算操作。

### 4.2.水平扩展能力
Druid查询性能在很大程度上依赖于内存的优化使用。**数据可以分布在多个节点的内存中，因此当数据增长
的时候，可以通过简单增加机器的方式进行扩容。为了保持平衡，Druid按照时间范围把聚合数据进行分区处
理。对于高基数的维度，只按照时间切分有时候是不够的（Druid的每个Segment不超过2000万行），故
Druid还支持对Segment进一步分区**。

**历史Segment数据可以保存在深度存储系统中，存储系统可以是本地磁盘、HDFS或远程的云服务**。如果
某些节点同现故障，则可借助Zookeeper协调其他节点重新构造数据。

Druid的查询模块能够感知和处理集群的状态变化，查询总是在有效的集群架构中进行。集群上的查询可以进
行灵活的水平扩展。Druid内置提供了一些容易并行化的聚合操作，例如`Count`、`Mean`、`Variance`
和其他查询统计。对于一些无法并行化的操作，例如：`Median`、`Druid`暂时不提供支持。在支持直方图
（`Histogram`）方面，Druid也是通过一些近似计算的方法进行支持，以保证Druid整体的查询性能，这些
近似计算方法还包括`HyperLoglog`、`DataSketches`的一些基数计算。

### 4.3.实时分析
Druid提供了包含基于时间维度数据的存储服务，并且任何一行数据都是历史真实发生的事件，**因此在设计
之初就约定事件一但进入系统，就不能再改变**。

**对于历史数据druid以Segment数据文件的方式组织，并且将它们存储到深度存储系统中，例如文件系统或
亚马逊的S3等。当需要查询这些数据的时候，Druid再从深度存储系统中将它们装载到内存供查询使用**。

## 5.Druid的技术特点
+ 数据吞吐量大。
+ 支持流式数据摄入和实时。
+ 查询灵活且快。
+ 社区支持力度大。

## 6.Druid的Hello,World








