Spark安装
================================================================================
## 1.Spark先决条件
在安装Spark之前，请确保您的群集满足以下先决条件：
+ HDP集群堆栈版本为3.0或更高版本
+ Ambari版本为2.7.0或更高版本（可选）
+ HDFS和YARN部署在群集上

**仅支持Spark版本2**。

此外，请注意以下有关可选Spark服务和功能的要求和建议：
+ Spark Thrift服务器需要在群集上部署Hive。
+ SparkR需要在所有节点上安装R二进制文件。
+ 通过Livy访问Spark需要在群集上安装Livy服务器。
+ PySpark和相关库需要在所有节点上安装Python2.7或更高版本，或Python3.4或更高版本。
+ 要获得MLlib的最佳性能，请考虑安装`netlib-java`库。
    ```
    netlib-java相关信息：
    https://github.com/fommil/netlib-java
    ```

## 2.使用Ambari安装Spark
使用以下步骤在Ambari管理的群集上安装Apache Spark。

### 2.1.关于这个任务
下图显示了使用Ambari的Spark安装过程。在使用Ambari安装Spark之前，请参阅Ambari管理和监视群集
指南中的“*添加服务*”，以获取有关如何使用Ambari安装Hortonworks Data Platform（HDP）组件的
背景信息。

![安装spark](img/1.png)

```
警告

在安装过程中，Ambari会创建并编辑多个配置文件。如果使用Ambari配置和管理群集，请不要在安装期间或之后编辑这些文件。而是使用
Ambari Web UI修改配置设置。
```

### 2.2.程序
1. 单击Ambari仪表板上“*服务*”旁边的省略号（...）符号，然后单击“*添加服务*”。
2. 在“*添加服务向导*”上，选择“*Spark2*”，然后单击“*下一步*”。
3. 在分配`Masters`页面上，查看`Spark2 History Server`的节点分配，然后单击 *下一步*。
4. 在“分配Slaves和客户端”页面上：
    1. 滚动到右侧，选择要运行Spark客户端的客户端节点。这些是可以将Spark作业提交给YARN的节点。
    2. 要安装可选的Livy服务器以实现安全性和用户模拟功能，请选择`Livy for Spark2 Server`框
    以获取所需的节点分配。
    3. 要安装用于ODBC或JDBC访问的可选`Spark Thrift`服务器，请查看`Spark2 Thrift Server`
    节点分配并将一个或两个节点分配给Thrift服务器。
5. 单击“*下一步*”继续。
6. 在“*自定义服务*”页面上，为Thrift服务器设置以下配置属性：
    1. 单击高级 **spark-thrift-sparkconf**
    2. 将`spark.yarn.queue`属性值设置为要使用的YARN队列的名称。
7. 单击“*下一步*”继续。
8. 如果在群集上启用了`Kerberos`，请查看“*配置标识*”页面上的主体和密钥表设置，根据需要修改设置，
然后单击“*下一步*”。
9. 查看Review页面上的配置，然后单击 *Deploy* 以开始安装。
10. “安装”、“启动”和“测试”页面显示安装状态。
11. 当进度条达到100％并显示“*成功*”消息时，单击“*下一步*”。
12. 在“*摘要*”页面上，单击“*完成*”以完成安装Spark。

## 3.验证Hive访问的Spark配置
使用以下步骤验证Hive访问的Spark配置。使用Ambari安装Spark时，会自动使用`Hive Metastore`位置
填充`hive-site.xml`文件。






























dd
