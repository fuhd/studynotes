MapReduce和Hadoop简介
===================================================================================
Hadoop MapReduce作业被分成一系列运行在分布式集群中的 **map任务** 和 **reduce任务**。每个任务都
工作在被指定的小的数据子集上，因此负载是遍布集群中各个节点上的。**map任务主要负责数据的载入、解析、
转换和过滤。每个reduce任务负责处理map任务输出结果的一个子集。然后，reducer任务从mapper任务处复
制map任务的中间数据，进行分级和聚合操作**。从简单的数值聚合到复杂的关联操作以及笛卡尔积操作。

MapReduce作业的输入是一系列存储在Hadoop分布式文件系统（HDFS）上的文件。**在Hadoop中，这些文件
通过输入格式（`input format`）被分成了一系列的输入split（`input split`）。输入split可以看作是文件在字
节层面的分块表示，每个split由一个map任务负责处理**。

Hadoop中的每个map任务可以细分成 **4个阶段：`record reader`、`mapper`、`combiner`和`partitioner`。
map任务的输出被称为中间键和中间值**，会被发送到reducer做后续处理。reduce任务可以分为 **4个阶段：混
排（`shuffle`）、排序（`sort`）、reducer和输出格式（`output format`）。map任务运行的节点会优先选
择在数据所在的节点，因此，一般可以通过在本地机器上进行计算来减少数据的网络传输**。

## record reader





