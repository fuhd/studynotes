Apache Flink架构
================================================================================
## 1.系统架构 
Flink并没有依靠自身实现所有功能，而是在已有集群基础设施和服务之上专注于它的核心功能——**分布式数
据流处理。Flink和很多集群管理器（如：`Apache Mesos`、`YARN`及`Kubernetes`）都能很好地集成**；
同时它也可以通过配置，作为独立集群来运行。**Flink没有提供分布式持久化存储，而是利用了现有的分布
式文件系统（如`HDFS`）** 或对象存储 （如`S3`）。**它依赖`Apache ZooKeeper`来完成高可用性设
置中的领导选举工作**。

### 1.1.搭建Flink所需组件
Flink的搭建需要 **四个不同组件**，它们相互协作，共同执行流式应用。这些组件是，**JobManager、
ResourceManager、TaskManager和Dispatcher**。
+ **作为主进程（`master process`），JobManager控制着单个应用程序的执行。换句话说，每个应用
都由一个不同的JobManager掌控。JobManager可以接收需要执行的应用，该应用会包含一个所谓的JobGraph，
即逻辑Dataflow图，以及一个打包了全部所需类、库以及其他资源的JAR文件**。JobManager将JobGraph
转化成名为ExecutionGraph的物理Dataflow图。该图包含了那些可以并行执行的任务。JobManager从
ResourceManager申请执行任务的必要资源（**TaskManager处理槽**）。一旦它收到了足够数量的
TaskManager处理槽（`slot`），就会将ExecutionGraph中的任务分发给TaskManager来执行。在执行
过程中，JobManager还要负责所有需要集中协调的操作，如创建检查点。
+ **针对不同的环境和资源提供者（`resource provider`）（如YARN、Mesos、Kubernetes或独立部
署），Flink提供了不同的ResourceManager。ResourceManager负责管理Flink的计算资源单元——
TaskManager处理槽**。当JobManager申请TaskManager处理槽时，ResourceManager会指示一个拥有
空闲处理槽的TaskManager将其处理槽提供给JobManager。如果ResourceManager的处理槽数无法满足
JobManager的请求，则ResourceManager可以和资源提供者通信，让它们提供额外容器来启动更多
TaskManager进程。同时，ResourceManager还负责终止空闲的TaskManager以释放计算资源。
+ **TaskManager是Flink的工作进程（`worker process`）。通常在Flink搭建过程中要启动多个
TaskManager。每个TaskManager提供一定数量的处理槽。处理槽的数目限制了一个TaskManager可执行的
任务数。TaskManager在启动后，会向ResourceManager注册它的处理槽**。当接收到ResourceManager
的指示时，TaskManager会向JobManager提供一个或多个处理槽。之后，JobManager就可以向处理槽中分
配任务来执行。在执行期间，运行同一应用不同任务的TaskManager之间会产生数据交换。
+ **Dispatcher会跨多个作业运行，它提供了一个REST接口来让我们提交需要执行的应用。一旦某个应用提
交执行，Dispatcher会启动一个JobManager并将应用转交给它**。REST接口意味着Dispatcher这一集群
的HTTP入口可以受到防火墙的保护。Dispatcher同时还会启动一个Web UI，用来提供有关作业执行的信息。

下图展示了应用提交执行过程中Flink各组件之间的交互过程：

![应用提交及组件交互](img/19.png)

### 1.2.应用部署
Flink应用可以通过 **两种模式** 进行部署。

#### 框架模式
在该模式下，**Flink应用会打包成一个JAR文件，通过客户端提交到运行的服务上**。这里的服务可以是
Flink Dispatcher，Flink JobManager或是YARN的ResourceManager。无论哪种情况，运行的服务都
会接收Flink应用并确保其执行。如果应用提交到JobManager，会立即开始执行；如果应用提交到Dispatcher
或YARN ResourceManager，它们会启动一个JobManager并将应用转交给它，随后由JobManager负责执
行该应用。

#### 库模式 
在该模式下，**Flink应用会绑定到一个特定应用的容器镜像（如Docker镜像）中。镜像中还包含着运行
JobManager以及ResourceManager的代码。当容器从镜像启动后会自动加载ResourceManager和
JobManager，并将绑定的作业提交执行。另一个和作业无关的镜像负责部署TaskManager容器**。容器通过
镜像启动后会自动运行的TaskManager，后者可以连接ResourceManager并注册处理槽。通常情况下，外部
资源管理框架（如 **Kubernates**）负责启动镜像，并确保在发生故障时容器能够重启。

**基于框架的模式采用的是传统方式，即通过客户端提交应用（或查询）到正在运行的服务上；而在库模式中，
Flink不是作为服务，而是以库的形式绑定到应用所在的容器镜像中**。后者常用于微服务架构。

### 1.3.任务执行
**一个TaskManager允许同时执行多个任务。这些任务可以属于同一个算子（数据并行），也可以是不同算
子（任务并行），甚至还可以来自不同的应用（作业并行）**。下图展示了TaskManager、处理槽 、任务以
及算子之间的关系：

![算子、任务以及处理槽](img/20.png)

左侧的JobGraph（应用的非并行化表示）包含了5个算子，其中算子A和C是数据源，算子E是数据汇。算子C
和E的并行度为2，其余算子的并行度为4。由于算子最大并行度是4，因此应用若要执行则至少需要4个处理槽。
如果每个TaskManager内有两个处理槽，则运行两个TaskManager即可满足该需求。JobManager将
JobGraph“展开成”ExecutionGraph并把任务分配到4个空闲处理槽。对于并行度为4的算子，其任务会每个
处理槽分配一个。其余两个算子C和E的任务会分别放到处理槽1.1、2.1和处理槽1.2、2.2中。将任务以切片
的形式调度至处理槽中有一个好处：TaskManager中的多个任务可以在同一进程内高效地执行数据交换而无须 
访问网络。

**TaskManager会在同一个JVM进程内以多线程的方式执行任务**。和独立进程相比，线程更加轻量并且通
信开销更低，**但无法严格地将任务彼此隔离**。因此只要有一个任务运行异常，就有可能“杀死”整个
TaskManager进程，导致它上面运行的所有任务都停止。**如果将每个TaskManager配置成只有一个处理槽，
则可以限制应用在TaskManager级别进行隔离，即每个TaskManager只运行单个应用的任务**。通过在
TaskManager内部采用线程并行以及在每个主机上部署多个TaskManager进程，**Flink为部署应用时性能
和资源隔离的取舍提供了极大的自由度**。

### 1.4.高可用性设置
流式应用通常都会设计成7*24小时运行，因此对于它很重要的一点是：**即便内部进程发生故障时也不能终止
运行。为了从故障中恢复，系统首先要重启故障进程，随后需要重启应用并恢复其状态**。

#### TaskManager故障
如前所述，为了执行应用的全部任务，Flink需要足够数量的处理槽。**假设一个Flink设置包含4个
TaskManager，每个TaskManager有2个处理槽，那么一个流式应用最多支持以并行度8来运行。如果有一个
TaskManger出现故障，则可用处理槽的数量就降到了6个，这时候JobManager就会向ResourceManager申
请更多的处理槽。若无法完成（例如应用运行在一个独立集群上），JobManager将无法重启应用，直至有足
够数量的可用处理槽**。应用的重启策略决定了JobManager以何种频率重启应用以及重启尝试之间的等待间
隔。

#### JobManager故障
和TaskManager相比，JobManager发生故障会更为棘手。它用于控制流式应用执行以及保存该过程中的元
数据（如已完成检查点的存储路径）。**如果负责管理的JobManager进程消失，流式应用将无法继续处理数
据。这就导致JobManager成为Flink应用中的一个单点失效组件**。为了解决该问题，**Flink提供了高可
用模式，支持在原JobManager消失的情况下将作业的管理职责及元数据迁移到另一个JobManager**。

**Flink中的高可用模式是基于能够提供分布式协调和共识服务的Apache ZooKeeper来完成的，它在Flink
中要用于”领导“选举以及持久且高可用的数据存储。JobManager在高可用模式下工作时，会将JobGraph以
及全部所需的元数据（例如应用的JAR文件）写入一个远程持久化存储系统中。此外，JobManager还会将存储 
位置的路径写入ZooKeeper的数据存储。在应用执行过程中，JobManager会接收每个任务检查点的状态句柄、
（存储位置）。在检查点即将完成的时候，如果所有任务已经将各自状态成功写入远程存储 ，JobManager就
会将状态句柄写入远程存储，并将远程存储的路径地址写入ZooKeeper。因此所有用于JobManager故障恢复
的数据都在远程存储上面，而ZooKeeper持有这些存储位置的路径**。如下图：

![Flink高可用设置](img/21.png)

**当JobManager发生故障时，其下应用的所有任务都会自动取消**。新接手工作的JobManager会执行以下
步骤：
1. 向ZooKeeper请求存储位置，以获取JobGraph、JAR文件以及应用最新检查点在远程存储的状态句柄。
2. 向ResourceManager申请处理槽来继续执行应用。
3. 重启尖用并利用最近一次检查点重置任务状态。

如果是在容器环境（如：Kubernates）中以库模式部署运行应用，容器编排服务通常会自动重启故障的
JobManager或TaskManager容器。当运行在YARN或Mesos上面时，Flink的其余进程会触发JobManager
或TaskManager进程重启。Flink没有针对独立集群模式提供重启故障进程的工具，因此有必要运行一些后备
JobManager及TaskManager来接管故障进程的工作。

## 2.Flink中的数据传输
在运行过程中，应用的任务会持续进行数据交换。**TaskManager负责将数据从发送任务传输至接收任务。
它的网络模块在记录传输前会先将它们收集到缓冲区中。换言之，记录并非逐个发送的，而是在缓冲区中以批次
形式发送。该技术是有效利用网络资源、实现高吞吐的基础**。

每个TaskManager都有一个用于收发数据的网络缓冲池（每个缓冲默认32KB大小）。如果发送端和接收端的
任务运行在不同的TaskManager进程中，它们就要用到操作系统的网络栈进行通信。流式应用需要以流水线方
式交换数据，因此每对TaskManager之间都要维护一个或多个永久的TCP连接来执行数据交换。在Shuffle连
接模式下，每个发送端任务都需要向任意一个接收任务传输数据。对于每一个接收任务，TaskManager都要提
供一个专用的网络缓冲区，用于接收其他任务发来的数据。如下图：

![TaskManager之间的数据传输](img/22.png)

如上图所示，由于接收端的并行度为4,所以每个发送端任务至少需要4个网络缓冲区来向任一接收端任务发送
数据。同理，每个接收端任务也需要至少4个缓冲区来接收数据。缓冲区内的数据在向对方TaskManager传输
时会共享网络连接。为了使流水线式的数据交换平滑进行，TaskManager必须提供足够多的缓冲区来同时服务
所有进出连接。**在Shuffle或广播连接的情况下，每个发送任务都需要为每个接收任务提供一个缓冲区，因
此所需的缓冲区数量可达到相关算子任务数的平方级别。Flink默认的网络缓冲区配置足以应对中小型使用场
景**。而对于大型使用场景，需要根据第9章“内存和网络缓冲”中所介绍的内容调整配置。

**当发送任务和接收任务处于同一个TaskManager进程时，发送任务会将要发送的记录序列化到一个字节缓
冲区，一旦该缓冲区占满就会被放到一个队列里。接收任务会从这个队列里获取缓冲区并将其中的记录反序列
化。这意味着同一个TaskManager内不同任务之间的数据传输不会涉及网络通信**。

Flink采用多种技术来降低任务之间的通信开销。

### 2.1.基于信用值的流量控制
通过网络连接逐条发送记录不但低效，还会导致很多额外开销。若想充分利用网络连接带宽，就需要对数据进
行缓冲。**在流处理环境下，缓冲的一个明显缺点是会增加延迟，因为记录首先要收集到缓冲区中而不会立即
发送**。

**Flink实现了一个基于信用值的流量控制机制，它的工作原理如下：接收任务会给发送任务授予一定的信用
值，其实就是保留一些用来接收它数据的网络缓冲。一旦发送端收到信用通知，就会在信用值所限定的范围内
尽可能多地传输缓冲数据，并会附带上积压量（已经填满准备传输的网络缓冲数目）大小。接收端使用保留的
缓冲来处理收到的数据，同时依据各发送端的积压量信息来计算所有相连的发送端在下一轮的信用优先级**。

**由于发送端可以在接收端有足够资源时立即传输数据，所以基于信用值的流量控制可以有效降低延迟。此外，
信用值的授予是根据各发送端的数据积压量来完成的，因此该机制还能在出现数据倾斜（`data skew`）时有
效地分配网络资源。不难看出，基于信用值的流量控制是Flink实现高吞吐低延迟的重要一环**。

### 2.2.任务链接
**Flink采用一种名为任务链接的优化技术来降低某些情况下的本地通信开销。任务链接的前提条件是，多个
算子必须有相同的并行度且通过本地转发通道（`local forward channel`）相连**。如下图中算子所组
成的流水线就满足上述条件。它包含了3个算子，每个算子的任务并行度都为2且通过本地转发方式连接。

![满足任务链接条件的算子流水线](img/23.png)

下图展示了流水线如何在任务链接模式下执行。**多个算子的函数被“融合”到同一个任务中，在同一个线程内
执行。函数生成的记录只需通过简单的方法调用就可以分别发往各自的下游函数。因此函数之间的记录传输基
本上不存在序列化及通信开销**。

![单线程执行的链接任务“融合“了多个函数，并通过方法调用进行数据传输](img/24.png)

虽然任务链接可以有效地降低本地任务之间的通信开销，但有的流水线应用反而不希望用到它。举例而言，
**有时候我们需要对过长任务链接进行切分或者将两个计算量大的函数分配到不同的处理槽中**。如下图展示
了相同的流水线在 **非任务链接模式** 下执行。其中每个函数都交由单独的任务、在特定线程内处理。

![利用专用线程执行非链接任务并通过缓冲通道及序列化进行数据传输](img/25.png)

**Flink在默认情况下会开启任务链接**。

## 3.事件时间处理
**虽然处理时间是基于处理机器的本地时间**，相对容易理解，**但它会产生一些较为随意、不一致且无法重
现的结果**。相反，**事件时间语义会生成可重现且一致性的结果，这也是很多流处理用例的刚性需求**。但
和基于处理时间语义的应用相比，基于事件时间的应用需要一些额外的配置。此外，相比纯粹使用处理时间的
引擎，支持事件时间的流处理引擎内部要更加复杂。

**Flink不仅针对常见的事件时间操作提供了直观易用的原语，还支持一些表达能力很强API，允许使用者以
自定义算子的方式实现更高级的事件时间处理应用。在面对这些高级应用时，充分理解Flink内部事件处理机
制通常会有所帮助，有时候更是必要的**。上一章我们介绍了Flink在提供处理时间语义时所采用的两个概念：
**记录时间戳和水位线**。接下来我们会介绍Flink内部如何实现和处理时间戳及水位线以支持事件时间语义
的流式应用。

### 3.1.时间戳
**在事件时间模式下，Flink流式应用处理的所有记录都必须包含时间戳。时间戳将记录和特定时间点进行关
联，这些时间点通常是记录所对应事件的发生时间**。正如“时间语义”中所述，**基本上所有现实应用场景都
会出现一定程度的时间戳乱序**。

**Flink内部采用8字节的Long值对时间戳进行编码，并将它们以元数据（`metadata`）的形式附加在记录
上。内置算子会将这个Long值解析为毫秒精度的Unix时间戳（自`1970-01-01-00:00:00:0000`以来的毫
秒数）。但自定义算子可以有自己的时间戳解析机制，如将精度调整为微秒**。

### 3.2.水位线
**除了记录的时间戳，Flink基于事件时间的应用还必须提供水位线（`watermark`）。水位线用于在事件
时间应用中推断每个任务当前的事件时间。基于时间的算子会使用这个时间来触发计算并推动进度前进**。例
如：基于时间窗口的任务会在其事件时间超过窗口结束边界时进行最终的窗口计算并发出结果。

**在Flink中，水位线是利用一些包含Long值时间戳的特殊记录来实现的**。如下图：

![包含带有时间戳的记录及水位线的数据流](img/26.png)

水位线拥有两个基本属性：
1. **必须单调递增。这是为了确保任务中的事件时间时钟正确前进，不会倒退**。
2. **和记录的时间戳存在联系。一个时间戳为T的水位线表示，接下来所有记录的时间戳一定都大于T**。

**第二个属性可用来处理数据流中时间戳乱序的记录**，例如上图中的时间戳为3和5的记录。对基于时间的算
子任务而言，其收集和处理的记录可能会包含乱序的时间戳。**这些算子只有当自己的事件时间时钟（由接收
的水位线驱动）指示不必再等那些包含相关时间戳的记录时，才会最终触发计算。当任务收到一个违反水位线
属性，即时间戳小于或等于前一个水位线的记录时，该记录本应参与的计算可能已经完成。我们称此类记录为
迟到记录（`late record`）。为了处理迟到记录，Flink提供了不同的机制，我们将在第6章“处理迟到数
据”中讨论它们**。

**水位线的意义之一在于它允许应用控制结果的完整性和延迟。如果水位线和记录的时间戳非常接近，那结果
的处理延迟就会很低，因为任务无须等待过多记录就可以触发最终计算。但同时结果的完整性可能会受影响，
因为可能有部分相关记录被视为迟到记录，没能参与运算。相反，非常“保守”的水位线会增加处理延迟，但同
时结果的完整性也会有所提升**。

### 3.3.水位线传播和事件时间 
**Flink内部将水位线实现为特殊的记录，它们可以通过算子任务进行接收和发送。任务内部的时间服务（
`time service`）会维护一些计时器，它们依靠接收到水位线来激活。这些计时器是由任务在时间服务内注
册，并在将来的某个时间点执行计算**。例如：窗口算子会为每个活动窗口注册一个计时器，它们会在事件时
间超过窗口的结束时间清理窗口状态。

当任务接收到一个水位线时会执行以下操作：
1. **基于水位线记录的时间戳更新内部事件时间时钟**。
2. **任务的时间服务会找出所有触发时间小于更新后事件时间的计时器。对于每个到期的计时器，调用回调
函数、利用它来执行计算或发出记录**。
3. **任务根据更新后的事件时间将水位线发出**。

> Flink对通过DataStream API访问时间戳和水位线有一定限制。普通函数无法读写记录的时间戳或水位线，
> 但一系列处理函数除外。它们可以读取当前正在处理记录的时间戳，获得当前算子的事件时间，还能注册计
> 时器。所有函数的API都无法支持设置发出记录的时间戳、调整任务的事件时间时钟或发出水位线。为发出
> 记录配置时间戳的工作需要由基于时间的DataStream算子任务来完成，这样才能确保时间戳和发出的水位
> 线对齐。举例而言，时间窗口算子任务会在发送触发窗口计算的水位线时间戳之前，将所有经过窗口计算所
> 得结果的时间戳设为窗口的结束时间。

接下来我们详细解释一下任务在收到一个新的水位线之后，将如何发送水位线和更新其内部事件时间时钟。正
如第2章的“数据并行和任务并行”中所述，**Flink会将数据流划分为不同的分区，并将它们交由不同的算子
任务来并行执行。每个分区作为一个数据流，都会包含带有时间戳的记录以及水位线**。根据上下游连接情况，
其任务可能需要同时接收来自多个输入分区的记录和水位线，也可能需要将它们发送到多个输出分区。

**一个任务会为它的每个输入分区都维护一个分区水位线。当收到某个分区传来的水位线后，任务会以接收值
和当前值中较大的那个去更新对应分区水位线的值。随后，任务会把事件时间时钟调整为所有分区水位线中最
小的那个值。如果事件时间时钟向前推动，任务会先处理因此而触发的所有计时器，之后才会把对应的水位线
发往所有连接的输出分区，以实现事件时间 到全部下游任务的广播**。

下图展示了一个有4个输入分区和3个输出分区的任务在接收到水位线后，是如何更新它的分区水位线和事件时
间时钟，并将水位线发出的。

![利用水位线更新任务的事件时间](img/27.png)

对于那些有着两条或多条输入数据流的算子，如`Jnion`或`CoFlatMap`（详见第5章的“多流转换”），它们
的任务同样是利用全部分区水位线中的最小值来计算事件时间时钟，并没有考虑分区是否来自不同的输入流，
这就导致所有输入的记录都必须基于同一个事件时间时钟来处理。如果不同输入流的事件时间没有对齐，那么
该行为就会导致一些问题。

**Flink的水位线处理和传播算法保证了算子任务所发出的记录时间戳和水位线一定会对齐。然而，这依赖于
一个事实，所有分区都会持续提供自增的水位线。只要有一个分区的水位线没有前进，或分区完全空闲下来不
再发送任何记录或水位线，任务的事件时间时钟就不会前进，继而导致计时器无法触发。这种情形会给那些靠
时钟前进来执行计算或清除状态的时间相关算子带来麻烦。因此，如果一个任务没有从全部输入任务以常规间
隔接收新的水位线，就会导致时间相关算子的处理延迟或状态大小激增**。

**当算子两个输入流差距很大时，也会产生类似影响。对于一个有两个输入流的任务而言，其事件时间时钟会
受制于那个相对较慢的流。而较快流的记录或中间结果会在状态中缓冲，直到事件时间时钟到达允许处理它们
的那个点**。

### 3.4.时间戳分配和水位线生成
**时间戳和水位线通常都是在数据流刚刚进入流处理应用的时候分配和生成的。由于不同的应用会选择不同的
时间戳，而水位线依赖于时间戳和数据流本身的特征，所以应用必须显式地分配时间戳和生成水位线**。
Flink DataStream应用可以通过三种方式完成该工作。

#### 在数据源完成 
我们可以 **利用`SourceFunction`在应用读入数据流的时候分配时间戳和生成水位线。`SourceFunction`
会发出一条记录流。每个发出的记录都可以附加一个时间戳，水位线可以作为特殊记录在任何时间点发出。如
果`SourceFunction`（临时性地）不再发出水位线，可以把自己声明成空闲。Flink会在后续算子计算水位
线的时候把那些来自于空闲`SourceFunction`的流分区排除在外。数据源空闲声明机制可以用来解决上面提
到的水位线不向前推进的问题**。我们会在第8章的“实现自定义数据源函数”中详细讨论数据源函数。

#### 周期分配器
**DataStream API提供了一个名为`AssignerWithPeriodicWatermarks`的用户自定义函数，它可以
用来从每条记录提取时间戳，并周期性地响应获取当前水位线的查询请求。提取出来的时间戳会附加到各自的
记录上，查询得到的水位线会注入到数据流中**。这个函数会在第6章的“分配时间戳和生成水位线”中介绍。

#### 定点分配器
**另一个支持从记录中提取时间戳的用户自定义函数叫作`AssignerWithPunctuatedWatermarks`。它可
以用于需要根据特殊输入记录生成水位线的情况。和`AssignerWithPeriodicWatermarks`函数不同，这
个函数不会强制你从每条记录中都提取一个时间戳（虽然这样也行）**。我们同样会在第6章的“分配时间戳和
生成水位线”中详细讨论它。

## 4.状态管理 
本节我们会对Flink支持的 **不同类别的状态** 进行介绍。我们将解释如何利用 **状态后端
（`state backend`）对状态进行存储和维护，以及有状态的应用如何通过状态再分配实现扩缩容**。通常 
意义上，函数里所有需要任务去维护并用来计算结果的数据都属于任务的状态。你可以把状态想象成任务的业
务逻辑所需要访问的 **本地或实例变量**。如下图展示了某个任务和它状态之间的典型交互过程：

![带有状态的流处理任务](img/28.png)

任务首先会接收一些输入数据。在处理这些数据的过程中，任务对其状态进行读取或更新，并根据状态和输入
数据计算结果。我们以一个持续计算接收到多少条记录的简单任务为例。当任务收到一个新的记录后，首先会
访问状态获取当前统计的记录数目 ，然后把数目增加并更新状态，最后将更新后的数目发送出去。

应用读写状态的逻辑通常都很简单，而难点在于如何高效、可靠地管理状态。**这其中包括如何处理数量巨大、
可能超出内存的状态，如何保证发生故障时状态不会丢失。所有和状态一致性、故障处理以及高效存取相关的
问题都由Flink负责搞定**，这样开发人员就可以专注于自己的应用逻辑。

在Flink中，状态都是和特定算子相关联。**为了让Flink的运行层知道算子有哪些状态，算子需要自己对其
进行注册**。根据作用域的不同，状态可以分为两类：**算子状态（`operator state`）和键值分区状态
（`keyed state`）**。

### 4.1.算子状态 
算子状态的作用域是某个算子任务，这意味着所有在同一个并行任务之内的记录都能访问到相同的状态。**算
子状态不能通过其他任务访问，无论该任务是否来自相同算子**。

![带有算子状态的任务](img/29.png)

Flink为算子状态提供了 **三类原语**：

+ **列表状态（list state）**：
将状态表示为一个条目列表。

+ **联合列表状态（union list state）**：
同样是将状态表示为一个条目列表，**但在进行故障恢复或从某个保存点启动应用时，状态的恢复方式和普通
列表状态有所不同**。

+ **广播状态（broadcast state）**：
专门为那些 **需要保证算子的每个任务状态都相同的场景** 而设计。这种相同的特性将 **有利于检查点保
存或算子扩缩容**。

### 4.2.键值分区状态 
**键值分区状态会按照算子输入记录所定义的键值来进行维护或访问。Flink为每个键值都维护了一个状态实
例，该实例总是位于那个处理对应键值记录的算子任务上。当任务在处理一个记录时，会自动把状态的访问范
围限制为当前记录的键值**。

因此所有键值相同的记录都能访问到一样的状态。下图展示了任务和键值分区状态的交互过程：

![带有键值分区状态的任务](img/30.png)

**你可以把键值分区状态想象成一个在算子所有并行任务上进行分区（或分片）的键值映射**。Flink为键值
分区状态提供不同原语，**它们的区别在于分布式键值映射中每个键所对应存储值的类型不同**。下面讨论一
下键值分区状态最常用的几个原语：
+ **单值状态（value state）**：每个键对应存储一个任意类型的值，该值也可以是某个复杂数据结构。
+ **列表状态（list state）**：每个键对应存储一个值的列表。列表中的条目 可以是任意类型。
+ **映射状态（map state）**：每个键对应存储一个键值映射。该映射的键和值可以是任意类型。

通过这些状态原语，我们可以为Flink状态指定不同的结构，从而实现更加高效的状态访问。

### 4.3.状态后端
**有状态算子的任务通常会对每一条到来的记录读写状态，因此高效的状态访问对于记录处理的低延迟而言至
关重要。为了保证快速访问状态，每个并行任务都会把状态维护在本地**。至于状态具体的存储、访问和维护，
则是由个名为 **状态后端** 的可插拔组件来决定。**状态后端主要负责两件事，本地状态管理和将状态以检
查点的形式写入远程存储**。

对于本地状态管理，状态后端会存储所有键值分区状态，并保证能将状态访问范围正确地限制在当前键值。
**Flink提供的一类状态后端会把键值分区状态作为对象，以内存数据结构的形式存在JVM堆中，另一类状态
后端会把状态对象序列化后存在RocksDB中，RocksDB负责将它们写到本地硬盘上。前者状态访问会更快一些，
但会受到内存大小的限制；后者状态访问会慢一些，但允许状态变得很大**。

**由于Flink是一个分布式系统但只在本地维护状态，所以状态检查点就显得极其重要**。而考虑到
TaskManager进程以及它上面所有运行的任务都可能在任意时间出现故障，因此它们的存储只能看做是易失的。
**状态后端负责将任务状态以检查点形式写入远程持久化存储，该远程存储可能是一个分布式文件系统，也可
能是某个数据库系统。不同的状态后端生成状态检查点的方式也存在一定差异**。例如：**RocksDB状态后端
支持增量检查点。这对于大规模的状态而言，会显著降低生成检查点的开销**。

### 4.4.有状态算子的扩缩容
流式应用的一项基本需求是根据输入数据到达速率的变化调整算子并行度。**对于无状态的算子，扩缩容很容
易。对于有状态算子，改变并行度就会复杂很多，因为我们需要把状态重新分组，分配到与之前数量不等的并
行任务上**。Flink对不同类型的状态提供了 **四种扩缩容模式**。

**带有键值分区状态的算子在扩缩容时会根据新的任务数量对键值重新分区。但为了降低状态在不同任务之间
迁移的必要成本，Flink不会对单独的键值实施再分配，而是会把所有键值分为不同的键值组（`key group`
）。每个键值组都包含了部分键值，Flink以此为单位把键值分配给不同任务**。下图展示了键值分区状态通
过键值组进行重新分区的过程。

![算子扩缩容时键值分区状态的调整](img/31.png)

**带有算子列表状态的算子在扩缩容时会对列表中的条目进行重新分配。理论上，所有并行算子任务的列表条
目会被统一收集起来，随后均匀分配到更少或更多的任务之上**。如果列表条目的数量小于算子新设置的并行
度，**部分任务在启动时的状态就可能为空**。下图展示了算子列表状态的重分配过程：

![算子扩缩容时算子列表状态的调整](img/32.png)

**带有算子联合列表状态的算子会在扩缩容时把状态列表的全部条目广播到全部任务上。随后由任务自己决定
哪些条目该保留，哪些该丢弃**。下图展示了算子联合列表的重分配过程。

![算子扩缩容时算子联合列表状态的调整](img/33.png)

**带有算子广播状态的算子在扩缩容时会把状态拷贝到全部新任务上。这样做的原因是广播状态能确保所有任
务的状态相同。在缩容的情况下，由于状态经过复制不会丢失，我们可以简单地停掉多出的任务**。下图展示
了算子广播状态的重分配过程。

![算子扩缩容时算子广播状态的调整](img/34.png)

## 5.检查点、保存点及状态恢复 
Flink是一个分布式的数据处理系统，因此必须能够处理一些故障，例如：进程被强制关闭、机器故障以及网
络连接中断。**由于每个任务会把状态维护在本地，Flink要保证发生故障时状态不丢不错**。

本节我们将介绍 **Flink的检查点（`checkpoint`）及故障恢复机制**，看一下它们 **如何提供精确一
次的状态一致性保障**。此外我们还会讨论 **Flink所独有的保存点（`savepoint`）机制**。

### 5.1.一致性检查点
**Flink的故障恢复机制需要基于应用状态的一致性检查点。有状态的流式应用的一致性检查点是在所有任务
处理完等量的原始输入后对全部任务状态进行的一个拷贝**。我们可以通过一个朴素算法对应用建立一致性检
查点的过程进行解释。朴素算法的步骤包括：
1. **暂停接收所有输入流**。
2. **等待已经流入系统的数据被完全处理，即所有任务已经处理完所有的输入数据**。
3. **将所有任务的状态拷贝到远程持久化存储，生成检查点。在所有任务完成自己的拷贝工作后，检查点生
成完毕**。
4. **恢复所有数据流的接收**。

注意，Flink没有实现这种朴素策略，而是使用了一种更加复杂的检查点算法。我们会在本节后面介绍该算法。

下图展示了针对一个简单应用的一致性检查点。

![某流式应用的一致性检查点](img/35.png)

该应用有一个数据源任务，负责从一个递增数字（1,2,3,4......）流中读取数据。数字流会被分成奇数流
和偶数流，求和算子的两个任务会分别对它们求和。**数据源算子的任务会把输入流的当前偏移量存为状态；
求和算子的任务会把当前和值存为状态**。在上图中，Flink会在输入偏移到达5的时候生成一个检查点，此
时两个和值分别为6和9。

### 5.2.从一致性检查点中恢复 
在流式应用执行过程中，**Flink会周期性地为应用状态生成检查点。一旦发生故障，Flink会利用最新的检
查点将应用状态恢复到某个一致性的点并重启处理进程**。下图展示了整个恢复过程。

![从检查点恢复应用](img/36.png)

应用恢复要经过3个步骤：
1. 重启整个应用。
2. 利用最新的检查点重置任务状态。
3. 恢复所有任务的处理。

**如果所有算子都将它们全部的状态写入检查点并从中恢复，并且所有输入流的消费位置都能重置到检查点生
成那一刻。那么该检查点和恢复机制就能为整个应用的状态提供精确一次的一致性保障**。数据源能否重置其
输入流取决于它的具体实现以及所消费外部系统是否提供相关接口。例如，**类似Apache Kafka的事件日志
系统就允许从之前的某个偏移读取记录。相反，如果数据流是从套接字（`socket`）消费而来则无法重置**。
因为套接字会在数据被取走后将它们丢弃。因此只有所有输入流都是来自于可重置的数据源，应用才支持精确
一次的状态一致性。

应用从检查点恢复以后，它的内部状态会和生成检查点的时候完全一致。**随后应用就会重新消费并处理那些
从之前检查点完成开始，到发生系统故障之间已经处理过的数据**。虽然这意味着Flink会重复处理部分消息，
但上述机制仍然可以实现精确一次的状态一致性，因为所有算子的状态都会重置到过去还没有处理过那些数据
的时间点。

需要强调的是，**Flink的检查点和恢复机制仅能重置流式应用内部的状态**。根据应用所采用的数据汇算子，
在恢复期间，某些结果记录可能会向下游系统（如事件日志系统、文件系统或数据库）**发送多次**。对于某
些存储系统，**Flink提供的数据汇函数支持精确一次输出，例如在检查点完成后才会把写出的记录正式提交。
另一种适用于很多存储系统的方法是幂等更新**。有关端到端精确一次应用所面临的挑战和解决方案会在第8
章的“应用一致性保障”中详细讨论。

### 5.3.Flink检查点算法 
**Flink的故障恢复机制需要基于应用的一致性检查点**。针对流式应用，生成检查点的 **朴素方法** 就
是暂停执行，生成检查点，然后恢复应用。**但这种“停止一切”的行为，即便对于那些具有中等延迟要求的应
用也很不切实际**。而Flink的检查点是基于 **`Chandy-Lamport`分布式快照算法** 来实现的。**该算
法不会暂停整个应用，而是会把生成检查点的过程和处理过程分离，这样在部分任务持久化状态的过程中，其
他 任务还可以继续执行**。接下来我们解释一下这个算法的工作原理。

**Flink的检查点算法中会用到一类名为检查点分隔符（`checkpoint barrier`）的特殊记录。和水位线
类似，这些检查点分隔符会通过数据源算子注入到常规的记录流中**。相对其他记录，它们在流中的位置无法
提前或延后。**为了标识所属的检查点，每个检查点分隔符都会带有一个检查点编号**，这样就把一条数据流
从逻辑上分成了两个部分。**所有先于分隔符的记录所引起的状态更改都会被包含在分隔符所对应的检查点之
中；而所有晚于分隔符的记录所引起的状态更改都会被纳入之后的检查点中**。

我们通过一个简单流式应用的示例来一步一步解释这个算法。应用包含了两个数据源任务，每个任务都会各自
消费一条自增数字流。数据源任务的输出会被分成奇数流和偶数流两个部分，每一部分都会有一个任务负责对
收到的全部数字求和，并将结果值更新至下游数据汇。应用细节如下图所示：

![拥有两个有状态的数据源、两个有状态的任务，以及两个无状态数据汇的流式应用](img/37.png)

如下图所示，**JobManager会向每个数据源任务发送一个新的检查点编号，以此来启动检查点生成流程**。

![JobManager通过向所有数据源发送消息来启动检查点生成流程](img/38.png)

**当一个数据源任务收到消息后，会暂停发出记录，利用状态后端触发生成本地状态的检查点，并把该检查点
分隔符连同检查点编号广播至所有传出的数据流分区。状态后端会在状态存为检查点完成后通知任务，随后任
务会给JobManager发送确认消息。在将所有分隔符发出后，数据源将恢复正常工作。通过向输出流中注入分
隔符，数据源函数定义了需要在流中哪些位置生成检查点**。

下图展示了流式应用为数据源任务的本地状态生成检查点并发出检查点分隔符。

![数据源为状态生成检查点并发出检查点分隔符](img/39.png)

**数据源任务发出的检查点分隔符会传输到与之相连的任务。和水位线类似，检查点分隔符总是以广播形式发
送，从而可以确保每个任务能从它们的每个输入都收到一个分隔符。当任务收到一个新检查点的分隔符时，会
继续等待所有其他输入分区也发来这个检查点的分隔符。在等待过程中，它会继续处理那些还未提供分隔符的
分区发来的数据。对于已经提供分隔符的分区，它们新到来的记录会被缓冲起来，不能处理。这个等待所有分
隔符到达的过程称为分隔符对齐**，我们在下图中对它进行了展示：

![任务等待接收所有输入分区的分隔符，来自已接收分隔符输入分区的记录会被缓存，其他记录则按常规处理](img/40.png)

如上图所示，**任务在收齐全部输入分区发送的分隔符后，就会通知状态后端开始生成检查点，同时把检查点
分隔符广播到下游相连的任务**。

![任务在收到全部分隔符后将状态存入检查点，然后向下游转发检查点分隔符](img/41.png)

任务在发出所有的检查点分隔符后就会开始处理缓冲的记录。待所有缓冲的记录处理完后，任务就会继续处理
输入流。下图展示了此时的应用状态：

![任务在转发检查点分隔符后继续进行常规处理](img/42.png)

**最终检查点分隔符到达数据汇任务。数据汇任务在收到分隔符后会依次执行分隔符对齐，将自身状态写入检
查点，向JobManager确认已接收分隔符等一系列动作。JobManager在接收到所有应用任务返回的检查点确
认消息后，就会将此次检查点标记为完成**。下图展示了检查点算法的最后一步。如下图所述，应用在发生故
障时就可以利用这个生成好的检查点进行恢复。

![数据汇任务向JobManager确认收到检查点分隔符，在所有任务成功将自身状态存入检查点后整个应用的检查点才算完成](img/43.png)

### 5.4.检查点对性能的影响 
虽然Flink的检查点算法能够在 **不停止整个应用的情况下** 为流式应用生成一致的分布式检查点，**但
它仍会增加应用处理延迟**。Flink实现了一些 **调整策略**，可以 **减轻某些条件下对性能的影响**。

任务在将其状态存入检查点的过程中，**会处于阻塞状态**，此时的输入会进入 **缓冲区**。由于状态可能
会很大，而且生成检查点需要把这些数据通过网络写入远程存储系统，该过程可能持续数秒，甚至数分钟。这
对于一些延迟敏感的应用而言时间过久。**按照Flink的设计，是由状态后端负责生成检查点，因此任务的状
态的具体拷贝过程完全取决于状态后端的实现**。举例而言，**文件系统状态后端和RocksDB状态后端支持异
步生成检查点。当检查点生成过程触发时，状态后端会为当前状态创建一个本地拷贝。在本地拷贝创建完成后，
任务就可以继续它的常规处理。后台进程会异步将本地状态快照拷贝到远程存储，然后在完成检查点后通知任
务。异步生成检查点可以有效降低任务恢复数据处理所需等待的时间**。除此之外，**RocksDB状态后端还支
持增量生成检查点，这可以有效降低需要传输的数据量**。

**我们还可以对分隔符对齐这一步进行调整，以降低检查点算法对处理延迟的影响。对于那些需要极低延迟且
能容忍至少一次状态保障的应用，可以通过配置让Flink在分隔符对齐的过程中不缓冲那些已收到分隔符所对应
分区的记录，而是直接处理它们。待所有的检查点分隔符都到达以后，算子才将状态存入检查点，这时候状态
可能会包含一些由本应出现在下一次检查点的记录所引起的改动。一旦出现故障，这些记录会被重复处理，而
这意味着检查点只能提供至少一次而非精确一次的一致性保障**。

### 5.5.保存点 
Flink的故障恢复算法是基于 **状态的检查点** 来完成的。**检查点会周期性地生成，而且会根据配置的
策略自动丢弃**。检查点的目的是保证应用在出现故障的时候可以顺利重启，**因此当应用被手动停止后，检
查点也会随之删除（可以通过配置让应用在取消的时候保留最近一次检查点）**。但除了用于故障恢复，**应
用的一致性快照还有很多其他用途**。

**Flink最具价值且独具一格的功能之一是保存点。原则上，保存点的生成算法和检查点完全一样，因此可以
把保存点看做包含一些额外元数据的检查点。保存点的生成不是由Flink自动完成，而是需要由用户（或外部
调度器）显式触发。同时，Flink也不会自清理保存点**。

#### 保存点的使用
**给定一个应用和一个兼容的保存点，我们可以从该保存点启动应用。这样就能用保存点内的数据初始化状态
并从生成保存点的那一刻继续运行应用。这个行为看上去和利用检查点将应用从故障中恢复完全一致，但其实
故障恢复只是一种特殊情况，它会在完全相同的集群上，以完全相同的配置，运行完全相同的应用。而将应用
从某个保存点启动还能让你做更多事件**。
+ **从保存点启动一个不同但相互兼容的应用**。这意味着你可以修复应用的一些逻辑Bug，然后在数据流来
源的支持范围内尽可能多地重新处理输入事件，以此来修复结果。**应用修改还可用于A/B测试或需要不同业
务逻辑的假想场景**。需要注意的是，**应用和保存点必须相互兼容**，只有这样应用才能加载保存点内的状
态。
+ **用不同的并行度启动原应用，从而实现应用的扩缩容**。
+ **在另一个集群上启动相同的应用**。这允许你把应用迁移到一个新的Flink版本，或是一个不同的集群或
数据中心。
+ **利用保存点暂停某个应用，稍后再把它启动起来**。这样可以为更高优先级的应用腾出集群资源，或者在
输入数据不连续的情况下及时释放资源。
+ **为保存点设置不同版本并将应用状态归档**。

保存点的功能如此强大，以至于很多用户都会 **周期性地创建保存点**，从而可以及时“回到过去”。我们在
生态中见到保存点最有趣的应用之一是 **不断将流式应用迁移到实例价格最低的数据中心**。

#### 从保存点启动应用
本节我们将介绍 **Flink在从保存点启动时如何去初始化应用状态**。

每个应用都会包含很多算子，而每个算子又可以定义一个或多个的键值或算子状态。算子会在一个或多个任务
上并行执行，因此一个典型的应用会包含多个状态，它们分布在不同TaskManager进程内的算子任务上。

下图展示的 **应用包含了三个算子，每个算子各有两个任务。其中一个算子（`OP-1`）有一个算子状态
（`OS-1`）,另一个算子（`OP-2`）有两个键值分区状态（`KS-1`和`KS-2`）。在生成保存点的时候，所
有任务的状态都会拷贝到某个持久化存储位置上**。

![为应用生成保存点和从保存点恢复应用](img/44.png)

保存点中的状态副本会按照算子标识和状态名称进行组织。该算子标识和状态名需要能将保存点的状态数据映
射到应用启动后的算子状态上。当应用从保存点启动时，Flink会将保存点的数据分发到对应算子的任务上。 










