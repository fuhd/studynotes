环境准备
================================================================================
## 1.运行环境介绍
**Flink执行环境主要分为本地环境和集群环境，本地环境主要为了方便用户编写和调度代码使用，而集群环
境则被用于正式环境中**，可以借助`Hadoop YARN`或Mesos等不同的资源管理器部署自己的应用。

### 1.1.环境依赖

#### 1.1.1.JDK环境
暂不详细介绍了。

#### 1.1.2.Scala环境
如果用户选择使用Scala作为Flink应用开发语言，则安装Scala执行环境，Scala环境可以通过本地安装
Scala执行环境，**也可以通过Maven依赖`Scala-lib`来引入**。

#### 1.1.3.Maven编译环境
**Flink的源代码目前仅支持通过Maven进行编译**，所以如果需要对源代码进行编译，或通过IDE开发Flink 
Application，则建议使用Maven作为项目工程编译方式。**需要注意的是，Flink程序需要Maven的版本在
3.0.4及以上，否则项目编译可能会出问题，建议用户根据要求进行环境的搭建**。

#### 1.1.4.Hadoop环境
对于执行在Hadoop YARN资源管理器的Flink应用，则需要配置对应的Hadoop环境参数。目前Flink（本书
完成时，Flink最新为1.7版本）官方提供的版本 **支持Hadoop2.4、2.6、2.7、2.8等主要版本**，所以
用户可以在这些版本的Hadoop YARN中直接运行自己的Flink应用，而不需要考虑兼容性的问题。

## 2.Flink项目模板 
Flink为了对用户使用Flink进行应用开发进行简化，**提供了相应的项目模板来创建开发项目**，用户不需
要自己引入相应的依赖库，就能够轻松搭建开发环境，**前提是在JDK(1.8及以上)和Maven(3.0.4及以上)
的环境已经安装好且能正常执行**。

### 2.1.基于Java实现的项目模板 

#### 2.1.1.创建项目
创建模板项目的方式有两种，一种方式是通过 **`Maven archetype`命令** 进行创建，另一种方式是通
过Flink提供的 **`Quickstart Shell`脚本** 进行创建。

**通过Maven Archetype进行创建**：
```shell
mvn archetype:generate 
    -DarchetypeGroupId=org.spache.flink 
    -DarchetypeArtifactid=flink-quickstart-java  
    -DarchetypeVersion=1.8.1
```
通过以上Maven命令进行项目创建的过程中，命令会交互式地提示用户对项目的`groupId`、`artifactId`、
`version`、`package`等信息进行定义，且部分选项具有默认值，用户直接回车即可。

**通过quickstart脚本创建**
```shell
curl https://flink.apache.org/q/quickstart-SNAPSHOT.sh | bash -s 1.8.1
```
通过以上脚本可以比较简单地创建项目，执行后项目会自动生成，但是项目的名称和一些GAV信息都是自动生成
的，用户不能进行交互式重新定义，其中的项目名称为quickstart，gourpid为`org.myorg.quickstart`,
version为0.1。这种方式对于Flink入门相对比较适合，其他用一定基础的情况下，**则不建议使用这种方
式进行项目创建**。

#### 2.1.2.编译项目
经过上述步骤创建后，可以使用Maven Command命令`mvn clean package`对项目进行编译。编译完民后
在项目同级目录会生成`target/<artifact-id>-<version>.jar`，**则该可执行Jar包就可以通过
Flink命令或者Web客户端提交到集群上执行**。
> 通过Maven创建Java应用，用户可以在Pom中指定`Main class`，这样提交执行过程中就具有默认的入口`Main Class`，否则需要用户在执行的Flink APP的Jar应用中指定`Main Class`。  

#### 2.1.3.开发应用
在项目创建和检测完成后，用户可以选择在模板项目中的代码上编写应用，也可以定义Class调用`DataSet API`
或`DataStream API`进行Flink应用的开发。

### 2.2.基于Scala实现的项目模板
Flink在开发接口中同样提供了Scala的接口，用户可以借助Scala高效简洁的特性进行Flink App的开发。
在创建项目的过程中，也可以像上述Java一样 **创建Scala模板项目，而在Scala项目中唯一的区别就是可
以支持使用SBT进行项目的创建和编译**。

#### 2.2.1.创建项目

##### （1）创建Maven项目

+ **使用Maven archetype进行项目创建**
下面代码是 **通过Maven archetype命令** 创建Flink Scala版本的模板项目，其中项目相关的参数同
创建Java项目一样，需要通过交互式的方式进行输入，用户可以指定对应的项目名称、groupid、
artifactid以及version等信息。
```shell
mvn archetype:generate  \
    -DarchetypeGroupId=org.apache.flink \
    -DarchetypeArtifactId=flink-quickstart-scala \
    -DarchetypeVersion=1.8.1
```

+ **使用quickstart curl脚本创建**
在创建Scala项目模板的过程中，也可以 **通过quickstart curl脚本** 进行创建，这种方式相对比较简
单，只要执行以下命令即可。
```shell
curl https://flink.apache.org/q/quickstart-scala-SNAPSHOT.sh | bash -s 1.8.1
```

##### （2）创建SBT项目
在使用Scala接口开发Flink应用中，不仅可以使用Maven进行项目的编译，也可以使用SBT进行项目的编译和
管理，**其项目结构和Maven创建的项目结构有一定的区别**。

+ 使用SBT命令创建项目 
```shell
sbt new tillrohrmann/flink-project.g8
```
执行上述命令后，会在客户端输出创建成功的信息，表示项目创建成功，同时在同级目录中生成创建的项目，
其中包含两个Scala的实例代码供用户参考。

+ **使用quickstart curl脚本创建项目**
可以通过使用以下指令进行项目创建Scala项目：
```shell
bash <(curl https://flink.apache.org/q/sbt-quickstart.sh)>
```
> 注意：如果项目编译方式选择SBT，则需要在环境中提前安装SBT编译器，**同时版本需要在0.13.13以上**，否则无法通过上述方式进行模板项目的创建。

#### 2.2.2.检查项目
对于使用Maven archetype创建的Scala项目模板，其结构和Java类似，在项目中增加了Scala的文件夹，
且包含两个Scala实例代码，其中一个是实现DataSet接口的批量应用实例BatchJob，另外一个是实现
DataStream接口的流式应用实例StreamingJob。代码清单如下：
```
├── pom.xml
└── src
    └── main
        ├── resources
        │   └── log4j.properties
        └── scala
            └── org
                └── fuhd
                    ├── BatchJob.scala
                    └── StreamingJob.scala

6 directories, 4 files
```

#### 2.2.3.编译项目

##### **使用Maven编译**
进入到项目路径中，然后通过执行`mvn clean package`命令对项目进行编译，编译完成后产生
`target/<artifact-id>-<version>.jar`。

##### **使用SBT编译**
进入到项目路径中，然后通过使用 **`sbt clean assembly`** 对项目进行编译，编译完成后再产生
`target/scala_your-major-scala-version/project-name-assembly-0.1.SNAPSHOT.jar`。

#### 2.2.4.开发应用
在项目创建和检测完成后，用户可以选择在Scala项目模板的代码上编写应用，**也可以定义Class调用
DataSet API或DataStream API进行Flink应用的开发**，然后通过编译打包，上传并提交到集群上运行。

## 3.Flink开发环境配置
我们可以选择IntelliJ IDEA或者Eclipse作为Flink应用的开发IDE，但是由于Eclipse本身对Scala语
言支持有限，所以 **Flink官方还是建议用户能够全用IDEA作为首选开发IDE**。

### 3.1.下载IntelliJ IDEA IDE
略

### 3.2.安装Scala Plugins
对于已经安装好的IntelliJ IDEA默认是不支持Scala开发环境的，如果用户选择使用Scala作为开发语言，
则需要 **安装Scala插件** 进行支持。略。

### 3.3.导入Flink应用代码 
略

### 3.4.项目配置
对于通过项目模板生成的项目，项目中的主要参数配置已被初始化，所以无须额外进行配置。

#### 3.4.1.Flink基础依赖库
对于Java版本，需要在项目的`pom.xml`文件中配置如下代码所示的依赖库，**其中`flink-java`和
`flink-streaming-java`分别是批量计算`DataSet API`和流式计算`DataStream API`的依赖库，
`{flink.version}`是官方发布的版本号，用户可根据自身需要进行选择**，本书中所有的实例代码都是基
于 **`Flink 1.7`** 版本开发的。
```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-java</artifactId>
    <version>{flink.version}</version>
    <scope>provided</scope>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-java_2.11</artifactId>
    <version>{flink.version}</version>
    <scope>provided</scope>
</dependency>
```
创建Scala版本Flink项目依赖库配置如下，**和Java相比需要指定scala的版本信息，目前官方建议的是使
用`Scala2.11`**，如果需要使用特定版本的Scala，则要将源码下载进行指定Scala版本编译，否则Scala
各大版本之间兼容性较弱会导致应用程序在实际环境中无法运行的问题。**Flink基于Scala语言项目依赖配
置库如下**：
```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-scala_2.11</artifactId>
    <version>{flink.version}</version>
    <scope>provided</scope>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-scala_2.11</artifactId>
    <version>{flink.version}</version>
    <scope>provided</scope>
</dependency>
```
另外在上述`Maven Dependences`配置中，核心的依赖库配置的Scope为`provided`，主要目的是在打包
阶段能够将依赖的Flink基础库排除在项目之外，当用户提交应用到Flink集群的时候，就避免因为引入Flink
基础库而导致Jar包太大或类冲突等问题。**而对于Scope配置成provided的项目可能出现本地IDE中无法运
行的问题，可以在Maven中通过配置Profile的方式，动态指定编译部署包的scope为provided，本地运行过
程中的scope为compile，从而解决本地和集群环境编译部署的问题**。
> 注意：由于Flink在最新版本中已经不再支持scala2.10版本，建议读者使用scala2.11，同时Flink将在未来的新版本中逐渐支持Scala2.12

#### 3.4.2.Flink Connector和Lib依赖库
除了上述Flink项目中应用开发必须依赖的基础库之外，如果用户需要添加其他依赖，例如Flink中内建的
Connector，或者其他第三方依赖库，需要在项目中添加相应的`Maven Dependences`，并将这些Dependence
的Scope需要配置成compile。

**如果项目中需要引入Hadoop相关依赖包，和基础库一样，在打包编译的时候将Scope注明为provided**，
因为Flink集群中已经将Hadoop依赖包添加在集群的环境中，用户不需要再将相应的Jar包打入应用中，否则
容易造成Jar包冲突。







