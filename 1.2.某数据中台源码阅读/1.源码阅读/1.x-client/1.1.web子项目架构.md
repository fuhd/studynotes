web子项目架构
================================================================================

## 1.web项目源代码结构
+ api包：**rest api**
    - ApiClusterService类： 获取集群列表信息
    - ApiDepAndEmpService类： 根据父部门信息获取子部门信息
    - ApiJobService类： 作业、作业日志与调试操作接口
    - ApiProjectService类： 获取用户所在的项目
    - ApiPropertyService类： 查询个人在大数据平台拥有的资源
    - ApiResService类： 资源操作接口（上传、下载等）
    - ApiUserService类： 用户操作接口
    - InnerRest类： 内部使用的一些rest接口？？
    - model包: **非表实体**
        + FunctionParam类： 函数参数model？
        + ProjectParam类： 项目参数model？
        + ResParam类： 资源参数model？
        + SyncTidbParam类： 同步Tidb参数model？
        + TableDataParam类： 表数据参数model？
        + TableVO类：表的VO对象？
        + UpdateTidbOwnerParam类： 更新Tidb的Owner的参数model？
+ controller包：
    - jobmgr包：**作业管理**
        + JobMgrRouteController类： 作业管理
    - pharos包：灯塔？
        + ClusterController类： 集群操作控制器
        + ColumnController类： hive表的列操作控制器
        + DataExportController类： 数据导出操作控制器
        + DataMgrRouteController类： 数据资源管理控制器？
        + DataQualityController类： 数据质量操作控制器
        + GroupController类： 用户组操作控制器
        + GroupMemberController类：用户组成员操作控制器
        + ProjectController类： 项目操作控制器
        + TableAccessLogController类： hive表访问日志操作控制器
        + TableController类： hive表操作控制器
        + TablePrivsController类：hive表权限操作控制器
        + TrackController类： **没使用**
        + model包：**非表实体**
            - ClusterConfigVO类： 集群配置VO对象
            - ColumnVO类： hive表的列VO对象
    - portal包： 门户？
        + DataSourceController类： 数据源操作控制器
        + EmpDepController类： 部门操作控制器
        + FolderController类： 文件夹（目录）操作控制器
        + FunctionController类：Hive函数操作控制器
        + JobAlarmController类：作业警报操作控制器
        + JobController类： 作业操作控制器
        + JobInstanceController类： 作业实例操作控制器
        + JobIoTableController类： 作业的输入输出表操作控制器
        + JobMetricsController类： 作业度量控制器
        + JobSchedulerController类： 作业调度控制器
        + JobStreamInstanceController类： 流作业实例操作控制器
        + OperStatisController类： 操作统计控制器
        + ProjectMemberController类： 项目成员操作控制器
        + RecycleBinController类： 回收站操作控制器
        + ResourceController类： 资源操作控制器
        + ResUseStatisController类： 数据资源使用统计控制器？
        + RouteController类： 用户登录路由控制器？
        + ServerMonitorController类： 服务监控控制器（jobserver?）
        + SqlController类： Hive的数据库与表操作控制器
        + UserInfoController类： 用户操作控制器
        + model包：
            - JobSchedulerVO类：作业调度VO对象 
            - PythonVO类： Python的VO对象 ？？
            - UserPwdVO类：修改用户密码的VO对象 
    - BaseController类： 控制器基类（包含一些通用接口方法）
    - ConfigController类： 配置操作控制器（`/config`的URL）
    - ExecuteTaskController类： 执行Task的控制器
    - GlobalExceptionHandler类：全局的异常处理器
+ support包：
    - security包： Spring Security的安全认证
        + CustomAuthenticationSuccessHandler类： 自定义身份验证成功的处理程序
        + CustomLoginUrlAuthenticationEntryPoint类： 自定义登录Url身份验证入口点
        + GeneralAuthProvider类： 通用身份验证提供程序
        + LdapUserDetailsService类： Ldap用户详细信息服务（用户存在Ldap？还是支持）
        + SecurityUtils类： 工具类 
        + SessionAuthenticationSuccessHandler类： Session身份验证成功处理程序
        + UserNameAuthenticationFailureHandler类： 用户名验证失败处理程序
    - Result类： 执行结果类
    - SecurityConfig类： Spring Security的配置文件
    - SqlFormatter类： SQL格式化工具类
    - WebConfig类： Spring web配置类
    - WebLogAspect类： web日志的AOP类
    - WebSocketConfig类： Spring WebSocket配置类
+ util包：
    - AuthUtil类：部分权限操作工具类
    - HttpsClientRequestFactory类：https操作的工厂类
    - RequestUtil类： 请求操作的工具类（主要用于获取IP）
    - UrlBean类： Url操作Bean类（只有getter与setter）
    - UrlUtil类： Url操作工具类
+ AppMain类： 程序入口


## 2.程序配置项（只列举与业务相关的）

### 2.1.写死在程序中的属性或配置
```ini
#注意：代码里还有xx公司的名称 
+ ApiJobService： String hdfsUrl = "/user/maxcompute/users/"+userId+"/";
+ ApiJobService： jobInstanceEntity.setClusterCode(configClient.getString("dc.api.task.default.cluster.code"));
+ ApiJobService： String sparkJobServerUrl = configClient.getString("dc.spark.jobserver.url");
+ ApiJobService[可能未使用]： String[] admins = configClient.getStringArray("dc.project.super.admin");

+ ApiPropertyService： String defaultFS = configurationLoader.getConfiguration(dataCenter).get("fs.defaultFS");
+ ApiPropertyService： String pPath = "/user/maxcompute/users/" + member.getUserId();

+ ApiResService： String[] ips = configClient.getStringArray("dc.allow.download.ips");
+ ApiResService： Boolean openWhiteList = configClient.getBoolean("dc.allow.download.whiteList", true);
+ ApiResService： boolean dataExportEnabled = configClient.getBoolean("dc.export.data.enabled", false);
+ ApiResService： filePath = "/user/maxcompute/users/" + userList.get(0).getUserId() + location;
+ ApiResService： File tmpFile = new File("/home/admin/tmp/upload/" + projectCode + "-" + fileName);

+ InnerRest： String userHome = "/user/maxcompute/users/" + loginUserid;
+ InnerRest： long hzUserPathSizeSum = hadoopFileManager.fileSizeByDataCenter("cec", userPath); 
+ InnerRest： long shUserPathSizeSum = hadoopFileManager.fileSizeByDataCenter("shanghai", userPath);
+ InnerRest： long shUserPathSizeSum = hadoopFileManager.fileSizeByDataCenter("yinni", userPath);
+ InnerRest： boolean connectHiveMysql = configClient.getBoolean("dc.allow.connect.hive.mysql", true);

+ JobMgrRouteController： modelMap.addAttribute("alarmDisable", configClient.getBoolean("dc.job.alarm.checkbox.disable", true));
+ JobMgrRouteController： modelMap.addAttribute("jobserverUrl", configClient.getString("dc.spark.jobserver.url"));
+ JobMgrRouteController： String yarnWebUI = configClient.getString("yarn.web.ui");
+ JobMgrRouteController： String sparkConsoleUrl = configClient.getString("dc.spark.console.url");
+ JobMgrRouteController： Result result = restTemplate.postForObject(configClient.getString("dc.alphax.apiUrl")+"api/alphax/cancel?params=" + entity.getFlinkJobId(), null, Result.class);
+ JobMgrRouteController： configClient.getBoolean("dc.sql.jobserver", false)) 
+ JobMgrRouteController： Result result = restTemplate.postForObject(configClient.getString("dc.flinkStream.apiUrl")
+"api/flinkstream/cancel?params=" + entity.getFlinkJobId(), null, Result.class);

+ ClusterController： String[] arr = configClient.getStringArray("dc.datacenter");
+ ClusterController： Utils.checkHadoopConfig("hdfs-site", clusterConfigVO.getHdfsConfig());
+ ClusterController： Utils.checkHadoopConfig("mapred-site", clusterConfigVO.getMapredConfig());
+ ClusterController： Utils.checkHadoopConfig("hive-site", clusterConfigVO.getHiveConfig());
+ ClusterController： Utils.checkHadoopConfig("yarn-site", clusterConfigVO.getYarnConfig());
+ ClusterController： Utils.checkHadoopConfig("core-site", clusterConfigVO.getCoreConfig());
+ ClusterController： Utils.checkHadoopConfig("hbase-site", hbaseConfig);

+ ColumnController： clusterCode = configClient.getString("cec.cluster.code", "cec-spark");
+ ColumnController： clusterCode = configClient.getString("shanghai.cluster.code", "shanghai-spark");
+ ColumnController： boolean isDataExportPartition = configClient.getBoolean("dc.export.data.partition.enabled", true);
+ ColumnController： boolean dataExportEnabled = configClient.getBoolean("dc.export.data.enabled", false);

+ DataMgrRouteController： String[] warehouseAdmins = configClient.getStringArray("dc.data.warehouse.admin");
+ DataMgrRouteController： String[] tidbAdmins = configClient.getStringArray("dc.tidb.admin");
+ DataMgrRouteController： boolean dataExportEnabled = configClient.getBoolean("dc.export.data.enabled", false);
+ DataMgrRouteController： String[] limitDlDcs = configClient.getStringArray("dc.limit.download.datacenter");
+ DataMgrRouteController： int nums = configClient.getInt("dc.table.partition.limit.nums", 2);
+ DataMgrRouteController： String pPath = "/user/maxcompute/users/" + userId;
+ DataMgrRouteController： String defaultFS = configurationLoader.getConfiguration(dataCenter).get("fs.defaultFS");

+ TableController： boolean connectHiveMysql = configClient.getBoolean("dc.allow.connect.hive.mysql", true);
+ TableController[未使用]： String[] admins = configClient.getStringArray("dc.data.warehouse.admin");
+ TableController： String[] tableNameNotCheck = configClient.getStringArray("dc.tableName.not.check");
+ TableController： modelMap.addAttribute("warehouseAdmin", configClient.getString("dc.data.warehouse.admin"));
+ TableController： boolean connectHiveMysql = configClient.getBoolean("dc.allow.connect.hive.mysql", true);
+ TableController： jobInstance.setClusterCode(configClient.getString("dc.api.task.default.cluster.code"));
+ TableController： Connection con = DriverManager.getConnection("jdbc:hive2://bigdata008:10000/default", "hive", "hive");

+ FolderController： String path = "/user/maxcompute/users/" + AuthUtil.currentUserName();

+ JobController： boolean dataExportEnabled = configClient.getBoolean("dc.export.data.enabled", false);
+ JobController： model.addAttribute("sqlCompletionType", configClient.getString("dc.sql.completion.type"));
+ JobController： model.addAttribute("stream_time_mode",serverConfigJsonObj.getString("time.characteristic"));
+ JobController： model.addAttribute("stream_checkpoint_mode",serverConfigJsonObj.getString("stream.flink.checkpoint.mode"));
+ JobController： model.addAttribute("stream_checkpoint_interval",serverConfigJsonObj.getString("stream.flink.checkpoint.interval"));
+ JobController： String[] admins = configClient.getStringArray("dc.project.super.admin");
+ JobController： boolean runJob = configClient.getBoolean("dc.run.flink.job.enabled", true);
+ JobController： String url = configClient.getString("dc.python.code.complete.url");
+ JobController： Boolean enabled = configClient.getBoolean("dc.python.code.complete.enabled", false);
+ JobController[未使用]： String autopepPath = configClient.getString("dc.python.autopep8.path");
+ JobController： String pythonPath = configClient.getString("dc.python.path");
+ JobController： String pyflakesPath = configClient.getString("dc.python.pyflakes.path");
+ JobController： String fileTmpPath = configClient.getString("dc.python.filetmp.path");

+ JobInstanceController： int days = configClient.getInt("dc.supplement.runing.days", 7);
+ JobInstanceController： int count = configClient.getInt("dc.supplement.runing.count", 100);
+ JobInstanceController： Integer num = configClient.getInt("dc.allow.running.instance.num", 8);

+ JobMetricsController： String sparkJobServerUrl = configClient.getString("dc.spark.jobserver.url");

+ OperStatisController： map.put("useTDBI", configClient.getBoolean("dc.tdbi.use.report", false));
+ OperStatisController： Long tdbiReportId = configClient.getLong("dc.tdbi.oper.report.id");

+ RecycleBinController： String filePath = "/user/maxcompute/" + projectCode + "/resources/" + id;

+ ResourceController： File tmpFile = new File("/home/admin/tmp/upload/" + projectCode + "-" + fileName);
+ ResourceController： String path = "/user/maxcompute/" + projectCode + "/resources/" + id + "/latest/" + fileName; 

+ ServerMonitorController： String url = "http://" + addr + "/sparkDriver/getRunTimeStageInfo";
+ ServerMonitorController： String url = "http://" + addr + "/sparkDriver/getSparkConfig";  
+ ServerMonitorController： String url = "http://" + addr + "/sparkDriver/getNodeInfo";
+ ServerMonitorController： String url = "http://" + addr + "/sparkDriver/getNodeInfo";
+ ServerMonitorController： String url = "http://" + addr + "/sparkDriver/getTaskInfo?stageId=" + stageId + "&stageTaskList=" + taskList;
+ ServerMonitorController： String url = "http://" + addr + "/sparkDriver/getRDDInfo";
+ ServerMonitorController： String url = "http://" + addr + "/sparkDriver/getAllStackTrace";
+ ServerMonitorController： String url = "http://" + addr + "/sparkDriver/getPythonStackTrace";
+ ServerMonitorController： String url = "http://" + addr + "/sparkDriver/getExecutorStackTrace?executorId=" + executorId;
+ ServerMonitorController： String webUrl = "http://" + yarnIp + "/ws/v1/cluster/apps"; 
+ ServerMonitorController： webUrl = "http://" + yarnIp + "/proxy/";

+ ConfigController： String msg = configClient.getString("notify.msg");

+ ExecuteTaskController： Path dirPath = new Path("/user/maxcompute/users/" + userId); 

+ SecurityUtils： ResponseEntity<String> result = restTemplate.exchange(configClient.getString("dc.platform.hamis.url")+"/platform/user/api/v2/hamis/platform/usermanager/queryUserName", HttpMethod.GET, requestEntity, String.class);
+ SecurityUtils： ResponseEntity<String> result = restTemplate.postForEntity(configClient.getString("dc.platform.hamis.url")+"/platform/user/api/v2/hamis/platform/usermanager/cancellation",new HttpEntity<String>("", headers), String.class);
```

### 2.2.写在配置文件中的配置项
```ini
server.port=8181
spring.application.name=maxcomputexxx
dc.redis.leader.election.ip=192.168.1.xxx
logging.path=/var/tmp/xxxxx
```

## 3.web资源
略
